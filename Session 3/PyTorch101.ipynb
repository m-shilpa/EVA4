{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of EVA4 S3 PyTorch101.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mshilpaa/EVA4/blob/master/Session%203/PyTorch101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kYM3ylzYo1Q",
        "colab_type": "text"
      },
      "source": [
        "![PyTorch](https://devblogs.nvidia.com/wp-content/uploads/2017/04/pytorch-logo-dark.png)\n",
        "\n",
        "An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcVqSTrWY6oz",
        "colab_type": "text"
      },
      "source": [
        "# Tensor - Pytorch's core data structure\n",
        "\n",
        "In Python we can create lists, lists of lists, lists of lists and so on. In NumPy there is a `numpy.ndarray` which represents `n`- dimensional array. In math there is a special name for the generalization of vectors and matrices to a higher dimensional space - a tensor\n",
        "\n",
        "Tensor is an entity with a defined number of dimensions called an order (rank). \n",
        "\n",
        "**Scalar** can be considered as a rank-0-tensor. \n",
        "\n",
        "**Vector** can be introduced as a rank-1-tensor. \n",
        "\n",
        "**Matrices** can be considered as a rank-2-tensor.\n",
        "\n",
        "# Tensor Basics\n",
        "\n",
        "Let's import the torch module first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-oFVL2tYYqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGxuR5IEaFJa",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Creation\n",
        "Let's view examples of matrices and tensors generation\n",
        "\n",
        "2-dimensional (rank-2) tensor of zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897JQc25aD3E",
        "colab_type": "code",
        "outputId": "5a8398a5-accd-4519-de47-9060b4c9c94a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(torch.zeros(3, 4)).size()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pujmpEBhaPRA",
        "colab_type": "text"
      },
      "source": [
        "Random rank-3 tensor:\n",
        "_read the print below and convince yourself how this is a rank-3-tensor and learn what those 2, 3, 4 values are there for_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBrznTNhaOL7",
        "colab_type": "code",
        "outputId": "f9350edf-4ccf-4f81-f7e8-1bcfeea97cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "torch.rand(2, 3, 4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.4527, 0.8047, 0.1790, 0.0405],\n",
              "         [0.3719, 0.2457, 0.3258, 0.2959],\n",
              "         [0.4676, 0.7489, 0.2575, 0.2816]],\n",
              "\n",
              "        [[0.9527, 0.0647, 0.3739, 0.6701],\n",
              "         [0.1815, 0.2522, 0.5853, 0.5799],\n",
              "         [0.9247, 0.3664, 0.6367, 0.6719]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO4fIr15asdi",
        "colab_type": "text"
      },
      "source": [
        "I am hoping you have noticed 4-elements in a row, 3 rows making one block and there are 2 blocks. \n",
        "\n",
        "Random rank-4-tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiCLvMGCaVZ3",
        "colab_type": "code",
        "outputId": "234c29ac-6b16-4d93-b1e0-200b52b1140b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "torch.rand(2, 2, 2, 3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.6854, 0.8092, 0.3993],\n",
              "          [0.4950, 0.4948, 0.6825]],\n",
              "\n",
              "         [[0.9659, 0.9548, 0.7385],\n",
              "          [0.2925, 0.2483, 0.0622]]],\n",
              "\n",
              "\n",
              "        [[[0.4726, 0.6103, 0.0535],\n",
              "          [0.0424, 0.9071, 0.2743]],\n",
              "\n",
              "         [[0.3386, 0.9801, 0.2309],\n",
              "          [0.0550, 0.9729, 0.6604]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSAl_XQH-ai6",
        "colab_type": "code",
        "outputId": "b2a5ade0-3e06-43f6-b5f5-ef07bf093276",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "x = torch.tensor([\n",
        "     [1, 2, 3],\n",
        "     [4, 5, 6]\n",
        "   ])\n",
        "print(\"x.shape = \",x.shape,end = '\\n\\n')\n",
        "print(\"torch.sum(x,dim=1) = \",torch.sum(x,dim=1),end = '\\n\\n')\n",
        "print(\"torch.sum(x,dim=0) = \",torch.sum(x,dim=0) ,end = '\\n\\n')\n",
        "print(\"torch.sum(x,dim=1,keepdim=True) = \",torch.sum(x,dim=1,keepdim=True),end = '\\n\\n' )\n",
        "print(\"torch.sum(x,dim=0,keepdim=True) = \",torch.sum(x,dim=0,keepdim=True) ,end = '\\n\\n')\n",
        "# dim = 0 indicates that it collapses the rows into a single row. So when it collapses the axis 0 (the row), it becomes just one row (it sums column-wise)."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x.shape =  torch.Size([2, 3])\n",
            "\n",
            "torch.sum(x,dim=1) =  tensor([ 6, 15])\n",
            "\n",
            "torch.sum(x,dim=0) =  tensor([5, 7, 9])\n",
            "\n",
            "torch.sum(x,dim=1,keepdim=True) =  tensor([[ 6],\n",
            "        [15]])\n",
            "\n",
            "torch.sum(x,dim=0,keepdim=True) =  tensor([[5, 7, 9]])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQ73oQfNC2HJ",
        "colab_type": "code",
        "outputId": "c64ec835-8b7b-4681-da06-63c93509017b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# When we look at the shape of a 3D tensor we’ll notice that the new dimension gets prepended and takes the first position i.e. the third dimension becomes dim=0.\n",
        "y = torch.tensor([\n",
        "     [\n",
        "       [1, 2, 3],\n",
        "       [4, 5, 6]\n",
        "     ],\n",
        "     [\n",
        "       [1, 2, 3],\n",
        "       [4, 5, 6]\n",
        "     ],\n",
        "     [\n",
        "       [1, 2, 3],\n",
        "       [4, 5, 6]\n",
        "     ]\n",
        "   ])\n",
        "y.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([3, 2, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF6VL68s3AMV",
        "colab_type": "text"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "How many dimensions are there in a tensor defined as below?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IbXcI3x3Ibt",
        "colab_type": "code",
        "outputId": "922ef61c-2b9a-4cea-f855-19b8699e1cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.rand(1, 1, 1, 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.5639]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P8RmuXkF8TC",
        "colab_type": "text"
      },
      "source": [
        "**Ans : 4**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZ958aiPD0WD",
        "colab_type": "code",
        "outputId": "1379dc89-b2cc-4030-fe05-fe9c31fe13e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "(torch.rand(1, 1, 1, 1)).shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 1, 1, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8OR661JKFcJm",
        "colab_type": "code",
        "outputId": "828c90bc-1fd6-4b6a-84d9-6d1d12c2b781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "print(torch.rand(2, 1))  # 2 rows 1 column\n",
        "print(torch.rand(1, 1))\n",
        "print(torch.rand(1, 1,1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9303],\n",
            "        [0.0623]])\n",
            "tensor([[0.3834]])\n",
            "tensor([[[0.4998]]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgpqssRa9jF",
        "colab_type": "text"
      },
      "source": [
        ".\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are many more ways to create tensor using some restrictions on values it should contatn - for the full reference, please follow the [official docs](https://pytorch.org/docs/stable/torch.html#creation-ops). \n",
        "\n",
        "\n",
        ".\n",
        "---\n",
        "\n",
        "\n",
        "# Python / NumPy / Pytorch interoperability\n",
        "\n",
        "You can create tensors from python as well as numpy arrays. You can also convert torch tensors to numpy arrays. So, the interoperability between torch and numpy is pretty good. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipgpeWPfa6gE",
        "colab_type": "code",
        "outputId": "ad590a8e-b50e-470b-e9b0-a510f04abb3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "# Simple Python List\n",
        "python_list = [1, 2]\n",
        "\n",
        "# Create a numpy array from python list\n",
        "numpy_array = np.array(python_list)\n",
        "\n",
        "# Create a torch Tensor from python list\n",
        "tensor_from_list = torch.tensor(python_list)\n",
        "\n",
        "# Create a torch Tensor from Numpy array\n",
        "tensor_from_array = torch.tensor(numpy_array)\n",
        "\n",
        "# Another way to create a torch Tensor from Numpy array (share same storage)\n",
        "tensor_from_array_v2 = torch.from_numpy(numpy_array)\n",
        "\n",
        "# Convert torch tensor to numpy array\n",
        "array_from_tensor = tensor_from_array.numpy()\n",
        "\n",
        "print('List:   ', python_list)\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_list)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2)\n",
        "print('Array:  ', array_from_tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List:    [1, 2]\n",
            "Array:   [1 2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Array:   [1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_x-86B8gqik",
        "colab_type": "text"
      },
      "source": [
        "**Difference between** `torch.Tensor` **and** `torch.from_numpy`\n",
        "\n",
        "Pytorch aims to be an effective library for computations. What does it mean? It means that pytorch avoids memory copying if it can. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHCKDGpygn_d",
        "colab_type": "code",
        "outputId": "ae8996b4-494c-48d4-e78d-2269d4996d3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "numpy_array[0] = 10\n",
        "\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array:   [10  2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([10,  2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CKtdNRM3SMp",
        "colab_type": "text"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "Assume that we moved our complete (cats vs dogs) image dataset to numpy arrays. Then we use torch.from_numpy to convert these images to tensor. Then we apply a specific data augmentation strategy called \"CutOut\" which blocks a portion of the image directly on these tensors. What will happen to the accuracy of a model trained on this strategy compared to the one without this strategy? CutOut strategy is shown below: \n",
        "\n",
        "![CutOut](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSnSyN835AmtQPKQbPjDHX-FmshNilbtexX95cRGQPwl56QCGDn)\n",
        "\n",
        "Ans : Increases\n",
        "\n",
        "## Question 3:\n",
        "Why do you think we are observing this behavior?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZq7O-aihSOA",
        "colab_type": "text"
      },
      "source": [
        "We have two different ways to create tensor from its NumPy counterpart - one copies memory and another one shares the same underlying storage. It works in the opposite way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNFOwV8EhPwQ",
        "colab_type": "code",
        "outputId": "72273c00-a44b-46f5-eb53-67970b6be0f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "array_from_tensor = tensor_from_array.numpy()\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)\n",
        "\n",
        "tensor_from_array[0] = 11\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor:  tensor([1, 2])\n",
            "Array:  [1 2]\n",
            "Tensor:  tensor([11,  2])\n",
            "Array:  [11  2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM9ytbvKhtCw",
        "colab_type": "text"
      },
      "source": [
        "## Data types\n",
        "\n",
        "The basic data type of all Deep Learning-related operations is float, but sometimes you may need something else. Pytorch support different number types for its tensors the same way NumPy does it - by specifying the data type on tensor creation or via casting. Ths full list of supported data types can be found [here](https://pytorch.org/docs/stable/tensors.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd6WkzJ4hpYi",
        "colab_type": "code",
        "outputId": "2f04b6bb-da06-4de7-806d-0817ee21fd7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "tensor = torch.zeros(2, 2)\n",
        "print('Tensor with default type: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.float16)\n",
        "print('Tensor with 16-bit float: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.int16)\n",
        "print('Tensor with integers: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.bool)\n",
        "print('Tensor with boolean data: ', tensor)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor with default type:  tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "Tensor with 16-bit float:  tensor([[0., 0.],\n",
            "        [0., 0.]], dtype=torch.float16)\n",
            "Tensor with integers:  tensor([[0, 0],\n",
            "        [0, 0]], dtype=torch.int16)\n",
            "Tensor with boolean data:  tensor([[False, False],\n",
            "        [False, False]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9F4Dkdr40TE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 4:\n",
        "We saw above that some times numpy and tensors share same storage and changing one changes the other. \n",
        "If we define a rank-2-tensor with ones (dtype of f16), and then convert it into a numpy data type using tensor.numpy() and store it in a variable called \"num\", and then we perform this operation `num = num * 0.5`, will the original tensor have 1.0s or 0.5s as its element values? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc95sVzZ5pIb",
        "colab_type": "text"
      },
      "source": [
        "https://stackoverflow.com/questions/55040217/pytorch-conversion-between-tensor-and-numpy-array-the-addition-operation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3FQEStQfwe_",
        "colab_type": "code",
        "outputId": "ea97d6c5-1330-47d5-d3b1-e120268be0cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "\n",
        "tr2 = torch.ones(2, 2, dtype=torch.float16)\n",
        "num = tr2.numpy()\n",
        "num = num*0.5\n",
        "print(tr2)\n",
        "print(num)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], dtype=torch.float16)\n",
            "[[0.5 0.5]\n",
            " [0.5 0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQt8Mmm51OE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: \n",
        "If the operation `num = num*5` is changed to `num[:] = num*5` will the original tensor have 1.0s or 0.5s as its element values? \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Ans : 5.0\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-uH3bSl3yBm",
        "colab_type": "code",
        "outputId": "70c467f6-b7c9-4d03-b845-3c8d20d4b469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "source": [
        "tr2 = torch.ones(2, 2, dtype=torch.float16)\n",
        "num = tr2.numpy()\n",
        "num[:] = num*5\n",
        "print(tr2)\n",
        "print(num)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[5., 5.],\n",
            "        [5., 5.]], dtype=torch.float16)\n",
            "[[5. 5.]\n",
            " [5. 5.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzh8UB8KiVmb",
        "colab_type": "text"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "Tensor provides access to its elements via the same `[]` operation as a regular python list or NumPy array. However, as you may recall from NumPy usage, the full power of math libraries is accessible only via vectorized operations, i.e. operations without explicit looping over all vector elements in python and using implicit optimized loops in C/C++/CUDA/Fortran/etc. available via special function calls. Pytorch employs the same paradigm and provides a wide range of vectorized operations. Let's take a look at some examples. \n",
        "\n",
        "Joining a list of tensors together with `torch.cat`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMaCDKPhiUAb",
        "colab_type": "code",
        "outputId": "cb4b5803-53b0-43b7-a3e5-74dbc5b54a9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "a = torch.zeros(3, 2)\n",
        "b = torch.ones(3, 2)\n",
        "print(a)\n",
        "print(b)\n",
        "print(torch.cat((a, b), dim=0))\n",
        "print((torch.cat((a, b), dim=0)).shape) # concatenate all the rows. so shape after concatenation is (3+3,2)=> (6,2)\n",
        "print(torch.cat((a, b), dim=1))\n",
        "print((torch.cat((a, b), dim=1)).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "torch.Size([6, 2])\n",
            "tensor([[0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.]])\n",
            "torch.Size([3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bj4aeE86zdH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 6: \n",
        "Is the transpose of concatenated a & b tensor on dimension 1, same as the contatenated tensor of a & b on dimension 0? \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Ans : no\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJDP8HwH_IgG",
        "colab_type": "text"
      },
      "source": [
        "torch.transpose(input, dim0, dim1) → Tensor\n",
        "Returns a tensor that is a transposed version of input. The given dimensions dim0 and dim1 are swapped.\n",
        "\n",
        "The resulting out tensor shares it’s underlying storage with the input tensor, so changing the content of one would change the content of the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aUqe21Ce9iFj",
        "colab_type": "code",
        "outputId": "c2805e52-5f2b-41f3-cf7e-3e3df6df20d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        }
      },
      "source": [
        "c = torch.cat((a, b), dim=0)\n",
        "d = torch.cat((a, b), dim=1)\n",
        "print(c)\n",
        "print(torch.transpose(d, 0, 1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AgcCaOb_gMU",
        "colab_type": "code",
        "outputId": "597b4aa9-fac5-4d3f-ea66-87fb5454daa7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "# The resulting out tensor shares it’s underlying storage with the input tensor, so changing the content of one would change the content of the other.\n",
        "d = torch.cat((a, b), dim=1)\n",
        "e = d\n",
        "print(d)\n",
        "print(e)\n",
        "\n",
        "(torch.transpose(d, 0, 1))\n",
        "print(d)\n",
        "print(e)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.]])\n",
            "tensor([[0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.]])\n",
            "tensor([[0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.]])\n",
            "tensor([[0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TXNne69j7LP",
        "colab_type": "text"
      },
      "source": [
        "Indexing with another tenxor/array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-Fsi4jVd6",
        "colab_type": "code",
        "outputId": "37b4b8e3-97d0-4ff6-9a65-cf1002ff84cf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "a = torch.arange(start=0, end=10)\n",
        "indices = np.arange(0, 10) > 5\n",
        "print(a)\n",
        "print(indices)\n",
        "# indices is a “mask” for a tensor (of the same size as the list), returning the entries of the tensor where the list is true.\n",
        "print(a[indices]) # prints those elements of a where indices' elements are True\n",
        "indices = torch.arange(start=0, end=10) %5\n",
        "print(indices)\n",
        "print(a[indices])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "[False False False False False False  True  True  True  True]\n",
            "tensor([6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS4dnlu47WQu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 7:\n",
        "\n",
        "`a` is defined as `torch.arange(start=0, end=10)`. We will create `b` using the two operations as below. In both cases do we get the same value?\n",
        "\n",
        "\n",
        "1.   indices variable created by the modulo operation on arange between 0 and 10. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "2.   indices variable created by the modulo operation on arange betwenn 1 and 11. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Ans :No\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy4tqnwDEi9O",
        "colab_type": "code",
        "outputId": "8bea2930-7a52-4cd0-9bc2-31e585a40bee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "a = torch.arange(start=0, end=10)\n",
        "print(\"a = \",a)\n",
        "indices = torch.arange(start=0, end=10) %5\n",
        "print(\"indices = \",indices)\n",
        "b1 = a[indices[:-5]]\n",
        "print(\"b1 = \",b1)\n",
        "indices = torch.arange(start=1, end=11) %5\n",
        "print(\"indices = \",indices)\n",
        "b2 = a[indices[:-5]]\n",
        "print(\"b2 = \",b2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a =  tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "indices =  tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "b1 =  tensor([0, 1, 2, 3, 4])\n",
            "indices =  tensor([1, 2, 3, 4, 0, 1, 2, 3, 4, 0])\n",
            "b2 =  tensor([1, 2, 3, 4, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ4ZCVsTk-KH",
        "colab_type": "text"
      },
      "source": [
        "What should we do if we have, say, rank-2-tensor and want to select only some rows?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GtRpotjkt1q",
        "colab_type": "code",
        "outputId": "f8a27dbc-73f1-4690-95df-57c59f1014c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "tensor = torch.rand((5, 3))\n",
        "# two ways to select only some rows\n",
        "rows1 = torch.tensor([0, 2])\n",
        "rows2 = torch.tensor([True,False,True,False,False]) # not feasible for large no. of rows\n",
        "print(tensor)\n",
        "print(tensor[rows1])\n",
        "print(tensor[rows2])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.7494, 0.7181, 0.8571],\n",
            "        [0.4414, 0.0782, 0.5938],\n",
            "        [0.6554, 0.0576, 0.6001],\n",
            "        [0.9816, 0.5513, 0.5808],\n",
            "        [0.7981, 0.8163, 0.7599]])\n",
            "tensor([[0.7494, 0.7181, 0.8571],\n",
            "        [0.6554, 0.0576, 0.6001]])\n",
            "tensor([[0.7494, 0.7181, 0.8571],\n",
            "        [0.6554, 0.0576, 0.6001]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIJnr_N2_Qaf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 8: \n",
        "\n",
        "Consider a tensor defined as `torch.rand((6, 5))`. Is the shape of the new tensor created by taking the 0th, 2nd and 4th row of the old tensor same as the shape of the a newer tensor created by taking the 0th, 2nd and 4th row of the old tensor after transposing it by operation `torch.transpose(tensor, 0, 1)` ?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Ans : No\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zDnHjPuTJek_",
        "colab_type": "code",
        "outputId": "9439acde-cb8a-42ed-bf2c-ac86c871e216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "t = torch.rand((6, 5))\n",
        "print(t)\n",
        "t_new1 = t[torch.tensor([0,2,4])]\n",
        "print(t_new1)\n",
        "print(t_new1.shape)\n",
        "# new_new_t = torch.transpose(new_t, 0, 1)\n",
        "# print(new_new_t[torch.tensor([0,2,4])])\n",
        "# print((new_new_t[torch.tensor([0,2,4])]).shape)\n",
        "\n",
        "t_new2 = torch.transpose(t, 0, 1)[torch.tensor([0,2,4])]\n",
        "print(t_new2)\n",
        "print((t_new2).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.1806, 0.4933, 0.3685, 0.8523, 0.6292],\n",
            "        [0.1543, 0.6267, 0.8194, 0.6599, 0.2546],\n",
            "        [0.3342, 0.3907, 0.5240, 0.9878, 0.2790],\n",
            "        [0.4722, 0.1994, 0.1260, 0.8918, 0.3766],\n",
            "        [0.4709, 0.1287, 0.2987, 0.9443, 0.3054],\n",
            "        [0.2658, 0.2972, 0.1598, 0.8562, 0.7262]])\n",
            "tensor([[0.1806, 0.4933, 0.3685, 0.8523, 0.6292],\n",
            "        [0.3342, 0.3907, 0.5240, 0.9878, 0.2790],\n",
            "        [0.4709, 0.1287, 0.2987, 0.9443, 0.3054]])\n",
            "torch.Size([3, 5])\n",
            "tensor([[0.1806, 0.1543, 0.3342, 0.4722, 0.4709, 0.2658],\n",
            "        [0.3685, 0.8194, 0.5240, 0.1260, 0.2987, 0.1598],\n",
            "        [0.6292, 0.2546, 0.2790, 0.3766, 0.3054, 0.7262]])\n",
            "torch.Size([3, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naDzFMkslU0b",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Shapes\n",
        "\n",
        "Reshaping a tensor is a frequently used operation. We can change the shape of a tensor without the memory copying overhead. There are two methods for that: `reshape` and `view`. \n",
        "\n",
        "The difference is the following: \n",
        "\n",
        "\n",
        "*   view tries to return a tensor, and it shares the same memory with the original tensor. In case, if it cannot reuse the same memory due to [some reason](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view), it just fails. \n",
        "*   reshape always returns the tensor with the desired shape and tries to reuse the memory. If it cannot, it creates a copy\n",
        "\n",
        "Let's see with the help of an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HClDkqLLlMJh",
        "colab_type": "code",
        "outputId": "ef75e45a-31d5-438b-8803-ed5701f4b188",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        }
      },
      "source": [
        "tensor = torch.rand(2, 3, 4)\n",
        "print(tensor)\n",
        "print('Pointer to data: ', tensor.data_ptr()) # Returns the address of the first element of the tensor.\n",
        "print('Shape: ', tensor.shape)\n",
        "\n",
        "reshaped = tensor.reshape(24)\n",
        "\n",
        "view = tensor.view(3, 2, 4)\n",
        "print('Reshaped tensor - pointer to data', reshaped.data_ptr())\n",
        "print('Reshaped tensor shape ', reshaped.shape)\n",
        "\n",
        "print('Viewed tensor - pointer to data', view.data_ptr())\n",
        "print('Viewed tensor shape ', view.shape)\n",
        "\n",
        "# assert produces an error like tensor converted to python boolean, may stop the back propagation process etc.\n",
        "assert tensor.data_ptr() != view.data_ptr()\n",
        "\n",
        "assert np.all(np.equal(tensor.numpy().flat, reshaped.numpy().flat))\n",
        "\n",
        "print('Original stride: ', tensor.stride())\n",
        "print('Reshaped stride: ', reshaped.stride())\n",
        "print('Viewed stride: ', view.stride())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.3146, 0.3772, 0.9019, 0.1887],\n",
            "         [0.0360, 0.2890, 0.8474, 0.8152],\n",
            "         [0.7927, 0.6679, 0.0178, 0.5702]],\n",
            "\n",
            "        [[0.4945, 0.7099, 0.8072, 0.1963],\n",
            "         [0.3364, 0.8708, 0.6680, 0.5012],\n",
            "         [0.5032, 0.9038, 0.2293, 0.3761]]])\n",
            "Pointer to data:  75874304\n",
            "Shape:  torch.Size([2, 3, 4])\n",
            "Reshaped tensor - pointer to data 75874304\n",
            "Reshaped tensor shape  torch.Size([24])\n",
            "Viewed tensor - pointer to data 75874304\n",
            "Viewed tensor shape  torch.Size([3, 2, 4])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b52c64ba3b57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# assert produces an error like tensor converted to python boolean, may stop the back propagation process etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreshaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUfeOS87Q-MZ",
        "colab_type": "code",
        "outputId": "791e51af-d108-4683-cc79-27cc9d905e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "tensor = torch.rand(2, 3, 4)\n",
        "print(tensor)\n",
        "print('Pointer to data: ', tensor.data_ptr()) # Returns the address of the first element of the tensor.\n",
        "print('Shape: ', tensor.shape)\n",
        "\n",
        "reshaped = tensor.reshape(24)\n",
        "\n",
        "view = tensor.view(3, 2, 4)\n",
        "print('Reshaped tensor - pointer to data', reshaped.data_ptr())\n",
        "print('Reshaped tensor shape ', reshaped.shape)\n",
        "\n",
        "print('Viewed tensor - pointer to data', view.data_ptr())\n",
        "print('Viewed tensor shape ', view.shape)\n",
        "print(view)\n",
        "\n",
        "# assert is used to make a warning like tensor converted to python boolean, may stop the back propagation process etc.\n",
        "assert tensor.data_ptr() == view.data_ptr() # checks if addresses are same\n",
        "\n",
        "# np.all requires all to be true. \n",
        "assert np.all(np.equal(tensor.numpy().flat, reshaped.numpy().flat))\n",
        "\n",
        "print('Original stride: ', tensor.stride())\n",
        "print('Reshaped stride: ', reshaped.stride())\n",
        "print('Viewed stride: ', view.stride())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0.8401, 0.6509, 0.6371, 0.0822],\n",
            "         [0.0443, 0.9864, 0.3638, 0.3530],\n",
            "         [0.5093, 0.4932, 0.4602, 0.0281]],\n",
            "\n",
            "        [[0.8114, 0.5088, 0.1052, 0.8060],\n",
            "         [0.8953, 0.7678, 0.3263, 0.2987],\n",
            "         [0.5925, 0.9870, 0.9818, 0.9691]]])\n",
            "Pointer to data:  75874432\n",
            "Shape:  torch.Size([2, 3, 4])\n",
            "Reshaped tensor - pointer to data 75874432\n",
            "Reshaped tensor shape  torch.Size([24])\n",
            "Viewed tensor - pointer to data 75874432\n",
            "Viewed tensor shape  torch.Size([3, 2, 4])\n",
            "tensor([[[0.8401, 0.6509, 0.6371, 0.0822],\n",
            "         [0.0443, 0.9864, 0.3638, 0.3530]],\n",
            "\n",
            "        [[0.5093, 0.4932, 0.4602, 0.0281],\n",
            "         [0.8114, 0.5088, 0.1052, 0.8060]],\n",
            "\n",
            "        [[0.8953, 0.7678, 0.3263, 0.2987],\n",
            "         [0.5925, 0.9870, 0.9818, 0.9691]]])\n",
            "Original stride:  (12, 4, 1)\n",
            "Reshaped stride:  (1,)\n",
            "Viewed stride:  (8, 4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIN5jSppm4yC",
        "colab_type": "text"
      },
      "source": [
        "The basic rule about reshaping the tensor is definitely that you cannot change the total number of elements in it, so the product of all tensor's dimensions should always be the same. It gives us the ability to avoid specifying one dimension when reshaping the tensor - Pytorch can calculate it for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3D19ERFmzOl",
        "colab_type": "code",
        "outputId": "23fb3572-d27c-4c7e-a4a0-fbbf22c7a5a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "print(tensor.reshape(3, 2, 4).shape)\n",
        "print(tensor.reshape(3, 2, -1).shape)\n",
        "print(tensor.reshape(3, -1, 4).shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObgCQKUiETak",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 9:\n",
        "\n",
        "Consider a tensor `a` created with [1, 2, 3] and [1, 2, 3] of size (2, 3) is reshaped with operation `.reshape(-1, 2)`. Also consider a tensor `b` created with [[2, 1]] and of size (1, 2), later operated with `view(2, -1)` operation. \n",
        "\n",
        "If we do a dot product of a and b (using `torch.mm`) and perform the sum of all the elements (using `torch.sum`) what do we get? (enter int value without any decimal point in the quiz)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "Ans : 18\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5etw90XdZLDc",
        "colab_type": "code",
        "outputId": "df207908-f31a-43f4-d01f-5c00e7ce8dff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "a = torch.tensor([[1,2,3],[1,2,3]])\n",
        "print(a.shape)\n",
        "a = a.reshape(-1,2)\n",
        "print(a)\n",
        "b = torch.tensor([[2,1]])\n",
        "b= b.view(2,-1)\n",
        "print(b)\n",
        "torch.sum(torch.mm(a,b))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 3])\n",
            "tensor([[1, 2],\n",
            "        [3, 1],\n",
            "        [2, 3]])\n",
            "tensor([[2],\n",
            "        [1]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mza7QPg3ndeV",
        "colab_type": "text"
      },
      "source": [
        "**Alternative ways to view tensors** - `expand` or `expand_as`.\n",
        "\n",
        "\n",
        "\n",
        "*   `expand` - requires the desired shape as an input\n",
        "*   `expand_as` - uses the shape of another tensor\n",
        "\n",
        "These operations \"repeat\" tensor's values along the specified axes without actually copying the data. \n",
        "\n",
        "As the documentation says, expand:\n",
        "\n",
        "\n",
        "> returns a new view of the self tensor with singleton dimensions expanded to a larger size. Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1. \n",
        "\n",
        "**Use case:**\n",
        "\n",
        "\n",
        "\n",
        "*   index multi-channel tensor with single-channel mask - imagine a color image with 3 channels (RGB) and binary mask for the area of interest on that image. We cannot index the image with this kind of mask directly since the dimensions are different, but we can use `expand_as` operation to create a view of the mask that has the same dimensions as the image we want to apply it to, but has not copied the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq1qtRj0yPMu",
        "colab_type": "code",
        "outputId": "854530a4-52bf-46dd-ce28-ace5a7c54e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "a = torch.zeros(size=(3, 4, 4), dtype=torch.int)\n",
        "print(a)\n",
        "(a[1,2:4-1,2:4-1]) = 7\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0]]], dtype=torch.int32)\n",
            "tensor([[[0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 7, 0],\n",
            "         [0, 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0],\n",
            "         [0, 0, 0, 0]]], dtype=torch.int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOeWgzfCB2A8",
        "colab_type": "text"
      },
      "source": [
        "torch.clamp(input, min, max, out=None) → Tensor\n",
        "\n",
        "Clamp all elements in input into the range [ min, max ] and return a resulting tensor:\n",
        "\n",
        "y_i = \\begin{cases} \\text{min} & \\text{if } x_i < \\text{min} \\\\ x_i & \\text{if } \\text{min} \\leq x_i \\leq \\text{max} \\\\ \\text{max} & \\text{if } x_i > \\text{max} \\end{cases}\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz33E-V7nPQT",
        "colab_type": "code",
        "outputId": "d143d90e-079e-4c63-e860-ed544521867a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Create a black image\n",
        "image = torch.zeros(size=(3, 256, 256), dtype=torch.int)\n",
        "\n",
        "# Leave the borders and make the rest of the image Green to produce rgb code rgb(0, 255, 0)\n",
        "image[1, 18:256 - 18, 18:256 - 18] = 255  \n",
        "\n",
        "# Create a mask of the same size\n",
        "mask = torch.zeros(size=(256, 256), dtype=torch.bool)\n",
        "\n",
        "# Assuming the green region in the original image is the Region of interest, change the mask to white for that area\n",
        "mask[18:256 - 18, 18:256 - 18] = 1\n",
        "\n",
        "# Create a view of the mask with the same dimensions as the original image\n",
        "mask_expanded = mask.expand_as(image)\n",
        "print(mask_expanded.shape)\n",
        "\n",
        "# to display an image it should be of format shape = (x,y,z) in comparison to what a tensor's shape is i.e (z,x,y)\n",
        "# so, perform transpose after converting to numpy array\n",
        "\n",
        "mask_np = mask_expanded.numpy().transpose(1, 2, 0) * 255 \n",
        "print(mask_np[1][1][1])\n",
        "image_np = image.numpy().transpose(1, 2, 0)\n",
        "print(image_np.shape)\n",
        "print(image_np[19][19][1])\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[0, mask] += 128\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[mask_expanded] += 128\n",
        "image.clamp_(0, 255)\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 256, 256])\n",
            "0\n",
            "(256, 256, 3)\n",
            "255\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiXBx0k3ptOI",
        "colab_type": "text"
      },
      "source": [
        "In the example above, one can also find a couple of useful tricks:\n",
        "\n",
        "\n",
        "*   `clamp` method and function is a Pytorch's analogue of NumPy's `clip` function\n",
        "*   many operations on tensors have in-place form, that does not return modified data, but change values in the tensor. The in-place version of the operation has trailing underscore according to Pytorch's naming convension - in the exmaple above it is `clamp_`\n",
        "*   tensors have the same indexing as Numpy's arrays - one can use `:` seperated range, negative indexes and so on.\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Images and their representations\n",
        "\n",
        "Now, let's discuss images, their representations and how different Python librarties work with them. \n",
        "\n",
        "Probably, the most well-known library for image loading and simple processing is [Pillow](https://pillow.readthedocs.io/en/stable/). \n",
        "\n",
        "However, many people in deep learning area stick with OpenCV for image loading and processing with some usage of another libraries when it is justified by performance/functionality. This is because OpenCV is in general much faster than the other libraries. Here you can find a couple of benchmarks: \n",
        "\n",
        "*   https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading\n",
        "*   https://github.com/albumentations-team/albumentations#benchmarking-results\n",
        "\n",
        "To sum up the benchmarks above, there are two most common image formats, PNG and JPEGs. If your data is in PNG format - use OpenCV to read it. If it is in JPEG - use libturbojpeg. For image processing, use OpenCV if possible. _We will be using PIL a lot along with these._\n",
        "\n",
        "As you will read the code from others, you may find out that some of them use Pillow/something else to read data. You should know, that color image representations in OpenCV and other libraries are different - OpenCV uses \"BGR\" channel order, while others use \"RGB\" one. \n",
        "\n",
        "To change \"BRG\" <-> \"RGB\" the only thing we need to do it to change channel order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYv4sZMmpndu",
        "colab_type": "code",
        "outputId": "b9173c4b-8010-44bc-c1f6-1aae64923c2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# import the necessary packages\n",
        "import numpy as np\n",
        "from urllib.request import urlopen\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# METHOD #1: OpenCV, NumPy, and urllib\n",
        "def url_to_image(url):\n",
        "\t# download the image, convert it to a NumPy array, and then read\n",
        "\t# it into OpenCV format\n",
        "\tresp = urlopen(url)\n",
        "\timage = np.asarray(bytearray(resp.read()), dtype=\"uint8\")\n",
        "\timage = cv2.imdecode(image, cv2.IMREAD_COLOR)\n",
        " \n",
        "\t# return the image\n",
        "\treturn image\n",
        "\n",
        "\n",
        "bgr_image = url_to_image('https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRCA40ftnscVzfV8ft8e7vIzQXfXeZdtco8nknJrfCUW6INI40U')\n",
        "# bgr_image = cv2.imread('https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRZpTmijaNOH6MmycM_eiPKcEl5mVvbwl7a8YKVGpEEMIanDcSt') \n",
        "# remember to add your own image in case you run this block, if you want to use the same image, \n",
        "# download it from: https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRCA40ftnscVzfV8ft8e7vIzQXfXeZdtco8nknJrfCUW6INI40U\n",
        "rgb_image = bgr_image[..., ::-1]\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(bgr_image)\n",
        "ax[1].imshow(rgb_image) # original image\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAACHCAYAAADtJRlTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nOy9e6hl2X3f+fmttfbzPO6z3lXdXaXu\nlrrlILnttgbjmHFwgjxO4kSg4BkFh2RiGYaImBCCgyGCOJMMGIKZBMbjAWMFgvxXIMZM4sRBVmyc\nBMkGO0or3Wq1St1ddW/Vfd+zz9mP9Zo/1ql2SdPqbqnrcat1v3Cos3cd9t73nO/+7d/6re/vuyTG\nyClOcYpTnOK9BfWwL+AUpzjFKU5x73Ea3E9xilOc4j2I0+B+ilOc4hTvQZwG91Oc4hSneA/iNLif\n4hSnOMV7EKfB/RSnOMUp3oO4L8FdRD4qIi+KyMsi8nP34xynOMXDwCm3T/GoQO61zl1ENPAS8GeB\n14EvAP9zjPGFe3qiU5ziAeOU26d4lHA/MvcfAF6OMb4SYxyAXwd+4j6c5xSneNA45fYpHhmY+3DM\nS8Brd22/Dnzkmz8kIp8EPrnc/D4RuQ+XAhG4V0e+l8f69s8r37DNt9h6UNd356wP4/v4dhFjJMZ4\nLy71RHH7vcJu+Ybzxjd5dwen7P5mvBW370dwf0eIMf4K8CsASqlo8uK+nMd7j1KKd3uDiQghhHd9\nnO8EeVAgngGLIkdJRBMxeFoyIA3BVAQQXNS4qFF6uG/XdOd7eBTsK9zQP9DzfTO3i/z+3GbvBW6r\nkOMFLAM5iiiKiMZjyGjvfAqiQgAdHTo6Bn3/tCCPErf7wX3L/7sfrLsBXLlr+/Jy3wNHjBGt9T07\n1oMm/53zeTwWg/cZlZ5jjGLRezAFWTQoHzAh0odAX+XgWkQHCAal7s9N8DCJ/zB+iyVOuX2PcDe3\nDZbMe+a6QhmD7xcUBkzMCF4RgyGEnrzqaR0ELZjAKbffBvcjuH8BeEpErpKI/5PA/3IfzvO2UEqd\noKfvctgbFYpAQOGFZY4SccvsBMCII0SFixkhKqKfQKzQTz1HdeV5xmevcX7lLCubl5nFHh0js53b\ntIdbZK/9Hs1X/ytx56t40+AJ6DiQi4NgCNpBUISQEcxAlIj4DMQDp9rYt8Ept98Edwo6KkIg8Rvx\n+OX4UuGWo0pwYlAxkEWHioGJj1QRnntK8/yVimtnx5xdOc/lzRX6OCNGze2dGVuHLb/3WsZ//WrD\nV3cijfEEPEPUOMkxAZwOqABZCAwmECWSecG/ESu/u9h9z9UyACLyPwG/BGjgV2OM//tbff5+lmW+\nGTHGh3JjKCLEZaalBlRUKPF0ocR4zVpp2Z5PgEvw0Z9n7QPP0WePEYymGHvmMeIygymgyGHeeogR\npdLzWRQEgRghy8Fby6rK8BHOjGH3+g0O/8Xfga9/njKHQd8CWyGiUNkcFTQhGEAIUaEkPNDv514h\nhHTdd2d1bugJIdyTdOg74fb9Kst8Mx4WtyMKvTzloEBFhRdFGTq0N9hyjcl8m0vAz38UnvvAGo9l\nPdoE/Lggxjkmc1AYyAt8OydGMHd+QyUgIZE7z7DWk6lViB7GZ7hxfZe/8y8O+fzXgbzklh6oLCgR\n5plCB4UJYfkACgR5NIP8m3G7H9y35PZ9Ce7fLh5kcH8zPIgam4oKCPTRIZkmp6DAcryoYfQca3/5\nF2DzCQ5azaQcCOMp83ycQogBXITcgw6QgVR5SpccxMGl9yJkhWZUweHNHhwQDBw0IIaNayP2csvF\nGw1bv/MZ4uf+ORtc50BFyqIiuAVBl9iYYbh/9foHjXsZ3L9dPMjg/mZ4UNwOgIs9OhMKciwF9eKY\n50bwC395jSc2QbcHDOWE6TgwzudvcDs68DkEDWSQV/IGt90QQUAEdJFBNaK/eQgOTIDmAIzA6NoG\nNt+juXGRz/zOFv/8c5HrbBDVAVVRsnCBUgeyaBke3lTjPcdpcH8b3PkO7mfd0QRFkEAfFaJWiX3G\nxo/8LO0zf4VF0ExXM2aqJNY19KSAPnJIYYl9gSjItCKWIAbCnWGuAgR0DkYBHlwDNDAuoNttERWJ\nuaI/X6AzYTpboI9uczy5wPDiF+GXfxaGG8A2ZVlgYxpKv1fw3RzcHwS3VTAECajYs6qErI/87I9s\n8FeeadFhQbY6pVQz6jq+wW03AlsIRR9BCUpnUMYUqeNy1KiWSUuuQRnwQOOgAYox7W5HVILKI8X5\nHsk0i9mU20eaC5NjvvjiwM/+MtwYYBsoyhITbSqBvkfwngruD2vo+U4QvUIUb9SviTrVz5Ump2XW\nrsO1n2b6oY9zvHaBbGUDl0UkU4jOiTEg2uOLDFTic/BA7VNB0yjoAoQAuUKVhoAnMxrbBYxSuAFy\nC8OhJ1/XDG5gspkTIgyAdBHXWOrDluY//j7lxhouOlYffx/hy/+G/X/781w907J9OKdTNRIcQQeI\nCk3AEAmPgETsbjwqwf0kc1v5FITv1K91TPVzrQItOevtjJ++Bh//0JQLa8dsrGTEzKEyIddCiBGv\nhazwqfRtFPiAr9NUkzIQukRtlYMpVZovMhmhs6n8ODiwOf5wQK/nDG4g35wsHwYDsRNs42gPa37/\nPzasbZS46Hjf46v8my8Hfv7f7tOeucr8cJtadbggBB2WcwVpfkB4tMqRp8H9AeGbZ7qDJMWKHwq4\n/HEuff/HuBkvEFfOQQHl5jk61UEZUr0xGgRFzJYBHIU2Cl8HTKUICoKNZEqwAxBB6bRPeSHX0B1b\nRipjfvMIsgqqHAjpZtJAH6iMohw8B0WHrkdsvPAKt7/0h6w+/WHmWy9zYT3y6md/hpVym1mEQiJE\nhUMIyCOX95wG93ePN+O2CVAMno9fho99/yUuxJucW4lQwLnNkk51hBJMASaCQghZTAEcUEYTao+q\nDKhAtAFRGQw2zdJqlfZ5BTrHHndkasTRzTlVBnkFgfRgQEPoQZkKP5R0xQGjWvPKCxv84Zdu8+Gn\nV3l5a05cv8DPfPZVtssViDOiFKgIsmT3ozbp+p4K7icaykHQGFUSg0K0pt388xTPfpy+vgqtpXr8\nMm2cIcU6MTfoyxX5KrSHliXl0SrpjotCoTXMfcp2TKkpFMz3epBs+Vnw8wGRnCipJF9EWNzeg6pH\n6ikRYbI6YrZ1QD2qWexsw+1jqCtwA/lzzzJ87otweYMrz17ltd/4V/DKFh/8wWf4b7/6k5BpSg4Q\nFWkxZI9YdvOoBPeTDKcSt0plUCGitfDnN1s+/mzB1brHtnD58YpZbFkvBJNHqssaVnPsYbtkNojS\nhBBQRQFa4/0cFOjSgCro9+ZksgyxSjPMPbkISExF+Viwd3tBX8G0FoTIaHXCwdaMelSzvbPg+DZU\ndUr0n30u54ufG9i4DFefvcK/+o3X2HoFnvnBD/KTv/rf0BkcUBKVYGgJy76RRwWnwf1BQfcQMnSs\nUWJY/aF/xlb2NJRrmHMGGxRiRohRSFHgo4MnR+A9xmpc25KvVAyLgNKKECLEmEo9CqIEaBSIohxB\n11mImqpStMcBoqSa5TDAzi7V5XXaZkBVY8LeEZJVxG4OwxyGBtYmbJw7Q/P7X2D9L/5poob9L92m\nHjIk9hz87r9j+r7HmLzyGXa//JuoeEgfDfdGXf3gcBrc3z16DVmAOmqMKP7ZD63ydLbFWgnmnEEF\ny8gIyghFIbjoGT0J3oO2hrZ1VCs5YTGgtCKGQIwkJYyCIBHVpE1GJbbr0BFUVRGOWySmuaZhgN0d\nWL9cMTQt40pxtBeoMmHeReYDNANM1uDMuQ2+8PsNf/ovroOO3P7SPtlQ00fh3/3uAY+9b8pnXpnw\nm1/e5TAqTOzhEWP3eyq4n7Shq0ch0aAJDGFB6afEH/wlZP0yfX6Joq7pJGDOX8A1c4wxuNzAuEyF\nyxrQmkyBnQckqtT8rAFFmgy9q7tYhTQURVnK8xkxQv+1FiUVoe/ABvLMMBwfgmSsbaxxcGsXuh7a\nIxBBGYPYJJekyKGuMJnC7WzB6gjWxjz54Ut87cUZ8fOfo7z+Au1Hvo/463+favQHeDdi7CNt1uNC\nynTu9cTdm8m+vlM8KsH9pHFb4TFRCGgWYWDqS37pByOX14VLeU9dFwTpuHDeMG8cxhhM7ijHS9Vv\nDVoDKiPMLSoKEN/gdlSC3P23hqQoswqy8yXESPu1nkoUXR8IFkyWc3g8kAmsbayxe+uAvoOjNilq\njFFgBWs9eZEyeJUZtnYco1UYr8GlDz/J7MWv8bnPR164XvJ9H2n5+78e+YNRxch5oh/TZy1ZSKKC\nk8zt0+B+n+BRhCiMdU+HwbVTxn/mMzT5JTi/Bt7B/gFsrENZUY7HDJkmZobCaLyGsJSARYDBLxuZ\nluNSk4I7S1IpBUTwClAd6lJJfQCLmzHdPCoStEr19VdvUK9vsLh+fSl8D+C7dCzvoPfL40aoKnCQ\nlSVSlQz9gunFixxHx+paxeHnf5vquKWTGfGF30Y3v0UsDhFbo5QF7r3k7jS4P1woPBIDvR5j6Ji2\njs/8mTGX8oa18+A8HOzD+gZUJYzHJTobMFlEmwK0hzykWg4RP7CsbfMGt6MS3oiXSqWbQHk6BeUl\nBQc18eYCOw9EpVA6oAzceBU21muuX1+QZ0l00C3p7Dz4Pr2PJGrjoCwzykpY9AMXL05x8ZhqbZXf\n/vwh7XHFTDp++4XIbzWawyJSW8EuuXeSuf1Wwf3Rmj0gfdEngfwAQqDUA01b4aqfgL/wu/TXLsHT\n72NDenAOvbFBuboCeU5vFCEGVG/pvMVKQCqIhtSgIZKIpAGd+H6HWMakIa7Pk86dWBBeW9Bc7yit\noDuL8QE9W8DXX2f69GP0+zup0cP20LfQ9tD1XHnq6fQAUOm8ufeYDOxizrBooawZbu/C4DkcF3Bm\nk7aZE4tV9If/Oh/8e/+Z0F25q/PvDQOje/bdKqXuW3v5ScVJ4nZAGHRJ1Tb8ROX43b8Al671vO9p\n6GUD52BjQ7OyWpLnoExPiAHbK6zvCGKhEjDxDrW/gdvc7YmzJHfIPWTLOaPXAt31BrElttMEb1jM\nNK9/HR57esrOfo+P0NtE676FvoOnn7qSJlhVuqW8zyEzzBeWdjFQl7B7e8APUIwP2TwD86ZltYj8\n9Q9r/vPf+yBXuvAnijceXW6fqLtH7gS3t9l3B3eegA8SVidrgBAVaEtnM8798D+m/IFPwbSF0UXY\n/Rp7TpJ0rM7pdIRKEQ3oSYWslbARYFPhFWgfyZ2khqQYQKeOUyAl1iOw3oPymKiQAlQvGGroI4t5\nC94xHM05d2YF2o5m/wiPBz9ApiBX5JMx2XjMay++mJ4UkVTEFMHPG2gbaI4wx0fYwcHWq1Ql8Mz3\nwrWrTDbO4jdr/viG5tKnfw8GwWaBzE8IpsM+xM6/kxQY3wyPAredtqgYUDFgNWS24x//8Dk+9QMl\n7RQujuBruyBuD1GQ156oO1QFmEg10ZRrQtgAtQkoT/QacTk4CPFOSWb5N0dgVOG9xStQ0UAhSK+o\nMcQe2vkC52F+NLBy5hxdC0f7DR7P4EFlSTo5nuSMxxkvvvga3qdjL6lNM/c0LRw1cHRscIPl1S2g\nrPjeZ+DqNTi7MaHe9Ogbf8zvffoSMkDILBOf0ZmAEvvAf487+E65faLKMm82/HmrelcI4YFndz5q\nKhxOoPcjznz4U+xMfxgmm6hLm4SuRzmPFiGMxvjpCLJlKlFpmCooIC8FN4Ow41FtJGQKymU2rcKd\nVAcAnaVYzDKZqK7B8PVIECGbgW0HmLdEa8G1MKpgaKFvqdc2WNzcxojCNfM0HFAKnEOJQmuN955g\n7XK/BW3QK2uUZ9aYX1yBQZMPluGrrzAqMuZBw+WrrI4dh79wlXH/Oo2aUohL8s+HgLdyNjwJZZlH\ngds6ehwViGPkez714TP88HSHzQlsXlL0XcA7hYhmPAqMpmnEpwBdgZoCBUiZw8zhdwKxVagsoErS\nxKn6BmpDppdD0uX2tYr49QGRALOMobW0c7A20rqU6LRDytY31mq2by5Qkmr+d1H7G7htbUApsA6M\nhrUVzdqZkpWLc/QAdsh55asDWTFChzlXL4Mbr3L1Fw55vR8zVQ1OihPJ7UemLPNmw5+3ym4exrBd\nRUMURe82uPz832LPPA/1OvrCOULXg7UEIlKV+EkBoxwzKtBFDjLAVKhKYbjRE242sNMQencX2/nG\n96RSOQPJgqCNRAUxC8Q2MHQD0TmisAzcmnw0gjyDLKdrW5g1xH5Iw1+lkoZehEDECUSj08yXgEwm\nqMkIkcj89g5rklHXiqEokCeuIZuPw5mLVEWgEQV/++s06jKr6hirPO8WIYRvO2u9w5u7eXISkpa7\n8Shw20SFksiG6/lbz1/mebPHeg3nLmj6LmAtRAJlJRQTTz6CYmTIC80gIFOQsqK/MdDcDDQ74Prw\nVtROxfIhWRDEFlAxaeHbyNANOBdBIkqBVjAa5WR5onfbdjQzGPr4zdQmEkAc2sQ0qSswmQijiSKK\nsHN7TiZrqLqmKAauPSE8vilcPAOhqFDS8PW/DZdVw7Faxat3n7k/aG6fqOD+KMBrxyIUjK/9OK+3\n30tcX2F0dZPYd3B4jOQ5bKziL2zCygg1zXESMCMozlUw97RfcfD6gN53oAVdZ4iSVGN/k19ESUDE\nYzJBi9B1gXpNU5YqqV3ybMno9KMP8wZ1ZhOlDWFnD1lfS/XxqsCsTKCukLJAFUWSpIWAKXLIMsbT\nCZvnNtMNhebgxVfJDalcMy5oNjOKMwVtr3DWUa4qsn/03zlUzxPiu/dN/26stZ8UOO0pwoIfvzbm\ne9vXWVmPbF4d0fWR40PIc2F1AzYveEYrkE8VQRyMDNW5Aj8H95WW4XVw+xrRkNUaUbJMPN5khCIK\nL4JkBhFN6Dr0Wo0qS/IiaQHuojbNfGDzjMJoxd5OYG1dQDxFBZMVQ1VDUQpFkaTEIUTywpBlMJmO\n2Ty3mfpBULz64gGYnKMGirEh22wozhSovsVZh1ot+e//KON5dUgf333W/qC5faLKMicROQHnSqwZ\nkJAhKnL2Q/+Am/1jZFevYVdWoFkks5f1VdSZNeT8Kj70CBl5rvBNxHUtLDpWL65ztDcQC48qC9Bp\nEQI/BPRyAQKfpSYRr0ndei4DInkp2KUSoCignUV0FPzxPI13FwG6OeDI3n+RuNXibm2RVxmD7ZKF\ngNaICK4fkizT2jSRGwLRupT6eM8TTz/Nje0tbHRw5QzaKvyiQz1xnnIE/R7EAUIGSnfgS8Knn0DF\nm4SYMS175qE4Ee6SJ6EscxIRyCmdYzCWLAhRCf/gQ2d5rL/JtasZKyuWRZO6+1fXYe2MYvW80AdP\nhqDynNh42s7RLWD94irD3hG+iBSlQmkAIQwetfSej5lHtAbtsQNkbmkZXOZpdnRJ7jhrkaiZH/sk\n/13AvEteeBffn9FuRbZuObIqp7MDKvIGt4c+yTKtdYhACIKz8Q61efrpJ9javoGLljNXQFlNt/Cc\nf0LBqIS9HoYIWaDTitLDE58O3IyKLAb6ckoR5ifCXfI9JYW8G/dSUvSt4FFkXpPlM+wwoT/7MdQT\nHyVsPImpc6SssbMZlAU8cxG1PiUYAKHqhPaoAVPB1jGjC2vM2wNYH6Efy6GH2EFoI9g/aesPKkL0\nSG1ApyErQF7C0IRkvjTc0X+VST2zsBgr+OCIMrDygSmzG4G46IjdApxDrCf2PaosUSHivU9DPOeQ\nEIk+2QgnPZlP4+CNVbKrF8iPe2Ln0NOaWeGSU1lroS8w+YAzOZU+ov2572Gc79KEnEp1J8Kk6VEM\n7g+C2wqP9hmzPGMyWD52tuejTyie3AjktaEuhdnMUpRw8RmYriswyVlIuormqKUycLwFaxdGHLRz\nRuuQP6YTR7tIbANpLjL9HVEFfARTL5Uzbhl/ypzQDNAnDYDvoBQgA7sAsQYXPINEph9YIdyY0S0i\niy7iHHgr9H2kLBUxqDe47RzEIHifVDveJWorDasbcOFqRn+c47pIPdW4YkauwbZQ9DDkhtw4jnTF\n9/xcy24+Jg8NnapOhLneu6q5i8ivishtEfnSXfvWReTfi8hXlv+uLfeLiPyfIvKyiPyxiDx37/6M\nN7n4BzDMCRJQytLYnOF9nyD/4E8RrjwJxuH2D7Hbt1EI1cXzcGFKuaYQG9GHA+3OfhLaGo06s8a8\nWcDmCowMvvN4FwlEdC0UUwh5RI2gmghUhpgN5JOInAF9HoaRhTFQkAKvySG45CCZC84Isc5hs+Bo\nsORrCnO+BhXRq2vETMHKOE1qjWtibhIDckPMM6RMLeG6rsAYVJ5DCMj2LvOb2yxubFG2jmIBiKJY\nKSBrCVoYa0c/TLn4i1+hkYKx7unjyW7lPuV2wCpFbhs+8b6Bn/pgzpNXAs7A4b7j9rZFUJy/WDG9\nAGqtJFphONTs77RUFWiTMvpFM2dlE8wIfOeJzieLuVrDtCDmAUYKmVSYCoYsEic5nBE4r7Gj4Q1u\nKw25ARdIvR45iHHkdaTYBDscodZy6vOGqGBtVaOyyHgFUIF6nOwPWN4iWR4pSkFrqGqNMZDnihBg\nd1vYvjln68YC15awKFACxUpBm4HogNNjpkPPV37xIoU09HpMdg9KkPcb74Q9vwZ89Jv2/RzwH2KM\nTwH/YbkN8GPAU8vXJ4H/695c5sNCWqtUKQt2lfjEjxFHk6Qdx0ObmoJ0UdB6C0Ngsd0TX9tFLyzF\neMporJNFr/EwyVPt0QvZsUY1kjIVG+mbIQXoGGmPhnQODVIKKgdyyKYZepzUNlJpskyhCgO2o143\nqFpDreBckdL9kEybMAZdFFx46gJqNKI4t4mNPnWUTMYwGSNFjipyKHJ8SFl7FAEfGK7fpMhyipUV\nXG/pX74BXcBEqHpF2FU0rRAq4ebCUP6lXyMGyPCEqDAn14vm1/gu5XYksdsqxaqFH3siMhlFfEzC\nla69UyHRWN8SBui3F+y+FrELzXRcoMcjMII3gXyy7MvwoI8zpFFpZGo9Q9MjOcToGI5afOQNbpOr\nN7itxhoK0JWgsgxTKDoLZr1G1wpVQ3FumewHoDAYk67xwlMXGI0Um+cKfLQgMJ6kV14IeaHIC/DB\nozSIRIKHm9cH8qxgZaXA9o4bL/eEDogG1Veo3YC0DVIFzOImv/aXSggRT5YW/jjB3vDvqCwjIk8A\nvxlj/J7l9ovA/xhj3BKRC8DvxBjfLyL/9/L9Z7/5c291/IdVc//WcrO7W6J7rDvH6k/8U5rRk7je\nQucg5pTlhK5fUDx9DbcyxmsHeweMNjeZSyTLa2zskWmebARcQBuD6wP0EVNoRMDO5sjGKEkeNclP\nJiwfCKO49BtwqcB9aNOsa8yp5kJfQdgNmELhcOhNg5dAPVMsZn3KwI0mNPNlPT2SqQI7m6HqmuCX\nCpfgwXoYPIIQmxnj0Yhmbx+lMlCKMPQ89dwzzArYbg5Q0ynhRp9KOUUGMaLLgsiCyRd+Hf9bn8RH\n/Y5KM/ezDPFWZZkHwe2HUXP/Vty++27vA5xzln/6E6s8OWqwvcN1kEeYlCWLvuPa0wXjFYfTnoM9\n2NwcEWVOnWf00ZJPU60+uIgxmtA7Yg+6MCDCfGYZbUiSPGqfJkZDeiDEERBSHT0LYA/TnGseQeYV\nVD1hN6AKg8NhNjVBPGpW088WGAPaKOZNIISUDxUqYzaz1LXC+8QpH8DbVO4RhFkTGY3G7O81ZEqh\nFPRD4JnnnoJixkGzzXSq6G8k/5usWFK81CyI/PoXJnzytzw6+ndUmrmf3L4fUshzd5F6Gzi3fH8J\neO2uz72+3Pf/g4h8UkS+KCJffFh1/7f+sgWJGT6fcuVHPs2svIo7asBFNusCFsf43EBusLkhxkhx\n2GCynPnsGLTCSgcjQcZCXIvEKTgiLAYyD27WYxdpoevY9biQhrkxBgiCeAOdQusMYgVi0EVF5gvM\nQmj7gCgLPvm4k7UEAQahnwOjAj3ShOAoz4/Qj01Q61O81+TTKSoXwKaXVphRjaqqpFwoC5pmhh6P\n0EVOWK6U+ZU/eokhB+qM4FMkkNKkLvOFJ8xhulLTfPhv0HQrSLRYX6Wmr7f5LU6ISuY9z20BsihM\nc8+nf+QKV8sZzZEjOijqTY4XYHKPycHklhgjzWFBnhmOZ3OUhk4sMuINbjONRBzDAvAZ/czhF5bC\nQN/FZflQE5aJi/GC6iDTmmq5RkdVaAqfIQtD6FusEpQHBkebARKQAZj3FCPQI40LgdH5ksljmum6\nQnvPdJojubrDbJSGemSoKoXJhKIUZk3DaKzJC/3GCgUv/dFXIB/Iauh8IOZgSoGg8QtgHqhXpvyN\nDzesdA02CpVPjV9v91s8FNn2uz1ATOz9thkcY/yVGOP3xxi//yGtZP+tsfQvj0Gx8uyneG3zydTB\n6RfUQbH7xf8CO9vYpmVy/jyh6wiHR/TO4oKDtZWkhMk9TA0hD8SgIRgKK+QuEuc9Jijolz4vfYbM\nwO+D6jRyDLIL6kDjrwOverI24Pc67E7E7fjUAXuYQT1QPQYUJbEJsPD4ukey1MFH9HjAD0kyFjLH\n4NokwxxVqLrE1BVBCSHThMwgeQZVhU+NswlKYdDs/8HrnD8zBikxGyOkzpfD3YKMPQ5fnLPijvjQ\nL77CIqxS6MWJUM18u3gvcjtK8nhRIfKpZ1d4cvM1mrln4UGFmv/yxV22d6BtLOfPT+i6wNFhwLoe\nFxwra1CUCp+DmULIAzpETACxBdHl9POICgbXJ5+XrAdmAvse3Sk4FtgV9IGC6x7/KoQ2o9vzxB2L\n33Fp5bFDz1ADj1WUBYQm4hfQ1x4ygUylEg8eP3ioClwWaN2AaKhGmrJWVLVBVEBnAZMFslyS54x4\n7rBbKdAYXv+DfcZnzlMKjDYMeS344Cm0Yo+M+YuHHLkVXvnFD7EaFix0cSJUM2+G7/Sqbi2HrCz/\nvb3cfwO4ctfnLi/3PVIwESwZ1nkOrv0o1RBYGWfIyLB44Q8Zn9kkO3sRUMzsgJpWaaWAaNPiGG7A\naEGmIwSF8irx6Kin39rHLxqit2mFo0mOWcmpK0VsI/EQws4C75ZDzQWUA5SDZuoVzHRSyShB9wLR\nUaic9vUI8wwVVGpIMoZoPRNXzdoAACAASURBVHYxoNYLLCljansglIjPcLMB3wWi1ygFWWnSmBhL\nJEdJAVaIKBQGHXWSS9qB7RdmFKWmEggEigsVQXcMoQTdsn9rnz/aXYFnP5Yiis8f7o/6zvGe5jbR\nkGHxzvKj1w4IQ0U2XsGMhD98YcHmmTEXz2YoYLAzqqmiD2BjWhxjcCDaMJoKCkH5NIfUH8H+Vk+z\n8FgfCTrV4fMVg6pqYhvhMLLYCQTnU4lwEWEo0UOJ8lP0bOltp0B6nZYNVgXx9ZZsDiqoO9TG28iw\nsBTriohNq8P3LWWAzAvDzBE6j/YxJSVlRsxTJp8TKUQhFhQRg0JHjbORwcLshW10WYBUBALVhYJO\nB8ow0GrYv7XPyu4f8bFnUx6Yv/vevfuC7zS4/wbw15bv/xrwr+/a/1NLZcH/ABy9XU3yJKINJWPV\ncObH/wmZC7TBc2Q7zCuvc360SqMKbFWzcnYTVRaEPplYqPEYlEIZwzD8yQLTIUDcb2EB2CTL8rki\nGA0u4rrI4thB04KPlLGG3R62ZsSthn6WJrhmN4FjC/MWeos/XGAw9LeOkZ4kP2tJN80CKDT5Wk6I\nEVULJmroI2romfQBtvbg1iGyc8Rw0NDf2ofDOZD9SfekTpaVQcAvX/XGOuP1CQRY9OlG7Y8GismI\nzAkcHIONjJtj+NF/iHVjjFk8jJ/yO8F7mttlaGnUmH/y42cILsOHls4e8forhtXReQrVUFeWzbMr\nFKViWHaYjsepNm2M+gZuEwLtfuKbWIjeo3KPNiF1nXYOd7ygbVJNvI4l/S7MtqDZijDrkzDh5gx7\nTLIa6GFx6DEYjm/10Msb3I4LYAG6gHxtuTRlrdAxedH0gyL0E/a24PAWHO0IzcHA/q2e+WFSDd/h\nttYsRfZLozDxrG/UTNbHEMD3C0KA4ahnNCkQl3F8kHK442bMP/xRGDvLwpzMSdV3IoX8LPCfgPeL\nyOsi8r8C/wfwZ0XkK8CPLrcB/l/gFeBl4P8B/rf7ctXvEN9pvbMwmia+n738abx4yAx6MsHe3mF7\n9zbl40/CaMIss4TcoGNEZRl62ahhjEFrfZd3CEmknhn02SnqsVX04xNk0zDZVGRawSDosoYYGBYW\nZgswGWhDrmB9E+JxpBJDpjRYhxjDWg7MHbEZ4GggLECsgkGjB4gNsAiE44hbAK0nHB4y3z9ID4l5\nS4YmD5o8qxjVU7IQCd7juy4ZdYQAMZKNR1RnN8lXJvRzS1aAP9jHKA2DReVgmwUozfTsOZrFATRj\n5PmfRp3AzP27kdvaFLw/Njyd7+ElecNMJpqd25bbu9s8+XjJZAQ2m2HyQIxJlfWtuI0IZZ6oOj2r\nWX1MMXlcYzYFtTlB6QwZoC41IYJdDCxmyW7JaJLr1+Y68ThipEKrDGfBGIF8DTeHoYkMR8AioKyg\nB2DQ0ETCAuJxgIXDt3B4GDjYn9PO04NCk6FDTpXlTOsRMWR4H+g6fze1GY0zNs9WTFZy7LyHImP/\nwKOVSUta5opFY9EKzp2dcrBoGDfw088LuT+ZZZlHuonp7fCduqlNlIc/91n21QhyjfJQjCrsjS3U\nk8+gjzRhXDOcHROtQNuQjyq8Ae8dTMZUaxPaKiJGiBJhCKig0RnYClQWUlfpviPsD9BU5KWQF9Ds\nd5RVSQiRYTYDZ8kurWO/doT4QDSSlsjzDmYN5GNMWeGUoEpF1KkSIhJRWogGIhHxQhlh8coNmDWY\n8+dw83ka57qlcRigyxIxWTIa8x4TwcfIZGWFhR9wOjI+s0lTWupJRr/rEBdw7QC+hKNDVGkwt3cY\n1kdc8I6tX/4+sqK51z/xO8Kj2MT0dvhOue3VhM/+ORipfXQOeEU1Kti6YXnmSYU+0tTjwPjsgNhI\n00I1ysF4nPeMJzBZq4hV+wa3wwA6qKSIqSwhU6gAbh+G/UDVLDtQi5xuv6GsSmIIzGYD1sH6pYyj\nr1mCF8REqjr10TUzGOdQlQZRDlWqtMCNRKIIolWyFF5ym1hy45UFzQzOnTfM5w5jkmHYnfnMstRk\nRpg3bukeaYjRs7IyYfALonZsnhljy4ZsUuN2e4IThtZRejg8Sot379w2jNYHnL/A9/3yFk3xcHo6\nHhnjsHuNd07+9Dkf0sz4frzGvlVkeQ7eU08mhKMev3EF9Jg2c9gS8BFjDGQlgy4QC6YYgyloPSgl\noJZ6chGCCdictJhvUIRdR9jTrOQ10ONjZL7wSBRsZ7HdADqjzMbYrS0woMcVarOEsA9xQI0nSate\n6bRgsAZyoFiWuo0jjAJx1RPKSGsixdVLVM8+jdMkd0iVUYzHaKUZb24gVYWzHqlqBI2PEULg+OYW\nbvcA+kDz+i2yRcbieou/fhv35VdhPsDOLeTMGkFnhHPvI+tbtljjQz/zL8kEBE1OwIeT3eB00vFO\nuX3nUxI8OsC1uI+y++R5hvcwmdT0R4ErG56xBpe1UFqiT1l6mUGhB7DCuDAUBvAtopJdNUuHx2AC\n5DatQxACbjeg9wJ1vkIPxOjxi/kb3B46S6ZhnJVsbVkwUI015aZiP6Tu/8lYMZ4kx0mqPC38seQ2\nEnHGE0YBvxqJZSCalktXC55+tgLtcA4ypRiPC7TSbGyOqSrBW0ddCRohRk8IsHXzmINdR+jh1usN\n2SKjvb7g9nXPq192DHO4tQNrZ4RMB953LtD2GWts8S9/5kMgGRohkJOFk1GEf08H93cKi06ruauB\noDxnn/8EtAHre6SucZ2lbzqyssYNA+Q5+agmakGKPI0vD49wPqSZcxvQFnAgYbkkjFaIVkiEzEMx\nB3oFnWOxIOnIQyB6TzQKLxFTFUDA46AYwajAz3cZb1ZJfJtXVOsjyD1OeSwhrXCDT01Macn59BBS\nBgLEEIk5xEqQqoTNDYJWOASvNc3hMe7gEOMCJs+JZU6e58lcLEkMWJtMMeMx9tUblCZLWX9uWNWG\nSYSR91x4fIRTLdHU0Fq+5C6z6MaMpCcgydL1FPcdGkuQwKAMXgU+8fxZQgu9t9S1YDtH1/TUZcYw\nOPIc6lGO6EheCNrA0SEE71CSlrrD6m/gttKgtCyziQzmBaoH1wGLxdKtMeB9RJlIFE9RpdY2h2dU\nJHrvzj3V5pgiS7F8tF7hc/DKEUgPAE/ExeUCNyrV8e/mNnlEqkhZCRuboHRAcGjtOT5sODxwBGfI\nc0NeRvI8J4RIVRkEmE7WGI8NN161ZKZMNsE5GL0KcYL3I0aPX6BVjtrEtDC4+xLjbkEvI4RAOCEK\nqdPgDqgolASgxOsr7KqLsDZisloTm55u7xiyAqcyovOIUnTL8otVAnmOjMdQj1FFjQ6avIuELt0A\nOp2EKCnp8MfQv9zBnkeCJD92/kSbnEULwwI9tDC0iPFcOr8CfUO133DctLD9GuiWeQgwERgrqAyI\npZhq8nUDlQItEARtlzejUgze0QWIVYGMKqgrfFBQjRFdIMHgltrdPMvx3icr4xBgGDjYvoXvh3Tt\nzRGTpx6H9RGHr16nPdyneelF9m7MqDKNCzmYBu+m8JG/i0PoQ04uD9+X47sBEhWBkhK4oj0X1S6j\nNahXJ/RN5Hivo8ggUw7vIkoJzneMJyDKkucwHgvjGupCoYMmdjl0IfEJvVwZMgIFHHu6l3v8XuLb\n0Car3DvctjFjMUA7aNoBvBFWzl+i6aHZr2ibY17bhlZDCHNkAmqc7JmsgJ4WmPUcVYHoNBeK1UhI\nrqrODxA6iipSjSStoRo84woKLZgghKVZ0x1uW5sePsMAt7YPGPp0Xx41lsefmjBah+uvHrJ/2PLi\nSw2zG3vorCIPjsbA1Hn+7kdAcOShx8nJmF96ZIP7Ww1Lv9n/+O0g4ulCTimBSz/0NwnKkOtI65aZ\nb1EgozFhmBFdS1SBoq7BWlgcMxnXSddeFrjDY/yipcsFZg1+vyPOYlo5aYBu2xMPI6YsEaORDLI6\nIysyEE/mWuzWDejm5H0PzTHDMLDXRpi3LFaFs5sVXDhHfmmKqVL81iqCihiTEbrlRGqbJld1VPSN\nJzoH3pHVJg0fco9MNIyKdBBvkRhQwYNS2FmDWIf3nnI6TQsG5EnquZqn4ae/eQOTAbf3YDpmvLkK\nWhheehm/mIPvufTMYxRn11n7yCeYdVOCsRSnifu3xL3kthchDx1BSv7mD13CqEDUOdG1oJK76Hgk\nzIZA6yJBReq6wFo4XkA9nlCUiqKE40NHu/BInnzUu31PnEWkVzAY/HZHPIyUpUEbgUze4LYXaF3G\njS3LvIO+zzluYBgGYrtHOwdZXVBtnuXcBZheypfJiiaq9ADJjIEuQBOhBWUFFTW+6XEu4jyYOsNn\nSXmrJ0IxSg8B6yFEwYek+mlmFmcF7z3TaUkIgTwXBgd5vooPGTduJjHF3m0YT2F1c4xoePmlgfnC\n03t47JlLrJ8t+MRH1ph2M6wJEE6Gw+0jG9zvJXJxGPEcdwUHnEOLYTg+xjVtGm5qlab3Mw11AXVF\nHzxYx3hjndnBwXKlFIURBX1PDEAcwZEQ9iMcgdsJsNMSDyzYgVwiYRiI1uK6Dr+/h93bo6xHYB3H\nL73IyhNXqMfjFHxVJPtTH+D2tqVYu8BgDW5Q+EHhO4itxy0csQN7nJqjYm/Bg6jlEjjGYHswuUYX\nyXgJHQEHRhH8kBRC3oMPxJiGrt1shohgyhwyzcHONj7XrNnI/I+/Ajs7cPVxDk2A85tQFnjroGu5\n8dIWXgkHQwk/8FdZ9YFj8/An8r8b4CTHi6HojjnHAUY0x8cDbeMQnWx5TZZW+ypqqGrwocdZWN8Y\nc3AwS1YGIigx9D0QIqMIcgRxP8ARhB1HuwP2IGnFo+QMQ8DaSNc59vY9e3uWUV3iLLz40jFXnlhh\nPK4RDVHBB/5Uht2+zYW1AmMH1OBQS/dT30bcwkEXcccW34PtkxGOWi60bQzQW3RuMIVOo+VUQUIZ\nGHzAi8f75LZxh9uzWYeIkJcGncH2zgE690S7xlf+eM7ODjx+FYI5ZPM8FCU462k72HrpBqI85XDA\nX/0BCH71/2PvzX4sP8/7zs+7/Jazn9qXrq7uZpNs7qSkWLQoyiIsBHEiGbITYIDcBJkYuQjmYgaY\nO/8PGSDA3CTIBMgAdhI7NpJogQPZkuyJZIqUqCYlNpvstaq79nPqrL/9XebiV6TtjLWNRDVF5wEO\nUDjVVadOn+/7vs/7PN/n+8Xr6QP+1Ov4hd3cf1j28pMyCYxXSGGhdYXMRWflhwrsmXOKEHVdXWuC\nIKg3eyUJ45g8ScAY4ihClhXGWWSzCVmBLAVYDQU0PYROEuhGLbc7TylOx5CkiMog8hKGI9rNFqYs\n0c6DF6ggwDhHfniMWlujGidgJT4HTi0kFTIHWUpakYbSYpICSovSChWGZy414j0tGwBTerw7YxGE\nArXUh2RGZ7Ffm2c7R6vVoqqquiwjBEEQUBVF/UO9HnZvj5EGHYZc+cynCG7cI3jsAp3L64jtTZwx\nnLt4CY3E3L2LkJru8/+UonQ4I3+iDPRvUvwssa28wQrJlRZELsM5R1XWm1udkNSSF1pDEARn9XOI\n45AkyTEGoiimKiXWGZpNSZGBKCXaUvPPfRPpQho6wDtJOreMTwvSBEwlKHPBaAitZpuyNHin695T\nUEtjHB/mrK0pknFVyw3kHnsKVQLkEllKdNTCllAkBluC0oowrNekkPI9LRsAX5raX0FKRAj9JcUs\ngf5iB6lr+uNfxrYQ9XsviupdaLO3Z0GPCEPNpz5zhXs3Ai48FrB+ucPmtsAYx6WL55Bo7t41aCn4\np893cWWBNH+9Jd7POz7UVMgfNyoXEamM/id/m6G/iDXgERA1oNsB3SbqLlG4HIQivHyZUkgYz6Es\nUCrC2lp7Bd49CBQyiHHGIJRCa40zJXY6AWeQKsZZS9xoYJzFnI5Y6XVJ8oQ0LxDG0XEw1RaabdYf\nusB0MKGKHa7S+Jy6ZOINlCWy1cRVlrC78BdDJtohe02c8HWDVWh0KDFn5W4h64VtrEPkEjWeYY6O\n64OtuwSVIYgiqvlZJuJquzNnSwgCyDKaWpAORjR/9ZNku8f4VgC9NnopwHzrLhQWpTXBxirl+BRn\nO/C7VyADFZz+XDQ3PoxUyB83IleRqYjf/mSfi34IxiLwNCLodKGtYakbkbsCJeDy5RApSuZjKEqI\nlMJb+y6yUbp+xIHEGIdSAq01pXFMphbjIFa1aFejEWOdYXRq6PZWSPKEIk9xRoDrYPWUdhMuPLTO\nZDDFxRW6cnV2XgmMt5QlNFsSWzkWuuF72HYamj2JFw5zpk0jQ8174Jb1OnTWIHPBbKw4PjJUJSx1\naw5AFAVM53VPwDvO+lHuXWgjdJPRIOWTv9rkeDcjaNWywsGS5u63DLYArRWrGwGn45KOdVz5XSCD\n00D9XLD9C0+FfL8PoK6uKPIWp24FbycEui5hyGYbihh0izIQRBbQou7cJ3MoS5SVtXdqkdeTdkkC\nsxnMExiPYTKB+YxqOMROZrXCowhw+Qy0oypTxGQMecLJwR5lXiC1wntB0QpYnY7oD3Yo33yd9M3v\nU33tGywlcxpmRuhTGN1idbvHcgcY77C5odCzUyQOJTVumsOsgkIhncQkvqZsOmqHGwt4jwaMEeju\nKkRdolZc85irol4JjRiiEB1FkBaQ56A0simg2yY9HKBXF2A0gH//b/CTHNlu0nr8ElbXQ95uOoZu\nG578LRrR6IMiFPZA4/3GdqW7tPKCFXfKxHqkDuqJ06YkLqClQQQl2AihocIxT6AsQdraOzUv6gnp\nd6GdzP8C2rM5DIcVs4kFV0u+zHKH05CWFeOJIMlh7+CEIi9RWiK8J2gVjKar7Az6vP5myfffTPnG\n1yrmyRIz0yD1IbdG0Ntehc4yO2NQG5uczjQOiZaKfOqoZqCKWprAJ6buujpdz5BYd2bPpxHGsNrV\ndCOIWxFCC4rKUxmIGxBGEEWaIq2hrRWIpqTdhcFhysKqZjCCf/PvIZ94mm3JpcdbSG3ReMZTR7sL\nv/UkjKLGBwLbD/4v+ACEKXL0hRdqaVAjsa0WqAg3y1BxjBICfzJABBqVV/iTETrJCecZdjzFT+eo\nokLlJSIvkcYRGIdLM/CeOIzqjMIYKA2ysmA9LS+xszmtVqveKFeWMZ0Wrt1EdjoUaUoqBeOj+/i2\n5rnPfZKlz73I5qc3kNs93GJA+/p3GPzJv+P4a7/PRZ2R3fwe5uY3OP/xFtLOCIUjFCHkEp9byKu6\nFl9R09osYD2VBNmOcEHNsCmGpwRSYYqCcH2tbh6HAWUrRq+v0NncrOmfd04AEHs7VN/8Mu2lVeRz\nL9HuxDipSPKiPuhmE4g7MD7i8kv/BB1+MBgFH/bIC8MLFzQ4izSWVssSKchmjjhWCKEYnHh0IKhy\nxejEkyeabB4yHVvmU09VKMpcUeYCZyTOBGRpvXFGYfwetE0JtpJ4C9K3mM8srVYLrWB5RdLqGJpt\nR6cjSdMCIVPuH43Rbc8nP/ccL35uiY1Pb9LblgSLju9cb/Pv/mTA73/tmExf5Hs3M75x09D6+Hlm\nVuJESChCZA4291R5LVRG5aFyYM+sF2RF1JbIwCEUnA4LlAwoCsPaekhV1V6tcatkZV2zudlhMoaT\nO/UtYGdP8OVvVqwutXnpOUncaaOko8gTkjlMZtCJ4WgM/+Sly4QfEFvFX4jN/f2uXzUbAdH6UyA1\nzeUNGlbVNnbWY8XZ6wcheVFgqZ+LO+26yeodUmusqVkl7+ZhzjlQCoHAVhU6DNFKQ1kh0tpMO9nd\ng8NjZmkCYaPebJ2C3NCQIVhH0WhB9yFGRwVXv36V6UHG1S/dwB8UmLsznv+tf4YbDFidnnD3D36X\n5At/wOJgn+SVq4RPL1HaY0o7IwwEPq1QXqOsxGYG4WqdDIE6YwVpnIagXzsvOGOgKAkQBF6AMTSW\nF2mvrzKbzQBJoGOIBP71P4fhLfKbr+DQTA9szcNvNwg7bYrRkPb6OdY2l9iz5yiLn95N/sMQ7ze2\ng0aTp9YjtISN5SbKNrBnXjAIWzcSAyiKvJ6PEJZ2J0YHddlaa4kxtu67nKHbOYdStTZ6VVnCUKNV\nPaafp4LpGPZ2E44PIUlnNELq2Q9Xc99D2cBZaDUKHupCcTTi6tevkh1MufGlqxQHntldwz/7recZ\nDBwn01V+9w/u8gdfSNgfLHL1lYSlp0OObcnMloggpEo92iukVZjM1mJIlUchakemCNCOTj+oKZPG\nURYgCBA+wBhYXG6wut5mNpshgVgHiAj+/HXPrSG8cjNH47AHUxohNNoN2p2Q4ajg3Hqbpc01ztk9\nqqL8wR/IzzE+GEfMA47MVFjZx2FJb9yA05TmledIUaDUGbCp63iNBgvb20zGU5wEtbRIHDdrrZbK\n1JOoZ1N7wjh8luGUwhhT3/eGQ2yzVW+aSuMXFjB5DmETrEU5j3UOY3NYWKAyU5ovPE9eGlw2p4oj\ntFtERRHWprz8rVuwP2Lzsy9y7rOf57vfvQm9ZVZeeg4BxI9fIt9Jao14IbH5mcO2crjyrE5pBT4A\nLwRSK7CGRrNFnqY0ul2S4Smh9bTbPVQI02kJaYoKAirn6+Ltw5doHb1Bcv2bcLGL76ygF7pko7xO\nbQKJF57pfEguNqBUBJGtaRL/Y6DpfYvKZPSlxeK4cSMlPYXnrjRRpCjFe9gWsnaE3N5eYDqegHQs\nLimacczoNMGceVfXIlsCZwRZ5lHKYYx5F9q0mpayAK0CFhZqpkyzHvTGO4VzltwaFhZgaiqef6GJ\nKXPmmSOKKxadJooUqbXc+tbLjPbhxc9u8vnPnuPmd7/Lcg+ee2kFEFx6PCbZyWthPAFFXjdHnQJK\nhw7P1FgDjxAepSXGQqvZIE1zut0Gp8MEb0N67TaEinI6JU3rZq93Fd0OXHoY3jhq8c3rCd2LsNLx\ndBc0+ShjNqlltb3wDOdTNkSOKsFGAfJMcO9Bxd+Yzf2H8YMT3yXMBcQFXHsNLj1BhoN2G1XkWC2h\n3YJWjGr1GO0dgwoQrQ6+1SbXASpq4I9PcMagSoN3Buc9aIWoTC115x2NzU2y+RyhI6z3uMpBFCCF\nxJUlutHAKo1oN4iXmjQubzN6ZwCqh7QdXFZgkoL5SgTTMUWSwBPP8MZujrt3G+IuxKuc/tdTfBDQ\naDQgMVTMIGygAoUPBb2tmNHOAFc10aKJKzwegbcOUVqyNEcHEdlkCg1NtLpJ4gtkluEKh1zdRAUz\nXGTgYIRef5zk+nUwBTpuYtI5xtXm2ywvA4agpcnowbyArd+Ek/8IXuCQv5B67x+U+GHY7voEkYcU\nMbx2DZ64BI6MdhvyQiG1pdWGuAW9luJ4b0SgoNMStFueQOc0IsXJsccYhykVxnm8d3UzvhIUVZ3l\nb242mM8zIn021l85ggikkJSlo9HQaGVptAXNpZjtyw0G74zoKehYSZE5isQQrcwZTyFJCp55AvLd\nN7h9z9GNYTWG0/96ShB4Go0GJoEZVa1OEChE6Im3egx2RjQrR1NofOEQeJz12FKQpxlRoJlOMnQD\nNlcjCp+QZRJXODZXJbNAYSLH6AAeX9dcv55QGGjGmnlar+93oW0A3QrokVHM4Te34D+e1HNdEvfA\n9N4/1Jv7Xwb9D73+ljGq02HZjinWNvHbF5k7B97hq5JGp08RalwUYJ1DobDTOd44fGlr5ogFEdY6\n2D4vkZXBKVnTHqsKHCgdkqUZIFFhhDSW0pzVRlwA85QiM4jlPnluYF6R33Lgu4jCI5xEqRhXJdhp\nClqgXIXor1EZDdQpkswduhOe1QSnEEWsbi1zfDzB5ilIGMUhemuZwICqIC0Erjy7eTgHWYbJi/om\nPk+YtVLCpQ7l/g64iHhlgzQ36OWAdq6RQpI8+3HioiTNM3QrJVQRmXX4bEbAjOloCeckBIL2Jz5P\n9uUvE1Yp5S9GdfADFT8utuOyVn0c22U21woubnucm+M8lJWn32mgw4IgcjhnUSjmU4szHlt6ggCw\nEIQCkJS5x1QSqRzeSaqqbqSGWpGlWV3dCxXWSJwpqXxtoZfOwWQF/WWByXOqObhbOV0PvhBIJ4iV\nIqkc6dTWzV2nWOsLtKkIqbN/l0vCjibJC6bzhCiC5a1VJsfHpLkFCWE8YnlLgwnq6dUixZQ17dM5\nR5ZBkRvwdXM4bc3oLIXs7JdEDjZWYkyeEixrdN5GCsnHn00oi5gsT0lbmkiFOJsxyzwzApZGU6Rz\niAA+/4k2X/5yRlqFSB5ciebHkfw9L4T4mhDimhDiTSHE/3r2/AfCJf5nEXrlMTJTUu3eZnY0pGx3\natEwHL4VYCRoJdhcXgYpkElGWFlCpeF4WLvQ2FpewAHOmNrUwhictbgkhcJix3NUbpG5qWt+xoGX\nCKdwSV43N5McPziF0qITjzwuUKlB2Rzvcmwyoq0VTHLE4nlWth6l0ovIqI3Qnt5KHzceYsqEc8+v\nwXafhceWOS5m0IlY+OgG3Uc3YD/E3bZk9wvy6Zm3pahpbUEUQatV/+eUVT3GaC1iNEXPE9Rwj/Tu\n28TrfbSMKZSCwGIP3iE5PcCP72GnxxSzIbLM6GYZ1Ve+iKOEyiKH9+isPIbNPVK4B7a1/03A9mMr\nmtJk3N6tGB7N6LRLwjDAAUHLgzQIpVle3kRIyBKJrUK0ChkeA07jrTiTF3B19l55jAFrHWnisAXM\nxxabK0xe92qcKZEelBPkSd3czBM4Hfi65p9oimOJSRW5VeTOM0osSrfJJ3B+UfDo1gqLuqIdSbwW\n9Fd6DMeOpDSsPX+O/jYsP7bArDgm6sDGRxfYeLRLuA/2tqO4n2Gnee1JLGohtCgK3oN2Vb4HbaYj\nQTLX7A0Vb99N6a/HxFKjVIEN4J0Dy8Fpwr2x53hqGc4KslKSZV2++JWKEoet4N5Q8thKB5/bs4z9\nwSUuP84rG+B/994/Afwy8L8IIZ7gA+AS/67x7A+KH5TROOFxXiLxiCjAdc/RK6ZMRifIS8vYUlJV\nJTIM8QKqPMHmFce3jo+RlQAAIABJREFU78FkhCpy3HRGeXQErSYqyxHzKcxSZF4AtcCRNBLSkiuP\nPlJPAAYhLs1wriCqKgJr0KbCJ1OwGY1uF4yAEkgT7GwKSY4YVzSyGHeaIjLHbDxECIUeVtx/8x2Y\n5bg0wxMwmRTQbxKuhey9dpfWYcXo2hhmsN2IGc08YeUQzQBnqrq5mhj8pEQUApOWlM4RBRGiKmtV\nSxOgM0tx53t8egvsa79HdPgq9uQEXTmqLGdalgQ+QJojECVtL2A+w6ZjUMDFi0hbgSoI8hQt6sEx\n4wIq/8AukB86bHvhkN7hkQSR4FzXMS16nIwmLF+SyNJSVhVhKEF4kryiyi33bh8zmtSlmtnUcXRU\n0mxBnimmc0E6gyKvtwuPRRpJmcIjj16BQBEGEVnqKJyjqiKMDaiMZpp4MgvdbgNhgBKSFKYzS55A\nNRbEWYP01OEywXA8QwlBNdS88+Z98hlkqSPAU0wmNPsQroXcfW2P6rDF+NoIZhA3tvGzEa4KCZqC\nyri6uZpYyolHFIIyNThXEgURZSWwEgIDNtN8704BW5/m916zvHoYcXJicZUmzyrKckrgA46MpBQg\nfJvZHMapBQUXL0JlJYWCNA9QQmOFJHAG7R8cceBHbu7e+wPv/WtnX8+At6iNgT8P/Nuzf/Zvgd84\n+/rzwP/t63gZ6L9rW/aBCy8oc4eyAfnUgm/TWb6MM0VN2HUWXdb0RVsUmEAhnCefJ3UTVCnIS8rZ\nHDeZocsSNxpBkqCNwaUJZHPefvll7K37FGVGc6XFcjOiGA5o9zq1NnuWQpKQnWvwwj96BmJAlvhi\nSr8bYSZ3me2+QS87Qpzsw3SCGg+pdm7XTgPpFJkMiKf36I9vwo3XcXdO4fpN4pUOJFPCO4fspjn8\nzh/y/EXJYj9HdSqsqvBWQZbjsxzSHOmhMCW+22TxI08iKRH3v41862t89V//CygSxHSGGtxjPpsQ\nLvVBeKrpGOc05DNm3/vOmXFwRXI6gP5yzRI6m8hNRjOclTj54ORRP8zYFh5cXhJYhZ3mtD1cXu5Q\nmLp0YB2YUmNKKAqLCgzeCZJ5Tp4blIIyh/msZDZxlKVmNHLUA9maJHXMM3j55be5f8uSlQWtlSZR\nc5nBsKDTq2+S6Rk/vnEu45l/9ALEUEqYFp6o2+fuxPDG7oyjrMf+iWAyheFYcXunwvta32aQSO5N\nY26O+7x+A07vOG5eh85KzDSBwzshebrLH/4OyIvPk/cXqTqKStU2e3kGeebJU8BLSlPQ7Hqe/Mgi\nJZJv3xd87S3Jv/jXXyUpYDYV3BsoJrM5/aU6wRtPK7RzzHL4zvdmOF8bcA9OE5b7NUvo3Ync2ShB\nWoeVD7aP9BPdGYQQF4GPAN/ip3SJ/1k4xP/0gwISnKYVKJyp72gT6xHeogUIagqYlpJQB8RK4+cp\nGgibzXoMPy/QxhLrAHN8wJWnnqARh5h0zurWWq1NU+SEq4uQH5OcvMXg4G0e/9VPMSoT7EqP9V/7\nNOojj3Gp3ef6iSdYWqa/dZ7m1hZLD/Xgwiaf+Id/C7a32Hz0EisXN+gudaEdgUlhqUfzoUvkYUQ6\nug1v/xn94zd46uIFht/4CsqOKSNDr7DQ6vKl1wcMY13bmjpbKysZWzszGYtLM9a21gj6fSZHA5rF\nmHjwFm7/Os1GC9Xqks9S8uODWvLGllDmNBY6QANpUkQ3JJIOkgk2GYHTtT1rURE6TzpJznRbPfLB\nD0l/6LAtqQfVVNCiNI4oAm8nWC9AaCy1aJaUmkCHaBWTzj2gaTZDpKzn8qzRBDrm4NjwxFNXCOMG\n89SwtrWKUJK8gMXVkOMc3jpJePtgwKd+9XGSckRvxfLpX1vnsY8o+u1L+JPrLC8FnN/qs7XVpPfQ\nEpsX4G/9w0+wtQ2XHt1k4+IK3aUuURtSA70luPRQkyjMuT1K+bO34Y3jPhcuPsVXvjFkbBUmKrFF\nj24LBq9/CR0PIbRYd6aHZ2pXJmvqW8Da1hr9fsDgaMK4aPLWIOb6vqPVaNJtKdJZzsFxPY1eWkle\n1gYlDSA1krArcDJiksAosWgH+PCMMxGSTFK0PyOO+gdXlvmx78NCiDbwB8D/5r2f/uVroffeCyF+\nIhR77/8V8K+gHtH+SX72pw3p69o4ACIgn06xjTa63cRI8DhwBoXHKoFVEpNlxGf2dia3iPBMbqAo\nqcKQan+fh/7Op1GBJpeKxtIyNqrVFoNIwdEtFoZ3aJsJx0GHvddfI0JSHA+YZwVuNOZOJWg2OvS6\niwx2d8Fa7txrQ5Hz53eGkM+ZpEN6vQ65BcZztCgxs4QqWoLwEZY++RxPfM7x7W++gtxo8pzc5Nkr\nz/DK7jFvvf4GnN+mvVMy10ntxhT161q+Ors+Vga84OjaLYgUW70+J4N9mB0glxZIXAe1uE3Y36YM\nu/jJBFNJ2LlJNpzC0ibudMDaxhJHR7W6JcqDXqJKE8hLTJ5hTQF6GScevMjShwnb9WZSozsQMJ3m\ntBuWZluDNDg8xoFHIZRFKkuWGZSsBb1sbvChQABlAWFYsb9f8em/8xA6UCiZs7zUIIosQoGKAm4d\nwZ3hAhPTphMc89rre0giBscFRTZnPHKI6g6dRpPFbo/d3QHWQvveHfIChnf+nHkOw3RCp9cDmzMf\nQyk0ycywFFU8EsJzn1zCfe4JXvnmt2luSDblczxz5VmOd1/hjdffYvs8lDttEj1nPoN+pMmdp1L1\n7dBU9Y3m1rUjVAT93hb7gxMOZrCwJOm4hO1FxXY/pBuWTCYeWRlu7sB0mLG5BINTx9LGGntHRyQ5\neAVLGpK0oswhyw2FsSxrmD5gBtiPdawIIQJq8P+O9/4Pz57+hXaJl8KBNLWqnJH4MsGgCb3BU2uX\nm6KEyYhOENAOAvL9fZjPWFhZIggVAo8KFHG7AY2A26++gs8sq+cvYkYTgiTFXf02XjlKlzGKNPfi\nLkX3HJkOKZwkDvvMDya1XZ9XpGXFfDas1SVVVPPjqxItJFprVrfPc/7cOVrLC0StEEMFB7u07Zy4\nWTKZT/mTL3wVE3a49cU/5upxxtePC+7snkBVIXSDeZmwaABjMOkpa+ttmh4W0fTDFq1GB3SA8DDM\nUwoTUdCjsX6F4JFfQi4/SdVYAadxeYYpC6JejNx8GBp9Or0Vjg6PIBnSW1kCM6bV6+BshihTrDOI\nMge/goYHWpr5MGLbCYmRvIftpPRoDMaHVPhau7wwjCYQBB2CoM3+fs5sDksrC6gwwCNQgaLRjgka\n8Mqrt7GZ5+L5VSYjQ5oEfPuqwylP5kp0NKIb3+NctyDUGdIV9MOYycEcUXmUh6pMGc7mSKGJFBhj\nKCuQQqO15vz2KufOnWdhuUXYiqgw7B7A3LYpmzHT+YSvfuFP6ISGP/7iLbLjqxTHX+dk9w5VBQ0t\nSMo5mEWMgdPU0F5fA99Es0gr7NNptAg04AVpPiQyBT0Krqw3+KVHAp5clqw0KrSDLHcUpSHuRTy8\nKek3YKXX4ejwiGECSys9xgY6vRaZdaSlwDhLXgpWzmQPHmRp5sdhywjg/wLe8t7/H3/pW78QLvE/\nUkXPOVw1RVQlQgVYLMynUKQ4k7O4vEw5GtBV8Njjj7K0ucro8D5lMgEFNpuRJ1Oa5zZ4+KMvcfPa\nDUa33qDa+Q6H3/rP0O9hjkasbV7h6U/+BrL7EIgmW62YRRLymy/D/lU2+hBVR/QaKXk1xRQT+mtd\ngrYGmdBb6BEGPY6LNt+/MWQ6dRTHM4SLUAuLDF/9Mvnv/3PsrVdYevgcT3/sGZJ8BsMjpsUhFwc7\nrGy38UlO+9I2zY9eYPHFZ2k8/zhHcoB2Gaf7d4m6Icmta0jlERasCGBhgeChF0mCh6lyRVVVNRVP\nKUSV1odEUeGsJg40Barm21c5k9EEckd2eoIvC3w6p5hOMEWKWl39+QHhr4kPO7adg2nlKCtBoAQW\ny3R+Jg1kHMvLiwxGJagujz7+GKubS9w/HDFJSlAwyyzTJGfjXJOXPvowN67d5I1bI76zU/Gfv3VI\nrw+jI8OVzTV+45NP81BX0hQQt7ZIWOTlmzlX94H+BkdVRNroMa1yJoWhu9ZHtwMSCb2FHr0gpF0c\nM7zxfdx0yuy4IHKCxQXFl18d8s9/P+eVW5ZzDy/xzMeeZpYnHA3hsJiyM7hIe3uFPPFsX2pz4aNN\nnn1xkcefbzCQR2ROc3f/lLAbce1WglcSrCAQloUFePGhgIeDBJVX72FbKUFaCYyBqjBo69BBjKKg\nG9cqHpPRBJfDyWlGUXrmqWcyLUgLw+qq+vkB4QfEj1SFFEK8CPw/wPf4i2rGb1PXJn8P2AZ2gP/J\ne396tmD+T+DXgBT4n7333/5hr/F+qkKKM972X2UX1CUVKRxF3iZuP00ZbOK652BpAeFCZNjCRg0I\nG8ggIIpbLC2vMcpTvAhQQcjs3h7oBs0oJh2cwvIG28tLzGf7nH79j6Bd8Mxn/jFha5GDN69xOBgg\nYkkv0gz3d4gXW+QHt8AWRAtrBFJjVUA2zUC1IGrRubjOxz/1Eq9881usdBYZHu0yuXub9kOX6AtF\n4iwz28DMTyE5RDqByy1IxcpHHyac5Zy88xZlNmKRBqdjB3/783Bu+0zWOIflDovtHpPxjIXzSwyu\n32O9s8LJ7gFWh6hIYEczOD1A5ANkbw2ra5PWMCgoXRsO3kJ01hFVgiuPQQYsNTTDXKBjidEtVHMJ\nO8+Iy4x8fB9x/EeEky9RuIBAvH/Z+w9Shfx5Yfv9UoX867D9rjapE5J2XvB0O2YzKDnXdSwsQegE\nrVDSiCyNEIJA0ooj1paXSPMRgfCEgWLv3oyGhjhqcjpI2ViGpeVt9mdz/ujrpxRt+MefeYbFVsi1\nNw8YDA6RsUBHPXb2h7QWY24d5BQW1hYitAwIlCWbZrTO/GHWL3Z46VMf51vffIXFzgq7R0Nu351w\n6aE2SvSxLqFhZ5zODYcJCCexuUNJePijK+SzkLfeOWGUlTRYxI1P+fzfhu1zNc0xt9BZhl57kdl4\nwtL5Be5dH7DSWedg94RQW0SkmI0sB6cwyAVrPUmkLcJDEYS0XclbB7DeESSV4Lh0BBJ0YwmRD5Gx\npqUNS01FNrdkZcz9cc4fHQu+NAkJXFEnSO9T/DBVyL8xkr/vLoB33++7o8HOtAjCLXK5CUuPQn8D\nicCJANFfQ8YdrBD1ipES5nM+/vf/Hq/88Z8RxyvkRydgjmj31yke/RjhaEZ6eI3m3lskgyGsPkn8\n9DPku4d1w7LZrNUj7RCEpRl6bJ5SvPEqK5dWOTk9IW5p8mnMuc/8Jof7t7GFhXYfshkcvwFpStxf\nJ4w6dI1mLy7x8SKdRo9+P+be92+x9MgvU929yrSc013ZYHp/QLDSpN3ts/F3/wH3dbM2ZpiDz1Lc\n27sU3uFWF+k9tI4zcHr9Dv7klNaTzzHb2YHJFFmkKFdhgzZKVBjrkcJiD+5DK0C7OUpZCi9RxIQq\nJxscEGw+RaXbkFhC6aim9/E7f0hQfKVW8Xsf65Mfdsnf/x7b70o6tIxjKwzYlDmPLsFGHwSSQDjW\n+oJOLBGiNnA/gzZ/7+9/nD/741dYiWNOjnKODKz323zs0YLZKOTaYcpbe02Gg4QnV+GZp2MOd3Oy\npIZ2nsHQghXgwyZpbnn1jYLVSyucnJ6gWzHxNOc3P3OO2/uH2MLSb8MsgzeOIU1hvR/TiUK06VLG\neyzGnl6jQ9zvc+v79/jlR5a4erdiXk7ZWOkyuD+luRLQ77b5B393g6a+D80GzC1p5tl92+F8weKq\nY/2hHhjHneunnJ54nnuyxc7OjOkE0kJSOUU7sFRC4W2thX//wBK0YO40VimkL4hR5CrkYJDx1GZA\nW1fYBJwMuT+t+MMdz1eKAO3eXwmC/7G58xfAf3ch1Ju7x5iIINiilJvI5Su43jraS4xuEC6sUXrN\n9sWL3N/fR4QB7d4Ck9N9nn7+Vzi6d0rlBKNbb3LluRe4eftt+nnO6f038Ndfho3ztD/y6+jeGp1m\ni3vXr0MY1gybhsKmM3wyIdJQHLwDdsalx5/gzuEpT/zSS1x7+VXIR9BfhdGcZ3/lBYpqwvUvfgGW\nV+sJjOQ2lA7iJbavPMvuzg7Ygva5y1jbJV5ZYkyAnzkY3ARO4dKLNC88Q3o6qCkRVUH3qUeoDk/I\nDo9rLZjFJdaeeZKjZIx0miae+f09gnxMMD0g1z0IY5wX4CuWui0moxPs9BQ/2aW12EFYy3x8yOrl\nKxwPKmSrg7MdAimpJgdw9z8Q2T/F2ajm079P8WHf3P97bONrnfPIGLaCgE1ZcmVZst5zSK9paMPa\nQoj2JRcvbrO/f58gFCz02uyfTviV55/m9N4RwlW8eWvEC89d4e3bN8nzPm/cP+Xl657zG/DrH2mz\n1tO0mh2uX79HGNZwUg3NLLVMEg864p2DgpmFJx6/xOnhHV76pSd49eVrjHJY7cN8BC/8yrNMqoIv\nfPE6q8s1tG8n4EpYiuHZK9vs7OxSWLh8rk3XWpZWYgLGuJnn5gBOgRcvwTMXmgxOU/ICigoeearL\nyWHF8WGtBbO0CE8+s8Y4OUI7iafJ3v054zzgYBrQ0zlxCMI7Kg+t7hInowmnU8vuxNNZbGGt4HA8\n58rlVarBMZ2WpGMdUgYcTCr+w134UxsRWYd9HwkzP2xz/4WQH/j/gPf/R/yVn32PnmSRuj7VoSTA\nYUyOyQ2t1R7pfE7U6jM6OGCl22GSJkwGA2QU8b1XXyXurtMMYy5ceYYb198haEqGO9eQeo7vdiBq\nIq1jcn+PSaBRjbgW7pICl5XgFHFvnSqdole2ERSkYpGFRx7n2o19aPb42PMvsNfqMCscr9/fZalB\nrUbp5oTVlNI1kNECbvUyu40t4nN9Lmyt8vb336R9cYPR8QFq/QLRlW3sQkhx67uQVqRHJ6hWszYZ\nafWYvn4dGjGNrfOsPLfM7quvcXTjNpiC8PwFwk4TQg2FIJ2eoLoep5cQoqYzpukUV+UEShK2A2I3\nYzA4IA4Cjq+9XksoT5q0Nx+nyCsgBZMhlcVYieTBNVUfZPyssf0uN8cCTksK7ygBR0BuDCY39FZb\nzOcp/VbEwcGITneFJJ0wGEyIIsmrr36P9W5MHDZ55soF3rl+A9kMuLYzZK4lna6nGYGzkr37E3Qw\nIW6oWrhLQpnVCpDrvZhpWrG9oikQLIqUxx9ZYP/GNXpNeOH5j9Fp7eGKGbv3X4fGEjqAuYNpFdJw\nJQuR5PKqY6uxS/9czOrWBd78/ttsXGxzcDziwrpi+0pEuGD57q2CKoWTo5RmqzYZ6bXg+utT4gac\n32qw/NwKr726y+0bRxQGLpwPaXZCdAiigJNpiu8qlrRDC4EHpmlKXjmkCgjaITMXczAYEAQxr187\nJlLQnDge32xT5QUpkBmwSiKtwT6gKdVfiMz9Z7EA/uovlEjASYNDYe0CyPPQvgjNFVTcwcomyBga\nvbozJSW63cIEMUEocGELSwiZYbHTwk6P6W88ys6f/hdgQNeCW7jI3HfqI3SegY6hv1CPQ1Pgba3U\niBCQTeuHakHcR0YBTgaAZvGRxzgtbF0fz0rW1zq4k5sMb13lwqJjIDfJWEE4Qzk7gDyj29+AjRWK\n228h8jm5taitZ1jeeoSj//ZVeh/7BDMR4VDI7gJKKaosRRmPKjPKqCLqtIm7HSaDFLoNmE7h8AgO\nb6FigYgXMF7X2jhVRrsdYWd7ZMO7ICp00CYWCptNKWWGdUsE7VVwKYGfk177T7Qa3yczbZR8/yb5\nPsiZ+88a2/XmLjHSoXAsWMt5CRfbsNKETqxoSkssodd4D9q02po4MIgwoBU6Qiwmg1ZnkeOp5dGN\nPv/lT3cYANguFxccHT8HDdkcYg0L/XqmrkDgbK3UKARMs/rRUtCPIYgkgXRo4LFHFrHFKbmFMoPO\n2jo3TxxXbw1xixfYlANWyDBOcDAryXLY6HdZ2YC3bhfMc4G1Oc9sKR7ZWuar/+2IT3ysRyRm9fvv\nSpRSpFmFN4qsVFRRSbsT0enGpIMJjW4N7aNDuHUIIlYsxALtDZWHrIKo3WZvZrk7zKgEtAONEjHT\nzJLJkiVnWW0HpA7mPuA/XUv5fqNF22RU8v1rrn7gnZh+1PHykzq+/8gQDidqXRfpPbXgM1ClyKRA\nGVerGQYKbVMCCvAWZz1CgVMhl7YvwmzCuYsXOL3xBotNx84rX6J74SKLl59n1ltjPrwP45sIa+uC\nZEOBy5CqwldnRX/ra3GLsAHdVeh0INDIIETqEKkV4927yPEJynnQnsPvXuV4b4jVbfbHMVH/Aq6a\nUg5usry2Cb0VpsmU9P4YEy9j+pvI5TW0OWJ4900IC8zgDn2forHgBdXBAXEYo3sdxMoqiA6uipie\nlgjZQPqIzvJG7UcWSKwJQIYo78A6lJLkRUZ2cIf18w9BXmGHR8ynYzJnIC+gmlEd7BJN7mLSMYQD\nKNugs5/dZ/uBix+O7p81tp0AJxzSgz9LYtCQVlAkEmcUxtSG2KnVFARYT51oKEGoHBe3LzGZwYWL\n53jjximuuciXXtnh4oUuz19eZK034/5wzs0xWCtoNkE1IHNQKYmvfO0VcOaz3ghhtVtDWwcQBpJQ\nS5SW3N0dczKWeKfwGq5+95Dh3jFtbYnH+1zoR0wrx81ByebaMis9mCZTxvdTlmPDZt+wtiw5Mpo3\n7w4pQrgzMKS+j6X2aj04qIjDmE5Ps7oi6AiIKkd5OqUhBZGXbCx3atvVAAJjCSU4r3AWpFJkRc6d\ng4yHzq9T5XA0tIync4zLKM7MznYPKu5OIsapYRBCu4TsAdZGPhCZu5DSBz8gc5dS/kidjZ82Ktsl\niDapqj5x7xKquYJotcicAKmx6Ho1xF1oNhA6xHtFbRgZs9VrcLq/Q5U4qtKgZIQQAnP8GmhFs3eO\nMlzGCYXDI8IQn5fvvsGzv6LWya7fb4CMIpwI0DrA6hCUJur2KJIZvqwgm6CzIWEUUDYXeOqpJxge\n7rF/tI893OOXP/vrvHnrHrODvfp3WwOTY2SjQ7MRkB/ewlgFa5dh8wrCRTQW10jTHOI2WIeMI5wQ\nEFiwZd1x272NiOoDwXtf68/7EuFLfJUhywFumtA7v0x2fwcbxoSBJLt7HbHxMH7vmJW1kFzHzG7+\nSxpKUUYJ0r5/jIIHm7kLH4V//Xv7eWC7ays2o4B+VXGpF7PSVLRaAuEytASNJVDQjaHRhFALlPc4\nA3EMjd4WO/unuKTClBWRVAgheO3YoDSc6zVZDkuUcHgcYSgo8zPSwhm0HXUGL6UkcI4oqpu6gdaE\n2qIV9LoRs6SgKj2TDIaZJohCFpolTzz1FHuHQ/aP9tk7tPz6Z3+Ze7feZO9ghqMerj6eQKchCRpN\nbh3mKGu4vAZXNiFygrXFBnma0o7roewolgjhsEHtyzOfw+1dsJFA+PrQ9U5RekvpBVnlGZSSZOpY\nPt9j535GHFpkEHL9bsbDG4LjPU+4tkKsc/7lzRlKNUiikuB9LLp/4GvuP2zVvd/gBxA+R5KDKBA2\nIc9bSFdhnUU0eyCjWrA6qqAKEF7W7ktVQWexz97JMX6egjagC5yP8SIG2YXxiLIfY4wDHEJrfFnW\nnSIp68xdCITQeGNwUoLUtQvSmVGIV/VNIp8nqEBjnQDVxIgE4wJwkht3j0hnJT73kCVc/fZr5IcD\n9OoqG+fOMZ5NIfQk4xHz0SmNziLtZodxkqJH+xij6K30KaVBS0dRligncbYijJuUoymdOKL97DOc\n3HwbUxYIX2GrCiHf9TPzxFGHrBkwnab4wiCbEdYZVLNNp7vILMmYnN6rbymhxBj1I29uv9jxg9H9\n88B27gU5kkJAYgWtPKdyEussvaYgkvVlrIogqEB6gdaSorL0Fzscn+yRzj1GQ6Eh9o5YeLoSRmOI\n+2WtAgloLShLT3lW6vG23tS1EBjjkdKhz1yQ3jUKcapWmEzmOTpQCGdpKkiEIXAG6eDo7g3KWYrP\nPUkGr337KoPDnNVVzblzG0xnY3wIo3HC6WjOYqdBp9kmTcbsjzTKGPorPYwscVJTlgXSKSrraMYh\n01FJFHd45tk2b988oSgNla9dpqwU9c0G6EQxQTMjnU4xhSdqSoyztJuKxW6H/5e9d4uVJMvO8761\n947IzHNOXfve03MniIEogDc/mJAEmDQECwKtJwGmIQh6kEHAfpGhB5l8sh/sB/lFkgHDMmHBoA0b\nlECbIEDAoAyLAPUgUdBwZAtDzgyH7Jme7q571bnkJSL23mv5Ye3Mc6qmh9OX6jqnus8CTlVmZGbE\njsw/dqy91r/+tVmd8L2HR7x8FUIPsRR+eFzi47MLMbmft6VQmfKGGK+TVw+ZH0RWGwFLWJoR9uZo\n6oijUdmgpSKxgMDJg9tQK5/7yo/x1tf/DVhPkBlVE3Lz89jNz1LEW+fRdUQMteqNtmtxCqAGCIrg\nmsFmE2Ck2CMYCORaIWfqZPT7+0xlgnATIWLm3khAkbzix//Cn+cb3/gGB69dZwqRd+/f4trVPUq/\nYDE31sXYlMpmVOY3X0T3FnDnLW79yz8mXXsFSwdYSOjVGxBn1M0eHK5YHVRO3nyTXb+0OpC63sdc\nlZormzxguTCnMphg04ZpHIgHL3N46xsQ9qgjvPLqFR7eyxCVWC9h+HFZDYlNnrgeIw9XmXgwRzYr\nksEsGfO9QJcUGyMbKrUoJTr19/aDE2qFH/vK5/g3X3+L3mAmgaSVz98UPnvTiFIog7c0MCLVFElQ\nqqLSmrAHdtiezDCgjwnDj1NrJmewqbK/3zOViZsBb5FnBlVRAqss/Pm/8ON84xvf4PprB8Qwcev+\nu+xdvcaiL9h8gZU1tWzQccOLN+cs9pS37sAf/8tbvHItcZCMFIwbV9UToZvK6hDqwYo33zzZdrlk\nqNB3CRRv8pErQ95QslGZIzawmYxhnHj5IPKNW4fsBWCsXHn1FfK9h2iEVM+vmOnyqgKCVBBDtdBR\nmYYTmATm14g4JoRbAAAgAElEQVQCpsUzT1JAo0sX1MJsf4+uT5wcH/PWt7/lGQwLqIFsa2LMlYv2\nr/aslidod4BWQSg4p0FAFFMQaUCwCgalDERNhJToYiJb9TBRzgh+kZi1Nmk1UzdrRDOHD29Tj77H\nfLhBYca0XPIwTiBX6V95mZe7yOG775B0YHP3LnZd6WRBTSPh8A55vAWWqWkB82vUgxe4/upnODx5\n4Bw1KzC2OGmpYBnRQtCC6gBdZFgdwWxGpwWth5RVhmlCXo5YSNy7dd/ZCKF9B5f2sViVgAkUVSod\nJ8OETHBtDoh3VVJ1pemoTbqgwt7+jNR3HB+f8K1vvwWhJWtN0fZ7iXlIpL+6z8lyxUGnSFUKskW2\nc7zViC2vUA0wGEohaSSlQIod1TIpQM7VJ32M2kLGuQrrTSWrcPvhId87qtwY5swoLJcTU3zIVYGX\nX+mJ3cu88+4hgybu3t2g142FdIypcucwcGvMZINFqlybwwsHlc+8ep0HJ4fMZlAMwoiHG4uRDYoK\nRQODKrGDo9Xg79WOw6rkVWGaIL4spGDcv3UPkUQNdq7IvhAJ1fM2CYBUzGWVkBSgB2RD3RyTKIhU\nqhaoGS0DWGW4d5eTu7d54eoBiHmDj1YSLiIYBSiEThjzEcQRkwyxsSS8NcxuHGbWtlePkecRrZky\nbMibNV30tW6dNgjeLzKIkaIw6xPSRSz0fPedd5ktEtNCqAcRrs9gHpBF4md/9qdZH91nnMHq5D66\nPuLg+hWuffkrdK98ifLiy2g5gqsds5sJjr/DvNzm8Pd/k3j/9+HetwjDPSKjs4asIjpRNg+xzT1C\nOaYb7sG4hLymnNyFk7vsccSVa69gyxV7n/2z6Dt/8KfLQlza07EgVKEh2whJoIeNwPGmUkhUcU2U\nXF2WoBrcvTdw++4JB1dfwAT6vkPkNAFcGrqlCxzlkTFCFtth+wlo77BdxW8IY4Zclc1QWG8yIXpi\ndzNVKk6zMQlITKR+RuyEPhjvvvNd0mKGLCbiQWV23UltaSH89M/+LPeP1jAbuX+y4mitXLl+wFe+\nfI0vvdLx8ouFo6J0VyHdnPGdY7hd5vzm7x/y+/cj37oH94bASET6jmrKpMLDTeHexjgugXtDx3L0\nBPXdk8LdEzhij1euXWG1NP7sZ/f4g3f0QmD70nPHVSDNaquSVELEGS51wDZL8uwA0+S9TjG0qUZi\nlRgT65Njru7vsTo5ehzR4m6KakG1B+ZQGw1SePy9bGVeA6qTB0IltOM42yTH5PK8VtDgKwJ3poRH\nD+43BcaOshaO4lXCoyU6bZi9eIPxwW32X1/wzX/6G5w8ug1ZYP8mfXeFk7ff5kRugS248forrL4Q\nmR4+pEyJ+Wc+x/DgIfNwzPDOW7B4gZm9wubOXWo3h5d+lMJEpFKHR9i4Qo8e8NIbX+T+0UMoS0ou\nlDqne+kKfb7JetwH+fYz+30/zabNA1Zp+goxUKswVFhujINZJqnRzcC84ydF3cNOMXJ8smZv/ypH\nJ6vHJ2vxaHJRpVdlji84R6Stgh8fRwjO3JlUnZUizkco5oVPKWai+HMJSqkAhqhx/8EjVr4gRNaF\nq/GI5aPAZlJuvDjj9oORxev7/MY//Sa3H50gGW7uw5Wu5+23T7glJywMXnn9BvELKx4+nEhT4XOf\nmfPwwcBxmPPWOwMvLOAVm3H3zoZ5V/nRl2CiUIk8Giqr0XhwpHzxjZd4eHSfZYGSC/NauPJSx83c\nsz+u+fYFWYhe2MldVZ+CXvv7s6JKpIIVCgGWh8TFFWqdCHYfygyb7yMykUyZViMhLYgpktcDi2tv\n8PDRETIZQkDEPOxiAtu4YWhL2caIqeYefDtbTA3bXRFOzwwYQSvFVkhM2PKwdT5YQAoQjRT3Wwd7\nz9yrGVx9BczQ2Yh0HeNyxd6P/BTLr/8uy3oH8siVL/4kJ/cfcOVLLzCpslkW6gzG5QO6qTAtV9TN\nMeHVG3zu86/w1neE62+8zrT+Ll09hBeuslkew92v0S8OmKbMwc3X6cqGRwj37t4CG7A6EJNRTSnj\nktC9DmnDQRwZn8mve54prfe2Z4ltVZ+cikGgcLiEK4vIVCv3LTArsD83JhHUEuNqYpECMUWGdeaN\nawuOHj3EJiEgmAgFZ5QIDu1t/+cttrPVHbYVMDVU/VdoxGOMQNXAygopCodL20E7JJfS3Y+JWisV\np0+aKa9cbVz6mdJ1wmo58lM/ssfvfn3JnbpkzPCTX7zCg/snvPClK6hOlOUGZpUHy5EydayWE8eb\nyo1XA698/nPId97i9Teu8931xGHtuPoCHC83fO0uHCx68jTx+s0DNqVDeMStu/cYDIZqWIqoVZZj\n4fUusEkwxgO4AOi+sJP7M7UCxAnCgFjBwgJN+yyuvsT6zttIWpLmK4p5KJ5cWvGOgkRWR0fsLRZs\npiXgF6+Ah12eMFVfsoUUds+/z0LwEnIz1BQyHs4xvAODjUDnV0A+gVqJXaRLc4Ya/fOqTrksBdm7\nynqTWfzYT9Bv7nB9tsD6GyzrXR4cDxA7YpxjGdbjGooy++JX0GlDFwtv336Tay99hjysWZcXeOnV\nz3O9h7f+8F0XEmPB51/+DG+/+29ZHh8xu36D0kfMepgyqhnpErau1MVIJ99lihvO3N0u7eOyAlOE\nIUAxYRGM/aS8dHXB23fWLJOwmidfHcpEyZCsoOaNLo6OViwWeywnr0VwvMp7Qft9YTu02P1ZbGdx\nOoomGA06vAXASXaefOwi89QR67CFNn3vDJyre0LerPmJH1twZ9OzmF3nRm/crUuG4wd0EeYxQjbW\n4xot8JUvzthMSokdb95+m8+8dI31kHmhrPn8qy9Bf513//At7g/CgsBnXv48//bdtzk6XnLj+ozY\nF3oz8gRZldQJdW2Mi8p3pWMTpwuB7As7uf8gz+apV6sCSRJFCiorktzAZECXS+YvvsE0f4jUDbZ6\nAFcWSOowCaBNn0aVPIzkqe4kWEXEE43bcwjB3Q0feIs/npFsNQNikxqN1HY3tlrd+W9LXUkGfcXK\nBLmH0FPaezUbGgpYIkhEBUwrIc2xXJAQ2WxGNut9jpYD2COY30S63pfjNcHenvd/XK/JFulSz/r2\nPV547bNYnHH1hZeJac69d95hf6bY5iHX94XDR3d4a/nAXa5pJOe5KxaWTBcjIh2WEvnkkH7vJnL8\ndUr0HpfPwi7IKnln54HtlSg3JDGIsVwqb7w45+F8YlOFBytjcQW6JARpnWzMOfjjkKlTfgzbtdgP\ngvYOz3IG57GdW4wRazITtZpjmhbiSULtYSpGn6EPtKg+WFZKUJJBFBdFq2rMU6BkIwZh3GzYX28Y\nlkc8Mrg5h74TAkaqyt4eJIms15VomT513Lu95rOvvcAsGi+/cJV5irzzzj10ts/DjSH717nz6JAH\ny7dIAcYJ5tm/i1yUGDs6EVIyDk8yN/d6vn4saCzEj1NQ5jH7wVj5oZO7iMyB3wVm7f2/bmb/pYh8\nEfg14AXgq8BfN7NJRGbA/wL8NPAA+I/M7Dsf9RS2tr0wnm7CQghSUAZgpJdKsYFHd25xde8KR4d3\nYLhN3H8F1egetKrHvVWckx5o8XBraK2nqDdDQnps3FqKXw3bC1m9iElVdyGc2EVq8VWABPWerNE8\nWJmirxxi+z4kYASCVqITz6gxIha9EEmUyII66/2XzOqef7cAxPn3VakS2Ltxk/W7bzIe3Ycrc46P\nFJ1NrIYHjA/v0IUNq7t3+dKXv8DtO3exeeXKwTUkzrHXXmR1eIv66DZ0HSqVulmzd+MamRVlcw99\n+E36/X2M6Sn+hh/cPg3YFqBIYEAZgSo9gxVu3XnElb2r3Dk84vYAr+xHoqp70Op4E3VOOsG9ePP5\n2Hnfp9AmhcdVKUvR94K2r2jbnBe7iBbn1GgQVmvFoqeYYvKykm3VfhAjYFQNCBEQYqxEE0Sccrkg\n0s8qzECzXyaLzs8/pa0cQuXmjT3efHfN/aOR+RXQo2OmmfJgWHHn4cgmdNy9u+ILX/4Sd+/cps6N\nawdXmEfhxdeMW4crbj+qdB1UUdabyrUbe6zI3NsUvvlQnc55AYKB7+f2MgI/Z2Y/DvwE8Jdao4K/\nC/w9M/sR4BHwN9v7/ybwqG3/e+19T812jJKnackBYjZR4xFTqWATNi5ZjUC6SphfpZ4cQh4JWJvM\nm1tSMtSM2Jb6VJzxoiOimVgrtCVrSsk9M6unf6ijMRomlWiKUKg6QsxY3aDDCYns5XSh889I8WKo\n9lemJaYTpY4o6tIAoSLzAHuJOl/AbA6ygO4Ame0RQiSGgMUEFpAAm3FErlyBV1+HxcIbKm8myskx\nko8RjO7aS/zJt99hce1VZnHOyf1DjleJ1bpy9eAmhOTnkDcs9vZZL5fQ9dywI9hfIpsLIRT2ice2\nJkCUyYyjWKllYjJYjgbjiqsJrs4DhyeVMXssnNDSRd5VklzBWv6o4HK+o0JWodaIKo9hu7YbQN0u\nAqL/VTHUIgVh1EqOsKnGyaBkEnWCzhfFFHGx0+3fcipMaoy1oChqkRpwDfk9WMwr8xksBA462JsJ\nMQRCiKTY+vQGYRw3XLkivP6qx/fphGlTOT4pHGfBEF661vHOt/+EV68tmMcZh/dPSKtj6nrFzYOr\nXoiFsMmV/b0Fy+WavoMju8FyH+rmYqwVf+jk3jq9L9vTrv0Z8HPAr7ftv8rjHeJ/tT3+deDfl6cq\nDPP0Tc1dhEAllTXIBglLGN6lrA7pqqLDBtYbbL1Eho2rHE1j08P2ZKyVAXRCLPt2WmxRFasTopk6\nbbAyEkS8ibEIXQgEq8yiELRQdbu8nUCH1vwxUeMeYe8GpAWxmwEGdfQ/nbA6YmXALPs4VBE1lyvI\npVEgekI3I/RzJM2pFqkWPZkbwi4kIGkGEtg/uI6VDoYj6vIdLFQsXcG6A5hd58H3vsW4ekDsIjI7\nQIcjNutjSD0mkdR1bDYbMGM/djx68DWiKdqVc/q1T+3TgO1oHveuBNYlsRFYBuHdAQ5XBa0dm0HZ\nrGG5NjaDsJk8BFHFaZTFYCheeZpNvL8Bp9ieqpFV2EyVsRgizvoSSYTQUS0gcUbRAFoxMyaDQR2W\nSWAvVm7sBRYJZp1XLY/V/yaFsRpDMbL5OFQFUyFPRsnuI/UJZl1g3gfmSYhWiVZR9TDS9qeaJSEI\nXD/YpyvG0QDvLCs1GFeScdAZ12fwre894MFqJHaRg5lwNCjH6w19gihG1yU2mw1m0MV9vvbgkd+8\nuvPtnbq19xVzF6+u+SrwI8B/D/wxcGhm2yv0bBf4XYd4MysicoQvb+8/sc9fBH7xo57A07DtTxFM\niUwUBmIwNPWIrBFJxAA1DjD2gLjWjBkWpbkmW976qRCUtbSK+ToX0+KvqaK1eWjN2zGrjMdruv19\ntOC0zLIm9e7phG6BWsLo6PdnTOslaCEkP4bWVustAWrGgFoEunac6BccJk5T23mIj89N6m6Ye3Bd\nz+pkCWyQeuxFJlNPlhuELsGVBMe3kK4jSoXxiBgUSialRBk3VDwXQQjM+8Kqfo9FjYxtgX3e9knH\n9hbdaoGJyEDBQqRPylqEJAIhMsRKPzoaui2co/c+3ULbC+1ayHCb61FrtEZrnaFwETIgJZonb6yP\nR/b3Oyi+Sl4XiL2v7hZdIJnSYcz2e5briaIQW2K2Vm01Hb6KAENKxZpkT4inC2lfIfv2J++62l4L\nGH0Hy5MVG+C4CoREP2VuSCZ1gXQFbh1D1wlVIkcjaIh+M0qJzVh22A4BSj/ne3VFrAuEkYtAFnhf\nk7t5GeRPiMh14DeAr3zUA59rh/gnLIj6OlSEwshMjxlLD+GAqg+QuI9qIuVjakkEDAk90lUsV+jm\nXgkYKhYj1iZcqZEQxHutetlq47fT+GOK1twkCCJhtqDkSm+CkqmirUp1j4QwEpnJRBo8MDrZBssA\nCQnxtMJVK2KKBiBngghSBQkJk45aBJE26ZK84jBEL5xSBc0EKXQ2keuKMJ2gZSSnfcJrP4rSo8NI\nSkaxEaNjKgHKPWrah8Z7R0YCFZFIscr44F8RU6GE6pID59wdHj752FYJTlsUGCkc64y+jBwEeKCV\n/SgkVY5zIpWKEeiDUDuhZmPeeQV3DUaMRkx+OrEKEoIXLal3c6IlY0Pj1OcmQRAVFrNAzQWxnoyi\nUhlKYU8jQiIyMskMGxKqsLEJspGAGOS0wlXxBjFByRlEAlKFFIROfNKvIogICa8Uj8ELp1Q91VQk\nMFnHqmZOpsBYlP2U+dHXAj3KOCiWEqMVOoxQJu4V2E+VCahERvHVUBShWuFfPRgpKVJDIdX4sXZf\ner/2gdgyZnYoIr8D/AxwXURS83DOdoHfdoh/W7yTwzU8+fTcmDr3EJEB1GmCJVdMO8yOKaKErlDN\niJ0r6JkEVwvasmAAs4xawNQ7tKi010TbBG+4BIEBik4jhEC25KGVWF3QcVqhIULoGMfMSCJEgTog\nRJQKJMwUxH/SEIKHiSy4jxWCx+hDgVaKZRKJXfPoa4FaCFEIIpRhwzid0MdMzcckBqbDAa5OdN2C\nXNf0ceXfhXlXK1C66IldagZLCAmthfkCxvFRSxd0HgyO55tQPWufFmxnFDEYROjUaYI1Fzo1js1Q\nKZQuYFaxzpVPgxi9k9ObQ+DhkWCKqtEFZ7CIiOOg+UqnyIZxUkKAZJlsRo3+wmqqxOB9SfM4khiR\nGBiqa8tUXPddzUhtwgwhMJQWR8dDLkWgeLoAxYhihC4S8Im9VJAYEAlshsLJNJJjz3GuDCSGw4np\nKiy6jnXNrGKPSmHb1UqBEL2IMVdIBgmhVIXFnEfjCF2gU2crT+fvuL8vtsxLQG7gXwB/EU8k/Q7w\nV3FWwd/g8Q7xfwP4F+31f2YfMEv01PXbP6CVUBEZEauEkMiTkOKCqiskGJii4wGhD5QyIPM5Mc3B\nPKYopaKASvVSJDFUWwJRfPI0MdeUoYC6F2SCuybiyaAtc6HfnzGOxwRxbzfQuaARK2JTrFRNEHok\nzAHQrMSUdtxjVKHrWzip5Ri6Hq2ARcQCkYIOE0ULktckWVOHh9QyUTcrmL3gmhubeyQ7oeMYpBKC\nNhG0DqsjtQyIOA2yFiX0mTq9A90aX74Ez1Ocs30asV1DYRShmpBCQKbMIiZWWrEgqMHBqIQ+MJTC\nfC7MUySYQ7MWX3pW0R22a+OzO0Om4V2g4LruIbSq1RY5DOGUcTbb7zkeR5IEVKDDM6orlFmImEJS\npQ8wb6wczUpKcYdtVeg7DyfF9mv0XfA+A+ZKl4XINChFC+ssrCXxcKhMpbLaVF6YgWnk3qZyYolj\nOpdtCIFSlU62cf9KFiHGDi2V3AfemSrrxswJ5gnni2Dvx3N/DfjVFpsMwD8xs98SkT8Afk1E/mvg\na8A/au//R8D/KiKtYSe/8EEHFUI4X20GMQ8nWKHo4OGOrsNToGusBEx6TzSiWA5NszpgQUBjK04t\nVIu+fG2JLSsVC8G3oYiYe9waCSKoaNOSTmCCKUx5A9PgSz3x5t2Yc96DRswEtalVDLYiJoxahrM8\nNM+MaYE4gxAJGEWLx9iLUWwrAlYJJZPrCsoJhD3Yu0q/9zLTkKHc8orDrvrNzNVAEJyy5jczJUgh\no6R+ZFq9QyfRXbpQXS5w1+7w3OxTh21r4YRiwqCFKELXQUBYA6EYvRhdMfdWs/veQQQJRmwrziIQ\nrRLaDQFo/HfzbYCJoOafEQk7bCe1Fh83NnlimGieP3TicfHaQ9SAmDGZF05Fcy/dgKHUx6BtxRnC\ns+jpJSNQtDgzuRi1dVWqKuQSPCRTYC/A1T14ea8nDxO3iufIatduZjtkN3Q3bBcJKJmxT7yzmojS\nOU00uArm+Qbj3C5Es45n0SD7w5pq595wPEB1D9VADAcIe5gssBBRnRHiHtItkJh8Qk8RC4qFBNLt\n+Og78m/jnqPVJ1wBQRAztHoCMtD48KWVMltAkmEiIB0hJrRNkCkliimSThtDSNehWUGNFDuUmSd5\nY4fhOjVbbrVKBow+Bco4YOMR3eIGJS3Q1copmHsHUCYWdofNWNjfT2ze+ueesgsdSCSmHi3OGNKh\nIn2B8CaLeo98Ttzfi9xm7zytU2UehIMIe6oEVQ5CZA9hIUYMxkyVvRhYdEKKjYGSBA0undvJKR/9\nLLRVfHIsPi8jCGaCVG0FUIFSlLGlrYN5IZOI7zPFQLBtYjahVujS6U/YdYJmV1PtYsIbRhpdhIQR\n5bRuIIu67EHqGcbC0WjcWHQsUmG1UuoEB3swFbhjC8q4Ie3v88/f2gAeMooCfYpMRZ0xNCilF94M\ncK8uMD6+NpF/ml34Zh0X2apORAK1rulmM7Rm1CJRAgGhatqFUjSDWEToEBKS3KM3aF7/mQIV8aXq\njWsHPHr0yF+bJmJMrsuuQi1KCoGS1/6RGLEcIHiLMifnN8qlmlMf2+5VwMbRg40zvwlB9mIoNaAg\nISHt5iBaEFGmYQPjBvKIza+jm4rMrmNxTYwjdXiA5RPS7GW0rLDqN6YQoucdLBPI1DqxmM+odsI0\nPEDnGS512y+UTVoJRNa1Mpt15Oq1EUEiguu2b0MpZCXaDtmuLtnQHXi8iEkaceDg2o0dtqfJSDGi\nBEQFLR7yXGef3WMUQnbPPKSINPEyoGnTbNkIgCjjaNQCixnE4FOrBsFUKXhhVWjvLyqoCJthYjO6\nIuX1uVE3yvWZsI7GGCMPhspJNl6eJVZFKdVJEDEEgrj8byYw1cpsvuDEKg+GiTxX0vlHGb/PLq+2\nH2Jd8jLohFLLQNdFio4UFaIYIh0xGMEixQJSIgTzhJT5hCehQE1YkFPJgRAIKCeHD2GaiCFQc0ZL\noYsZM6MzYdpMxE6o04RoRwgRkTlSc6tm9Z+w5glEsOqJLQGCVjR0mBaqBl+umnqISMWLtxphRdRI\ncWQqJ7B8BKkjlUfkNdjBFfqQkTxQ736HYRYI86tsHv4RMVTUBKF6eChPmE50KVD1EZZvM+sK9Ryb\nFlzaD7DUUTCUxFAqsesYtSBaMIl0IliIRAsEK8QiWMCJBGYEMUoQUgUJp5IDIYASeHh4wjT5jT/n\nSilKjp3nHaxj2kxIF5mmSqeeZ5qLkKtXajcmJFN2eWut23yFUNWbbBd1cT1CRM28l4K24q0GblNh\njImTMvFo6df0o+KNZa8cGDn0DFn4zt1KmA1cnQf+6OGGGiJiSm0r6ikrkxohdTzSyu1slG7mRYoX\n0J67yd3B8wzjlmKIFfK0ZN4HqvZ0ksk2euFS7AGlWmked6TWCZPQ6H4dIgkN0TnEfcKoSG7iwdWr\nWIMAJaNWvKoUb0PmBbDOPdccPOxRB2K3R0xzanUPSIhorU7rxH2qWgt+5XVQihd+gGefJqXvrpCz\n30g0T2QK5Naselwy6OAX7Hrh/GSE9PpniQjj7a/CcIsafdLuUs+wPoaUEfMS8an8CX1s+7P4gaiP\nW9GpZ6WeeBHsWWPbxMXEllMm9HN6rWTpGC1TDProcfdiFSUQVZhq9R4CNXr4RMRrGxBS31ExQhYU\nY6xKVq+9yAWKKS65Ik3JNKLZ4+IhK32KDFXZ6yLz5C0mUwjOmqmKbrULUEqt1ASdeC978eINut4L\ntq90/Q7bU1YKmU2LnCxHdtherK1FRZXPvu7X0Vdvj9waIEaftPvUcbweyAmXPAD+pExsYg94EveD\nBP2eFbafu8n9vCxFQRsgXN6isq2iMGvZfxNMOp+8NTWFSS8a2YYk8uiNhGUc/P/ocfdcSnPCK+Pg\nMfaU/DOedKXRCjPQk4Fk6slRzBuJqEGwM2p8Zbe2DVZbCTlQKrP5PtPm2J+rIq2jElpcim8sxNC5\nhOu0YXp0wt7Lr7FenSBdgs0DYuf63wA5r5Do0sVQMNkQGFFrhVsXgNN+ae9tEhOD6g7b9Wztm5lT\nIA06MSpGUqGYUILH5bchCR0zijGMPrlrFI+7lwwpUOH7sG2uNkZCyFXx6TKjlpi1y6poxVr+fYvt\nwi4i6RWw7YZYC+zPZxxvnGarjeHjHZUc2mWELni+aTNlTh5NvPbyHierNakTHmwgdHGH7VXOaBRM\nXcpsI8ZI8ByXyIXgtL+XXdiE6tPSvN5Vi37o83y8klO1w0QRmWF2ADLH2MOsA0kEOrTJ8XqoIjZJ\n3Qiq1NZer9PSvHTXfd+qOCK+3H1sBEG8V6oK0RLQU0LnSdVunxASZi4TvD3fogqqhH5GiD0qAS1K\n6juPSTbVvxCch1/Nl+NY8exAkxyOSVj0HZtSyaOLnYk9QPJbjZ1jj32/poUoGyQ8RGz1Ib/zp2sX\nLaF6UbD9ZI1yp4qKMRPhwIy5wB5G1zjmXUN3NGeRRPMEZoyuL+M1m95+rphSQivlaCqOJq0e5Ow5\nBHOhOzWSRXqgC4VOYL8LpBAI5pz27fl68xuY9YE+BoIoWpSuT0AhWNp9x6rOAisqFC/42EkOS4p0\n/YJaNpQxIwIPTHgrN4bME9guamwk8jAIK7sYM/qnOqG6vdN/eG6xF21s3ZcQshdqmOBC64kQtfHY\n2/taKUWUilLROrnnYYaEilEpIhi6o0gGlaZTY9tNp9aqZ0UDQYp3Ty0F6UDzypfMoQcLXpEoQjBD\nTQhaqTZiRJJEyrCBGN0nqe6bqKof1zJoRlKk1LrrDDUcLtH5AsToNaPlIWKT5xh2vWK3rt6aYAXJ\nIxI/2HL10j6YfVRsn0E2JpCDF+yI0VaGoDFQVZ9ANlSJVJTJS0YxM2oQKoZIcc99yx3QQBVp1dmP\n34i21bNBhSIBwygF6IRVVgJKH5xnH1qhlFnwWLgGRqtEjCiJzVDwKKF6/QbqXHiBbEZWiEmotew6\nQy0PBxbz9h7teViUyYRgZdcrdgvtdYRigTGL02cu+Gr0wk7uTyse9XQKRto+zO/mrildMVm2Zthz\noEckIto5Fzhmp35JIQVBtXgBFM6jlaA7j8kbFxgijVvTRI5Mmzyf+o1AxMhe6QQYUgeMPULooU7t\nfA3TQOkkyAEAABOpSURBVJCIMUdViKGnlAlSIpqh09araT1jxYhFqDZALGip9KVgUiibEWNiT78M\nJNb5XVJ4RK6VEMzlhM0QKglYUFjzLqTgqxYu9gVwHnaRsL3dgzRvNkikCizFi/XmCD04H169hiNH\nEDOKuKRFUfX6DlxrRoPs9uxKl4q2qu0ttkPjunuxttN7xfKuqnWowh5GHwJTC79Y+1yUwBxnh/Uh\nMpVCSh4ePYvtst1viQxWKRFqUUrpKWKMm8KE8WXdIwHv5jWPQqLW7LUoW869CwdTWPAua0LycNVF\nR/aFndwvqm2V8ACvMLWBKAUNBSEQohduAKh4fFFaw98/zULwBCv45G8iO/rkYzrfO65jbcfJgFJb\nWVyIghmotXi9GrV6sw4t+Uz4BLy6vl1hNmE2QfZJPYtnn0wyKb7KsDpEwwbSESX7eakqMS6gFXwh\nlc107DJ/lx77c2dnsa0CgxlFIiUoAUGit7oDGJvXKj+gK9NZCyGgZ4TGRE7pk2exvY10uCqqNmTj\n/Yxx+QDMPNeEs2BKrcTgzTN24Sk1ipkXwQpMBpO5emQR22E7i/FqTByuBjZBOUowNmqmqrKI0cNM\n5uqYx9PG1T2eE2xfTu4f0B73uioiG1SXTtVCnAUcoycSLbayayWE0xjekx7XrmvNbvH7+PHssaVs\nu6iiL6pNNqgGkOjPwUXEKK4vXyfAtdrtTJTVKZPmvPeqngDdXRziRVo1sL+4wkQmzlaU+oigA7lV\nyoqAMFBtIsSBkld03WZ3U/owy9YnmQTnXa7/abKz2K7ARoRlKwMVlIQQo1DMOe8moFVdOuN9YPvJ\nzMCT2N6iRaJ7+Rsxgqo3CgHAuy6VVhA1VRcHC8Zu74JTJk28CYhWZSPmOSkcknsIoRpXFvtkJlaz\nyKNaGDSAeOwdEQaEySpDDKxyYdN1u5vShwk3PmtsX07uH8m8yMFsIBCpNscktz5Izm6haVt4bLq6\nd342Ybp7bE/O66evn3n/9y8GFS8eb9u3F6NEFzRrxUsmkRS2EVPnKm+D+2LZY/TViLEj9dGpnp0w\nlEcoGa1rOslNnIwWiglUMjBS8wkhbhUuG6wueEzy0n6wGUDz3iOBuVWy2A7bCWlpqIAieMuZU+8b\nTmuObPfPE8c4U5dE+/xZc2Szi31v5QaiCBTdFS9FMUJIuyvDrO5C+9k8Rm9V6WIk9onRMtLBozKQ\nUdZVydL5StijjASETGUETnIlx7AtDfGxPAf+xqd6cv/orAVPXFIzwoln9NOCYgsInU+aktzL1tiW\nr67VwZlmHr4ra69vLx/wfi+hKS42r6ieXgDe0Qnfn6ewdkwbNSAGgnQ+h2twiubWk7Kh5Q2AYEjd\nQ5JQbaSMhkluNyJlhqJSXOQsBagVIYMZIU3UcUUMlR6jWPhIk/qTv8d7eTafRg78B7WPim2/fwdy\nhZMm9rZIwsKKKziakCQgUZrejOwUILfCWbsQ4Halt9uzUxkDTSGV5uGfqQXaYls5RfeOaWNKiNBJ\nADWC0hK5ju3BzMcghgXYq4IkYbSKjYUs5leLgDKjiMfnQ3KqZMZDm1MKrMZKDRGj9yTrR5jUnzW2\nP9WT+9OZHFx7PYgS2GB4ohH6pt/lfR+LxFY5Gr2U+j2UhbYXQxCv0JP2vz32ule5xugNtes2HilC\naPpXu89ZBjFiSl6WrQNiiSAdgauoedNgtUqMlVJHn8AF3F9SzArVPM1VrdCnzpfAQREb0XJMjE1E\neMci+njtvX63ywn/cXsqVEto2uuBDYGKYSE4Fz04pTAiRPFaBqc06nvqwZ2GbcJj2N4FXMxbVP4g\nbLvcB7vP5Rajd3VIY9BCMqGTwFVcqC9KoppSY2SsxZvJy/ZmIRQzqnk7+mKVLvUYhgYYTTguCjE2\nwTB9bMH9cdnTxPanenJ/KmZbnXRINmAY1QImpcE2oCpIXGAWd+XTH/mwZxO7OOhFosffW0zS9dLl\ntCybhFhHF/dQmbu4Uy2Yiis8irXJ3Qh1GwJq/IVQiQRKnTBTAiNmA9vqBN3dtM6nFPtyUn/6Fsx1\n0gkwWMIwglWKeG4oAKLKIjrnXUSeArLfG9tRhKCtWbwZU9yuLpxOnBA6E/Zix1x8xVlqdUJB16px\nt2Gj6gneLTOnBghEplpQ8wKlwQwauiOK6La69tnbh8X2+57cmyzqvwbeMbOf/zg7xO/0V56RyRlP\n+YN/WAltuVjDBFZIZgSbMZgQJBEkUewICT2BOTF2FCrYKSMm7jxmQCMROU02nZFlCVKxNolbjUin\nWBHQGa4V7zz50OL9WHT9G4QSvE3foJ13mNERsxUhTtSa/cJpF1UKRlWPsRepYIHoGTRmceWrgJg9\nDOOD/lD3rI/03T8Fe5a4bsd7brCtwg7bU6gUA7PEzAJiA0kCSQJHVuiDMCe4hj8FaXFrgELcYSMq\nrSDv+7FdJXj8XJVYDe0EKcasxdqthWESrvsbDbq2Gk7BuyZ1OmB0jBpYmTHFQK4VELa1PhYSubHN\nqhQPp1qkKqzijEELOTrXHVqW6jnE9ge5Jfwt4A/PPD+XDvEX2sxjk1ULRT1Eo2RMXExLdUOta6ou\nwUbAbwZYeawISsneDSrU5gmfRh5RlzKwGum6faTOkZBQGZ16aUIIiSCRwJxA75/RiKj3bQqWUTtk\nsodUO6bqEqWgFNdZD9ri5u6xY4GIorLCwgm5jDyTNeqzsUtcvw8Ta568VjZaqBgZZRJjUmOjyrpW\nlloZDSa8fUAxHiuCyg3dNTTtc07/oroUUqzGftcxb+3zRlHGFhZJITSee6AnELXdMFRQItkCh6Y8\ntIlj8/GUhm5tWusqZzx281XnSpSTYIwlP4vI4jOx9yU/ICJv4F3f/xvgbwP/IXAPeLU1Cv4Z4L8y\ns/9ARH67Pf4XrRXZbeClP61jzUXWc/9h9ljiyh6nOCnecEMk+HvslDKWZYYQidIjrar0dDl6SlkE\nWnmQL8+kBqAH6fGFl0/GahkhIDbDavDHTVZYtnozje3iPTGd5RJNEalke3wRF01PQzRWgRGzNUEK\nj9vFvxJ+kPzAx41ruNh67j/MzmJ7myLaYjvhk20Q8WpPO42rzyQTEXrxBjRnsf04siHhYbwQAqF6\nwVQvO2SjAbI5z35mTmEMLe/kvlSLwQvNEw/k4CwXtUgVIdnjWutqcReiqRYYgbUZ5UlphKf9hX4M\n9jTkB/4+8HeAK+35C3zEDvGfaJPqayJrZC7ZVmvi3jEG1bCQqFY9Fi7daeMMe3ICddMKQdLpBSKG\nyQQaCcwQiU1gLDR9dU+oqmRUCmIrjEIVdd5v9SRoNHGNm7PHsopJaZTHY0yrxz1N0FjQ4rmDEJ6H\nS+AH2iWuP6DVFoNXL6QmSBMXA0ojM1ptYT2rHgtvNwBwZcj33rGSxDuIgVejTuJdnGatETXmE3sM\ngSw+OWdRiigrE5cvlkowwyotCRopT8QnqimlifEdI1Q1ry63QImKlBbbf87zOO+nh+rPA3fN7Ksi\n8u89rQOLyC8Cv/i09vdh7WwhwQ8qxPjT7LFkx65qL2xVBNo2T/oILl8KQhDFtCJdag2z1+RWHScy\nf2wcAW3x+QShota48qLefJvoVa3mtUySoGomWMasYMElBEIrpLZqhOh9MJUWW9WOEP3Goq3dXrBM\nCCNCaSfTWPaaeM5x/7Hhuu37E4ftrW8Y2NJvQ6MSbid6j30LoBKoaqTOG2avFUp273nejr8TASN4\nuAXxMI3pVnEDVefYq5qTz6NAErJWsgWKGWMwJueo4cV5hsaASgtnitJpgOgqjtXUm25YYAyBgtAu\nV0Cdx/68g7vZ+/Hc/xzwV0TkLwNz4CrwD/iIHeLN7FeAXwFfun7UE3ka9liZ/1M2X+KeglpkwhBy\nHYFAkhmpJZdcWJRdsrWqNUolOCEsYLZNuDqt0qiNO++1gEqL5VvxAictp92DMZQK1OadCMmqV6xS\nCNJ6x5pi6tIFn0D7WHANn05sE04VSScRBGOsmQDMJEH0qeaUQ9ZuOlpbyEcIDd1i5p3F8EhnxXnr\nHrc3j6GbrwIUoSjbnu8eS2/oDsHZO9USJl7ZWiQwaEFNKOoVr59U+6G3KDP7ZTN7w8y+gDcE/mdm\n9tc47RAP790hHj5kh/hnaWc9GS/u+XiGGqJ50RBeDCTmUqedBMgFzQORSsQLhKRVfsIIUpHghRum\ntGRpohVf43H3DHHCGFA7hvAIwhEhbAgyIVJRy6jldvyJEAqmA6YbxJZoOcTqMapLongYJkn3vr+T\nC/wzf5990nENzw7bFgO1UQ0L5oJ5RIJ0lAxD1obs2JAtW2RTBRcaix7riRJIDd3beuocYIowYByb\n8ijAUYBNCEziipPZlGxejDSZUEJgUGOjxtKEw6IcV2OpSpWISKST9InE9tY+Sqbnv+Bj6hB/Uezp\n0tae9BC0lUhXuiQEEbDaxP8nsEhk4UVPobFdWqVctYgEQCtJBKVgMrj+h1Vv3CGtYtVaskrktNJV\nXZZV0FaghC9jm1Jl3I5VFD2jEf9Dz/CToQHzicc1PF1sfz+yAXPvWVKHSKCaH3MS14Nf4HTfGNQZ\nL0HoMaJVCNLaEicKyiDmEgeWKFpbOMhXsyEERMKu0tVUKCLt/Y7tKs6UoYWOoIUi7f3TFJ9HbF/Y\nZh0XwZ6paJU66ETEH2pErEMIlJhAA0kS1SJKuzDN6HBP3OUCKnIGsFsdmifPQVWdASNnzu8TrANz\n0Zp1XAR7ltiOWxFVEYg+mXcmBIQUC0EhSSJabVXRnuzMeI/X3DpAqcmZWP175xBUtRURnonrP3/z\n8vu2T3Wzjo9iz/Ru3bxqw5rGRqMiWiA0XXez5EqPwLaDgMTRn7fJuZ6VX7LtJP/4xSzhrLd1/jf3\nS3v29iyxfSog5iwxxUM4wSBoS6iah3Noui4AY/Tn26lLdzWlZ8ss7PEb1Vkt+Y/9zC62XU7uF8XO\neM5hJ75lp7OwiPPXoxI5jQFul5uqzqiJZ6/ZMyIfrt1x5sXt/k83PNXTubRL29pZvzJY2BUR7aJ/\nAqF6z1WIp+GiFiZUbbouEs/s53SfT2J7t/9mn1Zkf6In9203+e3jj+sYT90L2k30j+9X4ukNYHfM\ntikSvh/Fcvb9APaEWuCnFfbPvz2v2N5O9N8Xpz/jlTyJ7XBGvuDJ/bQPuCLlGWxfIvuDyQ88d/Ys\nxKQuBasu7TzsEtuX9sPsE/3rbelfH5dn47K6z1dk76NcsJddkS6OXWL7++0S24/bJ3pyv7RLu7RL\n+7TahYi5m9kyj8M3z3scH9BeBO6fj3r5h7IXef50UJ7WmD//FPbxoczMlsOYnydsn/nOnxt0X2L7\nPexCTO7AN83s3znvQXwQE5F//TyN+XkbLzyfY34Pe66w/Tx+55djfm+7DMtc2qVd2qV9Au1ycr+0\nS7u0S/sE2kWZ3H/lvAfwIex5G/PzNl54Psf8pD1v5/C8jRcux/yediG0ZS7t0i7t0i7t6dpF8dwv\n7dIu7dIu7Sna5eR+aZd2aZf2CbRzn9xF5C+JyDdF5Nsi8kvnPR4AEfmsiPyOiPyBiHxdRP5W235T\nRP5vEfmj9v+Ntl1E5L9r5/D/ichPnePYo4h8TUR+qz3/ooj8XhvbPxaRvm2fteffbq9/4RzGel1E\nfl1EviEifygiP/M8fMfvxy4iruH5xfbzhOs2jvPH9tky5mf9B0Tgj4EvAT3w/wJ/5jzH1Mb1GvBT\n7fEV4FvAnwH+W+CX2vZfAv5ue/yXgf8L1yv6d4HfO8ex/23gfwd+qz3/J8AvtMf/EPhP2+P/DPiH\n7fEvAP/4HMb6q8B/0h73wPXn4Tt+H+d1IXHdxvZcYvt5wnU79rlj+7yB9jPAb595/svAL5/nmH7A\nOH8T+IvAN4HX2rbX8AIVgP8R+I/PvH/3vmc8zjeA/wf4OeC3GljuA+nJ7xv4beBn2uPU3ifPcKzX\ngDefPOZF/47f57k9F7huY7vw2H6ecN2OeyGwfd5hmc8A3zvz/O227cJYW9b9JPB7wCtmdqu9dBt4\npT2+KOfx94G/w04slReAQ/Nmz0+Oazfm9vpRe/+zsi8C94D/uS23/ycR2efif8fvx56LsT5H2H6e\ncA0XBNvnPblfaBORA+D/AP5zMzs++5r5LfbC8EhF5OeBu2b21fMey/u0BPwU8D+Y2U8CK3ypurOL\n9h1/kux5wfZziGu4INg+78n9HeCzZ56/0badu4lIh4P/fzOz/7NtviMir7XXXwPutu0X4Tz+HPBX\nROQ7wK/hS9h/AFwXka2G0Nlx7cbcXr8G/P/t3a9OA0EQx/HvmoKluoI0IVgEAoFogquuI6HPQXgH\nXJ8CBKkuoPkjCCQgKIoKDIInmIqdhpomRXD7J79P0qS3d2J2M530di7td4PxzoCZmd358SXxA5Hz\nGq8r61gLy+3S8hoyye3Uxf0B2PHOd4vYABknjokQQiD+2/2bmZ0vnRoDQ38/JO5XLsZPvOt9APws\n3X41wsxOzaxjZtvEdbwxs2PgFhisiHkxl4Ff39i3NTP7Aj5DCLs+dAS8kvEa/0GWeQ3l5XZpeQ0Z\n5XaTjYYVzYc+sWP/AZyljsdjOiTeMj0DT/7qE/furoF3YAK0/foAjHwOL8B+4vh7/D5V0AXugSlw\nAWz4+KYfT/18N0Gce8Cjr/MVsFXKGq8xt+zy2uMqNrdLyWuPI3lu6+cHREQqlHpbRkRE/oGKu4hI\nhVTcRUQqpOIuIlIhFXcRkQqpuIuIVEjFXUSkQnMmvGYYKPh/XQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoi4TeNBGHle",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 10:\n",
        "\n",
        "Looking at the results above it can be said that the pixel values in the blue channels would be very small compared to red channel. True/False?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Ans: False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ugFd57zNwa",
        "colab_type": "text"
      },
      "source": [
        "# Autograd\n",
        "\n",
        "Pytorch supports automatic differentiation. The module which implements this is called **AutoGrad**. It calculates the gradients and keeps track in forward and backward passes. For primitive tensors, you need to enable or disable it using the `required_grad` flag. But, for advanced tensors, it is enabled by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMOp4aiou6JR",
        "colab_type": "code",
        "outputId": "9ac7e73f-800c-4b0b-d0bf-b8dfc26a8432",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad = True)\n",
        "print(a)\n",
        "result = a * 5\n",
        "print(result)\n",
        "\n",
        "# grad can be implicitly created only for scalar outputs\n",
        "# so let's calculate the sum here so that the output becomes a scalar and we can apply a backward pass\n",
        "mean_result = result.sum()\n",
        "print(mean_result)\n",
        "# calculate gradient\n",
        "mean_result.backward()\n",
        "# print gradient of a\n",
        "print(a.grad)\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.8948, 0.2556, 0.7809, 0.9732, 0.6847],\n",
            "        [0.3076, 0.9759, 0.9288, 0.9359, 0.0698],\n",
            "        [0.6890, 0.5161, 0.0719, 0.6695, 0.6571]], requires_grad=True)\n",
            "tensor([[4.4742, 1.2778, 3.9045, 4.8661, 3.4233],\n",
            "        [1.5378, 4.8797, 4.6442, 4.6793, 0.3491],\n",
            "        [3.4450, 2.5803, 0.3597, 3.3475, 3.2855]], grad_fn=<MulBackward0>)\n",
            "tensor(47.0541, grad_fn=<SumBackward0>)\n",
            "tensor([[5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.],\n",
            "        [5., 5., 5., 5., 5.]])\n",
            "tensor([[0.8948, 0.2556, 0.7809, 0.9732, 0.6847],\n",
            "        [0.3076, 0.9759, 0.9288, 0.9359, 0.0698],\n",
            "        [0.6890, 0.5161, 0.0719, 0.6695, 0.6571]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7OPWqlZcDGI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.sum([1.8365, 2.3827, 1.3032, 3.9060, 2.8554,2.2008, 1.2652, 4.8175, 2.4789, 2.6188,2.7512, 1.7930, 3.1524, 3.7307, 4.4118])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YUTsmiDdcrnf",
        "colab_type": "code",
        "outputId": "d1799167-255a-4602-a60d-4e2668e1d1f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "110/2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym0Amk2IGfLx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 11: \n",
        "\n",
        "Why the gradient of a is all 5s above?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PDgGq2R0k7I",
        "colab_type": "text"
      },
      "source": [
        "As we see, Pytorch automagically calculated the gradient value for us. It looks to be the correct value - we multiplied an input by 5, so the gradient of this operation equals to 5.\n",
        "\n",
        "# Disabling Autograd for tensors\n",
        "\n",
        "We don't need to compute gradients for all the variables that are involved in the pipeline. The Pytorch API provides 2 ways to disable autograd.\n",
        "\n",
        "`detach` - returns a copy of the tensor with autograd disabled. This \n",
        "\n",
        "1.   copy is built on the same memory as the original tensor, so in-place size / stride / storage changes (such as resize_ / resizeas / set / transpose) modifications are not allowed.\n",
        "2.   torch.no_grad() - It is a context manager that allows you to guard a series of operations from autograd without creating new tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqVG9fQb0cLW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "detached_a = a.detach()\n",
        "detached_result = detached_a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqpch2Be02J7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    detached_result = a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjh2rYOPJUAZ",
        "colab_type": "text"
      },
      "source": [
        "# Custom Network\n",
        "\n",
        "A fully-connected ReLU network with one hidden layer and no biases, trained to predict y from x by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses PyTorch tensors to manually compute the forward pass, loss, and backward pass.\n",
        "\n",
        "A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation.\n",
        "\n",
        "The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, just cast the Tensor to a cuda datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nf5RaB104Vp",
        "colab_type": "code",
        "outputId": "f48303a0-6c73-43b0-d97e-5cfebc46632b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y\n",
        "\n",
        "    # network:  x ---w1---> relu ---w2---> y_pred\n",
        "    \n",
        "    h = x.mm(w1)\n",
        "    h_relu = h.clamp(min=0) # replaces all values below 0 as 0\n",
        "    y_pred = h_relu.mm(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2*(y_pred - y) # derivative of loss w.r.t y_pred i.e change of y_pred that leads to change in loss\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "    grad_h = grad_h_relu.clone()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2\n",
        "print(w1,w2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 27870400.0\n",
            "1 21833706.0\n",
            "2 18824586.0\n",
            "3 16318100.0\n",
            "4 13432960.0\n",
            "5 10336553.0\n",
            "6 7467559.0\n",
            "7 5175940.5\n",
            "8 3529160.25\n",
            "9 2426408.5\n",
            "10 1711155.5\n",
            "11 1250420.875\n",
            "12 948992.4375\n",
            "13 745903.4375\n",
            "14 603770.875\n",
            "15 500344.0\n",
            "16 422067.0\n",
            "17 360810.625\n",
            "18 311562.5\n",
            "19 271068.03125\n",
            "20 237241.5625\n",
            "21 208650.140625\n",
            "22 184213.375\n",
            "23 163198.203125\n",
            "24 144994.46875\n",
            "25 129148.1796875\n",
            "26 115303.421875\n",
            "27 103161.7734375\n",
            "28 92480.53125\n",
            "29 83059.859375\n",
            "30 74730.625\n",
            "31 67342.9453125\n",
            "32 60786.1171875\n",
            "33 54950.85546875\n",
            "34 49745.58203125\n",
            "35 45089.5859375\n",
            "36 40919.734375\n",
            "37 37181.25390625\n",
            "38 33822.08203125\n",
            "39 30799.76953125\n",
            "40 28079.818359375\n",
            "41 25624.521484375\n",
            "42 23406.80859375\n",
            "43 21400.966796875\n",
            "44 19581.611328125\n",
            "45 17933.609375\n",
            "46 16438.0703125\n",
            "47 15079.216796875\n",
            "48 13843.07421875\n",
            "49 12717.8427734375\n",
            "50 11692.5986328125\n",
            "51 10757.1953125\n",
            "52 9903.955078125\n",
            "53 9123.400390625\n",
            "54 8410.1875\n",
            "55 7757.5439453125\n",
            "56 7159.81884765625\n",
            "57 6611.9013671875\n",
            "58 6109.21435546875\n",
            "59 5647.87158203125\n",
            "60 5223.9189453125\n",
            "61 4834.1845703125\n",
            "62 4475.5517578125\n",
            "63 4145.6962890625\n",
            "64 3841.799072265625\n",
            "65 3561.80810546875\n",
            "66 3303.763671875\n",
            "67 3065.710693359375\n",
            "68 2845.99658203125\n",
            "69 2642.884765625\n",
            "70 2455.20751953125\n",
            "71 2281.801025390625\n",
            "72 2121.406494140625\n",
            "73 1972.96630859375\n",
            "74 1835.4700927734375\n",
            "75 1708.182861328125\n",
            "76 1590.2237548828125\n",
            "77 1480.867431640625\n",
            "78 1379.445556640625\n",
            "79 1285.37646484375\n",
            "80 1198.04296875\n",
            "81 1117.06005859375\n",
            "82 1041.8028564453125\n",
            "83 971.9224853515625\n",
            "84 906.9526977539062\n",
            "85 846.54345703125\n",
            "86 790.3767700195312\n",
            "87 738.1196899414062\n",
            "88 689.5061645507812\n",
            "89 644.267578125\n",
            "90 602.1595458984375\n",
            "91 562.9219360351562\n",
            "92 526.3493041992188\n",
            "93 492.26019287109375\n",
            "94 460.483154296875\n",
            "95 430.8755187988281\n",
            "96 403.22747802734375\n",
            "97 377.4388732910156\n",
            "98 353.36907958984375\n",
            "99 330.90191650390625\n",
            "100 309.92376708984375\n",
            "101 290.347412109375\n",
            "102 272.05950927734375\n",
            "103 254.96536254882812\n",
            "104 238.98536682128906\n",
            "105 224.0457763671875\n",
            "106 210.08660888671875\n",
            "107 197.0186004638672\n",
            "108 184.8014678955078\n",
            "109 173.36895751953125\n",
            "110 162.67723083496094\n",
            "111 152.66397094726562\n",
            "112 143.28958129882812\n",
            "113 134.50836181640625\n",
            "114 126.2916259765625\n",
            "115 118.5914306640625\n",
            "116 111.3771743774414\n",
            "117 104.61781311035156\n",
            "118 98.28273010253906\n",
            "119 92.3453369140625\n",
            "120 86.77633666992188\n",
            "121 81.55471801757812\n",
            "122 76.66193389892578\n",
            "123 72.06817626953125\n",
            "124 67.7588882446289\n",
            "125 63.71699905395508\n",
            "126 59.92487716674805\n",
            "127 56.363983154296875\n",
            "128 53.02205276489258\n",
            "129 49.885589599609375\n",
            "130 46.938114166259766\n",
            "131 44.17130661010742\n",
            "132 41.57270050048828\n",
            "133 39.13188552856445\n",
            "134 36.838191986083984\n",
            "135 34.68214797973633\n",
            "136 32.65663146972656\n",
            "137 30.753528594970703\n",
            "138 28.96442985534668\n",
            "139 27.281150817871094\n",
            "140 25.699176788330078\n",
            "141 24.21231460571289\n",
            "142 22.813283920288086\n",
            "143 21.496721267700195\n",
            "144 20.259313583374023\n",
            "145 19.094661712646484\n",
            "146 17.998090744018555\n",
            "147 16.96703338623047\n",
            "148 15.996564865112305\n",
            "149 15.082950592041016\n",
            "150 14.22298812866211\n",
            "151 13.412796020507812\n",
            "152 12.650137901306152\n",
            "153 11.93221378326416\n",
            "154 11.256220817565918\n",
            "155 10.618792533874512\n",
            "156 10.018871307373047\n",
            "157 9.45364761352539\n",
            "158 8.921207427978516\n",
            "159 8.419200897216797\n",
            "160 7.94600772857666\n",
            "161 7.5000762939453125\n",
            "162 7.080451011657715\n",
            "163 6.683893203735352\n",
            "164 6.310582160949707\n",
            "165 5.958777904510498\n",
            "166 5.6266045570373535\n",
            "167 5.313380241394043\n",
            "168 5.018183708190918\n",
            "169 4.73994255065918\n",
            "170 4.477222919464111\n",
            "171 4.229254245758057\n",
            "172 3.9953365325927734\n",
            "173 3.7746400833129883\n",
            "174 3.566627025604248\n",
            "175 3.370272397994995\n",
            "176 3.1849589347839355\n",
            "177 3.0100362300872803\n",
            "178 2.844865083694458\n",
            "179 2.688850164413452\n",
            "180 2.5416879653930664\n",
            "181 2.4027047157287598\n",
            "182 2.2715342044830322\n",
            "183 2.1477162837982178\n",
            "184 2.030630588531494\n",
            "185 1.9201171398162842\n",
            "186 1.815648078918457\n",
            "187 1.7169556617736816\n",
            "188 1.6237233877182007\n",
            "189 1.5357513427734375\n",
            "190 1.4527065753936768\n",
            "191 1.374141812324524\n",
            "192 1.2997853755950928\n",
            "193 1.2296838760375977\n",
            "194 1.1634008884429932\n",
            "195 1.100710153579712\n",
            "196 1.0415209531784058\n",
            "197 0.9855967164039612\n",
            "198 0.9326099753379822\n",
            "199 0.8825579285621643\n",
            "200 0.8351934552192688\n",
            "201 0.7905447483062744\n",
            "202 0.7482872009277344\n",
            "203 0.7083181142807007\n",
            "204 0.6704999804496765\n",
            "205 0.634772539138794\n",
            "206 0.6008687615394592\n",
            "207 0.5688633918762207\n",
            "208 0.5386238098144531\n",
            "209 0.5099371671676636\n",
            "210 0.48284804821014404\n",
            "211 0.457258403301239\n",
            "212 0.4329897165298462\n",
            "213 0.4100598692893982\n",
            "214 0.38832855224609375\n",
            "215 0.36781373620033264\n",
            "216 0.3484586179256439\n",
            "217 0.32997244596481323\n",
            "218 0.31263282895088196\n",
            "219 0.29610544443130493\n",
            "220 0.2805374264717102\n",
            "221 0.2657358944416046\n",
            "222 0.2517521381378174\n",
            "223 0.23853877186775208\n",
            "224 0.22603575885295868\n",
            "225 0.2141476422548294\n",
            "226 0.20290839672088623\n",
            "227 0.19225916266441345\n",
            "228 0.1822238564491272\n",
            "229 0.17272613942623138\n",
            "230 0.1636839509010315\n",
            "231 0.15509222447872162\n",
            "232 0.14701856672763824\n",
            "233 0.13935792446136475\n",
            "234 0.1320635825395584\n",
            "235 0.12514987587928772\n",
            "236 0.11865416169166565\n",
            "237 0.11247259378433228\n",
            "238 0.10664094984531403\n",
            "239 0.10109665244817734\n",
            "240 0.09583790600299835\n",
            "241 0.09087727218866348\n",
            "242 0.08617379516363144\n",
            "243 0.08169406652450562\n",
            "244 0.07745897769927979\n",
            "245 0.07343552261590958\n",
            "246 0.06963620334863663\n",
            "247 0.06601998209953308\n",
            "248 0.06260333210229874\n",
            "249 0.059372372925281525\n",
            "250 0.05633683502674103\n",
            "251 0.05341819301247597\n",
            "252 0.05067512392997742\n",
            "253 0.048070453107357025\n",
            "254 0.0455930270254612\n",
            "255 0.043245017528533936\n",
            "256 0.04103722423315048\n",
            "257 0.03892645612359047\n",
            "258 0.03691624850034714\n",
            "259 0.03502708673477173\n",
            "260 0.03321219980716705\n",
            "261 0.03152166306972504\n",
            "262 0.029910385608673096\n",
            "263 0.02837761491537094\n",
            "264 0.026932576671242714\n",
            "265 0.02555766887962818\n",
            "266 0.02424916811287403\n",
            "267 0.023026060312986374\n",
            "268 0.021846231073141098\n",
            "269 0.020738760009407997\n",
            "270 0.01968216523528099\n",
            "271 0.018691690638661385\n",
            "272 0.01774596981704235\n",
            "273 0.016840945929288864\n",
            "274 0.015978991985321045\n",
            "275 0.0151733523234725\n",
            "276 0.014404195360839367\n",
            "277 0.01368315052241087\n",
            "278 0.01299627311527729\n",
            "279 0.012339405715465546\n",
            "280 0.011717332527041435\n",
            "281 0.011139705777168274\n",
            "282 0.010578879155218601\n",
            "283 0.01004776544868946\n",
            "284 0.009551051072776318\n",
            "285 0.009082440286874771\n",
            "286 0.008635531179606915\n",
            "287 0.008204109966754913\n",
            "288 0.007804859429597855\n",
            "289 0.007419876754283905\n",
            "290 0.007057389244437218\n",
            "291 0.006717096082866192\n",
            "292 0.006384427193552256\n",
            "293 0.00607683602720499\n",
            "294 0.005777761805802584\n",
            "295 0.005497502163052559\n",
            "296 0.005231617484241724\n",
            "297 0.0049849506467580795\n",
            "298 0.004748220089823008\n",
            "299 0.004521265160292387\n",
            "300 0.004304558038711548\n",
            "301 0.004105427302420139\n",
            "302 0.003912266809493303\n",
            "303 0.0037289990577846766\n",
            "304 0.0035551877226680517\n",
            "305 0.0033927252516150475\n",
            "306 0.0032356143929064274\n",
            "307 0.003085786709561944\n",
            "308 0.002945347223430872\n",
            "309 0.0028140698559582233\n",
            "310 0.0026865974068641663\n",
            "311 0.0025660190731287003\n",
            "312 0.0024541611783206463\n",
            "313 0.0023449219297617674\n",
            "314 0.0022433314006775618\n",
            "315 0.002141049597412348\n",
            "316 0.0020517471712082624\n",
            "317 0.0019617308862507343\n",
            "318 0.0018800077959895134\n",
            "319 0.0017990884371101856\n",
            "320 0.0017239899607375264\n",
            "321 0.0016532387817278504\n",
            "322 0.001586397411301732\n",
            "323 0.0015208788681775331\n",
            "324 0.0014583459123969078\n",
            "325 0.0013972967863082886\n",
            "326 0.0013415194116532803\n",
            "327 0.0012866699835285544\n",
            "328 0.0012357913656160235\n",
            "329 0.001188416383229196\n",
            "330 0.0011410311562940478\n",
            "331 0.0010966877453029156\n",
            "332 0.0010536074405536056\n",
            "333 0.0010140895610675216\n",
            "334 0.0009764368296600878\n",
            "335 0.0009388541802763939\n",
            "336 0.0009034895338118076\n",
            "337 0.0008710511610843241\n",
            "338 0.0008384495158679783\n",
            "339 0.0008089702459983528\n",
            "340 0.000780118047259748\n",
            "341 0.0007521437364630401\n",
            "342 0.0007260636775754392\n",
            "343 0.0006994829163886607\n",
            "344 0.0006760356482118368\n",
            "345 0.0006515247514471412\n",
            "346 0.0006298758671618998\n",
            "347 0.0006089760572649539\n",
            "348 0.0005876274080947042\n",
            "349 0.0005691133555956185\n",
            "350 0.0005498229875229299\n",
            "351 0.0005318879848346114\n",
            "352 0.0005144195165485144\n",
            "353 0.0004969629808329046\n",
            "354 0.0004822681949008256\n",
            "355 0.0004664151056203991\n",
            "356 0.00045170000521466136\n",
            "357 0.00043685853597708046\n",
            "358 0.00042335939360782504\n",
            "359 0.00041081273229792714\n",
            "360 0.0003987362142652273\n",
            "361 0.00038628571201115847\n",
            "362 0.00037506117951124907\n",
            "363 0.0003636122273746878\n",
            "364 0.0003533510898705572\n",
            "365 0.00034309900365769863\n",
            "366 0.00033271018764935434\n",
            "367 0.0003233634924981743\n",
            "368 0.00031341909198090434\n",
            "369 0.0003046153287868947\n",
            "370 0.0002966813917737454\n",
            "371 0.0002880469255615026\n",
            "372 0.0002808148565236479\n",
            "373 0.00027282536029815674\n",
            "374 0.00026544189313426614\n",
            "375 0.0002583846217021346\n",
            "376 0.0002512337523512542\n",
            "377 0.0002450227621011436\n",
            "378 0.00023814653104636818\n",
            "379 0.00023225088079925627\n",
            "380 0.00022539764177054167\n",
            "381 0.0002200528106186539\n",
            "382 0.0002145918842870742\n",
            "383 0.00020965057774446905\n",
            "384 0.00020437499915715307\n",
            "385 0.00019949869601987302\n",
            "386 0.00019426271319389343\n",
            "387 0.00019017176236957312\n",
            "388 0.00018525877385400236\n",
            "389 0.00018067241762764752\n",
            "390 0.00017599735292606056\n",
            "391 0.0001721052685752511\n",
            "392 0.00016769321518950164\n",
            "393 0.0001638737739995122\n",
            "394 0.0001604126882739365\n",
            "395 0.0001567583531141281\n",
            "396 0.0001528956781839952\n",
            "397 0.00014925583673175424\n",
            "398 0.00014621643640566617\n",
            "399 0.0001431069104000926\n",
            "400 0.00014025188283994794\n",
            "401 0.000136532835313119\n",
            "402 0.00013363487960305065\n",
            "403 0.0001306161575485021\n",
            "404 0.00012790324399247766\n",
            "405 0.00012560884351842105\n",
            "406 0.00012287979188840836\n",
            "407 0.00012019834684906527\n",
            "408 0.00011738147441064939\n",
            "409 0.00011534628720255569\n",
            "410 0.00011328465188853443\n",
            "411 0.0001110783705371432\n",
            "412 0.0001086124757421203\n",
            "413 0.00010691354691516608\n",
            "414 0.00010492988076293841\n",
            "415 0.00010303481394657865\n",
            "416 0.00010087067494168878\n",
            "417 9.878868877422065e-05\n",
            "418 9.701414091978222e-05\n",
            "419 9.532320837024599e-05\n",
            "420 9.328755550086498e-05\n",
            "421 9.164805669570342e-05\n",
            "422 9.001073340186849e-05\n",
            "423 8.8369837612845e-05\n",
            "424 8.64864414324984e-05\n",
            "425 8.532376523362473e-05\n",
            "426 8.350906864507124e-05\n",
            "427 8.179844007827342e-05\n",
            "428 8.067350427154452e-05\n",
            "429 7.9275036114268e-05\n",
            "430 7.78861649450846e-05\n",
            "431 7.668950274819508e-05\n",
            "432 7.547437417088076e-05\n",
            "433 7.40623363526538e-05\n",
            "434 7.245351298479363e-05\n",
            "435 7.145163544919342e-05\n",
            "436 7.02390680089593e-05\n",
            "437 6.906135240569711e-05\n",
            "438 6.803154974477366e-05\n",
            "439 6.701864913338795e-05\n",
            "440 6.561772170243785e-05\n",
            "441 6.475923873949796e-05\n",
            "442 6.356992525979877e-05\n",
            "443 6.272926839301363e-05\n",
            "444 6.170492997625843e-05\n",
            "445 6.073057738831267e-05\n",
            "446 5.9686164604499936e-05\n",
            "447 5.897856317460537e-05\n",
            "448 5.7938719692174345e-05\n",
            "449 5.7158918934874237e-05\n",
            "450 5.624205368803814e-05\n",
            "451 5.545143358176574e-05\n",
            "452 5.481158950715326e-05\n",
            "453 5.420252273324877e-05\n",
            "454 5.320585842127912e-05\n",
            "455 5.244125713943504e-05\n",
            "456 5.1969043852295727e-05\n",
            "457 5.091956700198352e-05\n",
            "458 5.029689418734051e-05\n",
            "459 4.946042099618353e-05\n",
            "460 4.886963506578468e-05\n",
            "461 4.791966421180405e-05\n",
            "462 4.7232286306098104e-05\n",
            "463 4.648665708373301e-05\n",
            "464 4.5861561375204474e-05\n",
            "465 4.5211141696199775e-05\n",
            "466 4.458163311937824e-05\n",
            "467 4.4003383663948625e-05\n",
            "468 4.333630567998625e-05\n",
            "469 4.277854168321937e-05\n",
            "470 4.212262501823716e-05\n",
            "471 4.183611599728465e-05\n",
            "472 4.145628190599382e-05\n",
            "473 4.088189598405734e-05\n",
            "474 4.034188896184787e-05\n",
            "475 3.966511940234341e-05\n",
            "476 3.924455086234957e-05\n",
            "477 3.880623262375593e-05\n",
            "478 3.841058423859067e-05\n",
            "479 3.7821831938344985e-05\n",
            "480 3.7459973100339994e-05\n",
            "481 3.6979305150453e-05\n",
            "482 3.6439349059946835e-05\n",
            "483 3.582616773201153e-05\n",
            "484 3.53921641362831e-05\n",
            "485 3.514467607601546e-05\n",
            "486 3.480894156382419e-05\n",
            "487 3.437355917412788e-05\n",
            "488 3.3908076147781685e-05\n",
            "489 3.3487296605017036e-05\n",
            "490 3.314038258395158e-05\n",
            "491 3.269457374699414e-05\n",
            "492 3.226038461434655e-05\n",
            "493 3.194747114321217e-05\n",
            "494 3.157337778247893e-05\n",
            "495 3.12009833578486e-05\n",
            "496 3.085752541664988e-05\n",
            "497 3.060387462028302e-05\n",
            "498 3.039320290554315e-05\n",
            "499 3.0013707146281376e-05\n",
            "tensor([[ 0.9754, -1.1159,  0.0265,  ...,  0.1641, -0.7607,  0.2672],\n",
            "        [ 0.7012, -0.8339,  0.8845,  ..., -0.0689, -1.3453,  0.2666],\n",
            "        [ 0.0105, -0.6182, -1.1527,  ...,  0.9926,  0.3766, -0.2960],\n",
            "        ...,\n",
            "        [-0.7587,  1.8046,  0.0354,  ..., -0.3061,  1.9948,  0.7740],\n",
            "        [ 0.6567, -0.3783, -0.4682,  ..., -0.6279, -0.6022, -2.7700],\n",
            "        [ 0.2767,  0.6113, -0.1151,  ..., -1.2892, -0.7368, -0.7997]]) tensor([[-3.5445e-01, -7.3993e-01, -4.9093e-01, -5.6230e-01,  4.0086e-01,\n",
            "          8.3145e-01, -6.9456e-01, -6.7651e-01,  1.6995e-01, -3.5673e-01],\n",
            "        [-1.0775e+00, -3.9300e-01, -9.7043e-01, -7.0227e-02, -6.4096e-01,\n",
            "          4.8033e-02,  1.9235e+00, -8.3508e-01, -1.3135e-01,  7.3372e-01],\n",
            "        [-2.6351e-01, -5.5947e-01, -4.6100e-02, -9.5667e-01, -9.0581e-01,\n",
            "          5.7055e-03, -2.3431e+00,  2.5344e-02,  9.5396e-01, -1.7957e+00],\n",
            "        [-1.4837e+00,  1.7217e+00, -5.3679e-01,  1.7619e-01, -7.8475e-01,\n",
            "         -9.0739e-03, -8.3693e-01,  8.6579e-02,  1.0419e-01,  6.1692e-01],\n",
            "        [-7.2519e-01, -2.0591e-01, -3.6743e-01,  4.3926e-01,  1.5309e-02,\n",
            "          9.9780e-01,  7.9241e-01,  1.6109e+00, -1.2886e-01, -1.6431e+00],\n",
            "        [ 1.3021e-01, -1.2246e+00, -2.6358e-01,  1.1334e+00,  7.3758e-01,\n",
            "          4.8019e-01,  1.1257e+00, -3.0341e-02,  4.3611e-02, -3.1699e-01],\n",
            "        [-5.3328e-01, -1.2269e+00, -1.3546e+00,  2.6565e-02, -6.3493e-01,\n",
            "         -3.2456e-01, -1.4331e-01, -2.2017e-01,  2.0883e-01, -2.3309e-01],\n",
            "        [-1.8347e-01, -5.4420e-01, -1.2257e+00,  7.0559e-01,  7.4324e-01,\n",
            "         -1.1815e-02,  1.1738e+00, -6.7215e-01, -7.7700e-01,  5.2534e-01],\n",
            "        [ 1.6880e-01,  2.2789e-01, -1.1857e-01,  3.9758e-01,  8.7258e-02,\n",
            "         -3.2112e-02,  4.7406e-01,  1.0474e+00, -1.5349e-01,  1.9023e-02],\n",
            "        [-3.2635e-01,  8.6676e-01, -2.3823e-01, -8.9546e-01,  8.4586e-01,\n",
            "          5.9778e-02, -1.8024e-01,  2.6845e-01, -6.1766e-01, -5.1835e-01],\n",
            "        [-8.0764e-01, -1.2518e+00, -1.1520e+00,  2.4168e-01,  1.1395e+00,\n",
            "         -7.3218e-01,  6.6603e-01, -4.0617e-01,  1.2756e+00,  1.3525e+00],\n",
            "        [-2.0686e-01, -4.3388e-02, -1.5753e-03,  3.4143e-01, -6.5628e-01,\n",
            "         -1.4188e+00, -8.5586e-01,  8.8088e-01, -5.9048e-01,  1.1819e+00],\n",
            "        [-7.7830e-01, -2.0563e-01,  2.8070e-01,  1.0741e-01,  1.1158e+00,\n",
            "          9.2120e-01,  1.2135e+00,  5.3946e-01,  4.1307e-01, -3.0765e-01],\n",
            "        [-3.2784e-01, -8.4587e-01, -2.3030e-01,  3.7763e-01,  4.8536e-01,\n",
            "          7.4510e-01,  2.9045e-01,  8.9696e-01,  5.2373e-01,  1.6566e+00],\n",
            "        [ 5.3738e-01,  3.2951e-01,  3.3442e-01,  6.9183e-01,  2.6005e-01,\n",
            "          3.1758e-02, -1.3675e-01,  8.4329e-01, -4.8690e-01, -1.2272e+00],\n",
            "        [-6.2892e-01, -2.1190e+00,  3.6258e-01, -1.1546e+00, -6.6584e-02,\n",
            "          1.8313e+00, -5.7370e-01, -7.9751e-01, -5.0182e-01, -3.4291e-03],\n",
            "        [-1.5252e+00,  4.9690e-02,  1.6628e+00, -5.5350e-01,  3.8530e-01,\n",
            "         -7.4182e-01, -1.0289e+00, -1.6067e+00, -3.6892e-01, -1.6161e-01],\n",
            "        [-6.8595e-01, -1.3554e+00,  7.1173e-01,  4.3559e-01,  4.7451e-01,\n",
            "          1.3365e+00, -1.0322e+00,  6.2614e-01,  9.4840e-01,  9.5965e-01],\n",
            "        [-8.1091e-02,  8.8551e-01, -1.0491e+00, -1.1418e+00, -5.7002e-01,\n",
            "          5.0679e-01,  1.4226e-01, -2.8187e-01, -1.4749e+00,  1.8026e+00],\n",
            "        [ 1.3677e+00,  1.6506e-01, -3.0253e-01,  7.0463e-01, -8.0796e-01,\n",
            "         -2.7051e-01,  4.4497e-01, -2.0808e-01,  1.2924e-01,  2.4284e-01],\n",
            "        [-4.2642e-01,  1.1646e+00, -4.6238e-02,  1.4097e+00, -6.0275e-01,\n",
            "         -1.6843e+00, -1.0700e-01,  3.0531e-01, -7.6603e-02, -8.5413e-01],\n",
            "        [-4.2666e-01,  5.2196e-01,  1.1226e+00,  3.6349e-01,  3.2816e-01,\n",
            "          2.1240e-01,  3.7204e-01, -9.0788e-03,  1.3446e-02, -7.3801e-01],\n",
            "        [-1.3250e+00, -7.5706e-01,  1.3077e+00,  7.6372e-01,  5.1429e-01,\n",
            "          7.9448e-01, -4.4387e-01, -3.8357e-01,  1.1010e-01, -1.3124e+00],\n",
            "        [ 1.1105e+00, -1.6872e-01, -1.1223e+00, -1.1898e+00, -1.4797e+00,\n",
            "         -1.5665e+00, -3.7728e-01, -1.5404e-01, -1.5963e-01, -6.8232e-01],\n",
            "        [-9.0649e-01, -3.9867e-01, -4.3208e-01, -8.8414e-01,  1.6889e+00,\n",
            "          1.4418e+00,  5.5818e-02, -2.5711e+00, -1.1539e+00, -1.5139e+00],\n",
            "        [ 1.5721e-02, -8.0450e-01,  1.2052e+00, -2.8471e-01, -1.7678e-01,\n",
            "          3.6302e-02,  5.5126e-01,  2.7820e-01, -6.7514e-01, -1.1247e+00],\n",
            "        [ 1.5461e+00, -7.2959e-01,  6.2623e-02,  1.5873e-01,  6.3596e-01,\n",
            "          1.0829e+00,  5.7829e-01, -1.2838e-01, -8.9319e-01, -1.0623e+00],\n",
            "        [ 7.0392e-01,  8.4494e-01,  2.3241e-01, -1.0968e-02, -7.4773e-01,\n",
            "         -3.6393e-02,  9.6634e-01,  4.9657e-01, -5.2757e-01, -5.0573e-01],\n",
            "        [-4.4808e-02,  1.6043e+00,  1.8763e-01,  4.6801e-01,  6.5035e-02,\n",
            "          8.6605e-01, -1.8445e-01, -9.3693e-01,  5.5115e-01,  5.0385e-01],\n",
            "        [ 4.3051e-01,  6.5322e-01,  1.0501e+00,  2.6701e-02, -6.0034e-01,\n",
            "         -1.3109e+00,  3.5989e-01, -2.9743e-01, -2.5058e-01, -4.6298e-01],\n",
            "        [-1.7617e-01, -4.1915e-02,  9.6044e-01,  1.2679e-01,  1.9673e-01,\n",
            "          8.2105e-01, -4.1004e-01,  3.7999e-01, -5.4156e-01, -4.0948e-01],\n",
            "        [-5.8061e-01, -7.8749e-01, -6.9915e-01, -7.7200e-02, -1.1178e+00,\n",
            "         -3.6806e-01, -3.2641e-01,  2.2017e+00,  2.2025e-01,  6.7364e-01],\n",
            "        [-2.4478e-01,  9.4225e-01, -8.8069e-01, -6.2998e-01, -1.9306e-01,\n",
            "          1.2264e+00,  4.9222e-01,  8.2811e-01,  1.1053e+00, -1.6980e+00],\n",
            "        [-2.2896e+00, -4.1400e-01,  1.2801e+00,  3.0121e-01,  5.9425e-01,\n",
            "         -1.9841e-01,  6.4816e-01, -4.9695e-02, -7.4724e-01, -8.9254e-01],\n",
            "        [ 9.6549e-01,  1.2330e-02, -2.4896e+00, -3.3962e-01, -8.8212e-01,\n",
            "         -1.4424e+00,  1.8623e-01,  1.3420e-01,  8.1019e-01, -5.1256e-02],\n",
            "        [-1.3159e+00, -4.2938e-02, -1.0005e+00, -2.1470e-01, -1.2820e-01,\n",
            "         -1.5670e+00, -8.7014e-01, -1.2356e+00, -6.9271e-01,  1.0295e+00],\n",
            "        [ 1.3466e-01, -6.3152e-01, -1.2923e+00,  1.4809e+00,  1.0512e+00,\n",
            "         -7.6264e-01, -1.8567e+00, -1.6114e+00, -8.2428e-01, -4.9746e-01],\n",
            "        [ 2.5591e-01,  7.7247e-01, -4.7933e-01, -4.9130e-01, -1.7051e-02,\n",
            "         -3.0743e-01, -2.8084e-01,  1.5002e+00, -4.9260e-01,  1.3108e-01],\n",
            "        [-3.6241e-01, -1.7624e+00, -3.0842e-01,  1.0935e+00,  9.5358e-01,\n",
            "         -3.8254e-01,  1.3089e+00, -1.2800e+00, -4.8907e-02,  1.8526e+00],\n",
            "        [-1.5653e+00, -3.2968e-01, -1.3788e+00, -9.2528e-01,  1.6154e-01,\n",
            "          4.4546e-01,  6.9852e-01, -3.7191e-01, -4.4650e-01,  4.4130e-01],\n",
            "        [ 6.1155e-01,  6.5431e-01,  8.5894e-01, -1.2944e+00,  2.4131e-01,\n",
            "          2.5256e+00,  1.1339e+00, -1.2062e+00,  2.2425e-01, -7.6384e-01],\n",
            "        [ 4.8141e-01,  1.9645e+00, -8.0732e-01,  1.0663e+00, -1.2773e+00,\n",
            "         -3.3427e-01, -9.7152e-01,  9.5236e-02, -1.9095e-01, -6.4082e-01],\n",
            "        [ 5.3923e-01,  3.1467e-01,  2.6267e-01, -1.1988e+00,  7.0692e-01,\n",
            "         -2.7098e-01, -2.1586e+00, -9.0189e-01, -3.2966e-01,  2.4389e-01],\n",
            "        [-8.1609e-01,  2.5258e-01,  1.7307e+00,  9.0483e-01,  1.3537e-01,\n",
            "         -5.0239e-01,  9.5836e-01, -8.5498e-01,  1.6848e+00,  4.4930e-01],\n",
            "        [ 1.8161e+00,  6.6717e-01,  3.4500e-01,  1.1024e+00, -1.4169e+00,\n",
            "         -6.4098e-01,  5.6042e-01,  3.4594e-01,  9.5356e-01, -1.2408e+00],\n",
            "        [ 6.8485e-01,  6.3870e-01, -9.1426e-01, -9.2053e-03, -4.2829e-01,\n",
            "          8.6897e-02,  7.8732e-01, -4.7227e-01, -9.7044e-01,  4.3202e-02],\n",
            "        [ 2.0471e-01, -1.8687e-01, -3.2834e-01, -3.6511e-01, -1.3238e+00,\n",
            "         -2.7674e-01,  4.2789e-01,  1.0267e-01,  3.2945e-02,  7.1234e-01],\n",
            "        [-5.3240e-01, -1.5261e+00,  9.2068e-02, -1.2287e+00,  1.9673e-01,\n",
            "          4.6750e-01,  1.2432e+00,  1.1040e+00, -7.5268e-03,  6.1912e-01],\n",
            "        [ 1.9351e+00,  1.5437e+00, -7.0051e-02,  1.5401e+00, -2.8298e-01,\n",
            "         -1.4864e+00, -3.3213e-01, -9.4743e-01, -8.6920e-01,  1.2102e+00],\n",
            "        [-1.1069e+00, -7.6856e-02,  1.0273e+00,  9.7095e-01,  1.0039e+00,\n",
            "          6.3331e-01,  7.8344e-01, -7.7243e-01, -8.5824e-02,  7.4425e-01],\n",
            "        [ 2.2922e-02,  1.0118e-01,  6.0181e-01,  1.2686e-01, -6.9940e-01,\n",
            "         -7.6327e-02, -1.5361e+00,  3.9626e-01,  1.4608e+00, -1.1726e+00],\n",
            "        [-2.8478e-01,  1.2923e+00,  6.6821e-01, -2.8530e-01, -6.8036e-01,\n",
            "          8.7585e-03, -8.3073e-01, -4.7959e-01,  2.7799e-01,  4.9818e-01],\n",
            "        [-5.9591e-01,  9.5594e-01, -1.1167e-01,  4.2816e-01, -7.3451e-01,\n",
            "         -1.9795e+00, -3.6509e-01,  6.0805e-02,  6.3278e-01,  6.0209e-01],\n",
            "        [-1.9597e-01,  5.1409e-02,  6.3648e-01, -8.4334e-01,  1.2825e-01,\n",
            "          6.9047e-01,  4.2424e-01,  1.2326e-01, -1.7460e-01,  5.7171e-01],\n",
            "        [ 1.6046e-01,  8.3972e-01, -5.7858e-01,  7.4861e-01, -1.1054e-01,\n",
            "         -9.6108e-01, -5.1680e-02,  3.1007e-01,  9.7452e-01, -9.7518e-01],\n",
            "        [ 1.0913e+00, -3.0372e-01,  3.3693e-01,  2.2265e-02,  8.1247e-02,\n",
            "          1.6333e+00, -1.0467e+00,  2.3837e-01, -1.4828e+00, -1.1612e+00],\n",
            "        [ 2.9554e-01,  6.0253e-01, -9.7188e-01,  4.9361e-01, -1.1581e+00,\n",
            "         -9.8340e-02, -3.8745e-01, -2.8015e-01,  1.0964e+00,  1.7047e+00],\n",
            "        [ 5.8115e-01,  1.4201e-02,  1.1678e+00, -4.3656e-01, -2.0548e-01,\n",
            "          1.2415e+00,  1.2882e+00,  1.0449e-01, -1.4317e+00,  6.5460e-02],\n",
            "        [ 7.9517e-01,  5.7982e-01, -2.2893e-01, -1.7200e-01, -2.0196e+00,\n",
            "         -1.0551e-01,  4.2160e-01, -1.4804e+00, -8.0875e-04,  1.5947e+00],\n",
            "        [-1.0775e+00,  1.3623e-01, -1.3464e+00,  8.0150e-01, -3.6590e-01,\n",
            "         -1.0147e+00,  7.8062e-01, -1.3897e-01,  5.8349e-01,  1.1708e+00],\n",
            "        [-1.2645e+00, -1.5981e-01, -5.9028e-01,  7.1653e-01, -7.0602e-01,\n",
            "         -6.1794e-01, -1.6764e-02,  1.2009e+00,  4.4905e-01,  2.8316e-01],\n",
            "        [ 1.8927e+00, -5.2188e-01, -5.2693e-01,  1.0102e+00, -2.8321e-02,\n",
            "         -1.0990e+00,  1.0488e+00,  4.3041e-01,  3.9746e-01,  4.1077e-01],\n",
            "        [ 2.7104e-01, -1.2069e+00, -5.1231e-01,  6.7721e-01,  6.4229e-01,\n",
            "          1.5389e+00, -2.0416e-01,  4.9490e-01,  6.8765e-01, -7.6980e-01],\n",
            "        [-3.8271e-01,  7.6167e-01, -8.7048e-01,  1.7631e-01, -2.1023e+00,\n",
            "         -2.5246e-01, -3.0992e-01, -2.4957e-01, -1.0033e+00,  8.5418e-01],\n",
            "        [ 1.2633e-01, -5.3382e-01,  4.9746e-02, -1.5376e-01, -4.6585e-02,\n",
            "          6.4596e-01, -1.0728e+00,  9.0797e-01,  3.9933e-01, -7.0583e-02],\n",
            "        [ 6.6312e-01, -3.0220e-01,  7.4751e-01,  8.2985e-02,  3.7736e-01,\n",
            "          9.6617e-01,  1.0900e+00,  1.6338e+00, -2.4312e-01,  1.7814e-01],\n",
            "        [-1.2171e+00, -1.2068e+00,  3.9970e-02,  4.7800e-01,  1.5070e+00,\n",
            "         -1.0721e+00,  5.8693e-01,  3.9025e-01, -2.6272e-01, -7.0493e-02],\n",
            "        [-2.0684e+00,  7.1119e-01, -6.3136e-01, -7.4928e-01,  5.3476e-01,\n",
            "          3.1104e-01,  1.1485e+00, -1.0579e-01, -1.1286e+00,  1.0138e+00],\n",
            "        [ 5.8575e-01,  2.9094e-01,  1.6491e+00,  6.8620e-01,  8.1706e-01,\n",
            "          6.2208e-01,  3.9438e-01, -2.7257e-01,  5.2556e-01,  1.2570e+00],\n",
            "        [ 5.4411e-01, -6.5825e-01, -4.8477e-01, -6.8638e-01,  3.9444e-01,\n",
            "         -2.0943e+00,  7.3344e-01, -3.1909e-01,  2.2581e-01, -5.6383e-01],\n",
            "        [-4.4140e-01,  3.1303e-01,  8.2146e-01,  5.7060e-01,  9.8648e-02,\n",
            "         -1.2218e+00, -1.0650e+00, -1.0858e+00,  1.2402e+00, -4.9998e-01],\n",
            "        [ 1.9985e-01,  7.5774e-01,  2.9330e-01,  4.9411e-01,  1.0813e+00,\n",
            "         -6.5440e-01,  1.0367e+00, -4.7635e-01,  6.9252e-01,  5.4608e-01],\n",
            "        [-5.2407e-01, -2.5220e-01,  1.7894e+00,  3.7584e-01,  2.2200e-01,\n",
            "          2.7031e-01,  1.2674e-01,  1.1463e-01,  9.2952e-01, -1.1131e+00],\n",
            "        [ 4.8292e-01,  7.8686e-01, -6.1996e-01,  8.0513e-01,  7.8580e-01,\n",
            "          4.1522e-01, -2.5509e-01, -2.5392e-01,  5.5520e-01, -8.4461e-01],\n",
            "        [ 1.2701e+00, -7.4365e-01,  4.2991e-01, -4.0493e-01,  1.7813e+00,\n",
            "          1.7080e+00,  4.3441e-01, -4.6150e-01,  1.6390e-01, -2.8805e-01],\n",
            "        [-1.1372e+00, -5.3186e-01, -1.0545e+00, -1.5347e+00,  3.8423e-01,\n",
            "         -7.0523e-02, -1.3589e+00, -5.7349e-01,  2.9625e-01, -4.6116e-01],\n",
            "        [-2.7040e-01, -1.3543e+00,  1.3637e-01,  8.3377e-01, -1.0807e+00,\n",
            "          3.8112e-01, -7.0091e-01,  1.2433e+00,  1.2389e+00, -6.5447e-01],\n",
            "        [ 8.2459e-01,  1.1087e+00,  9.4402e-01,  6.1804e-01, -7.1922e-01,\n",
            "          4.1097e-01,  5.1561e-01,  1.6080e+00,  3.8116e-01, -1.1650e-01],\n",
            "        [ 5.0393e-01, -1.2766e+00, -2.9336e-01,  3.9324e-01,  4.6244e-01,\n",
            "         -1.2441e+00, -2.4377e+00,  1.7697e-01,  3.9678e-01, -1.2620e+00],\n",
            "        [ 1.3297e+00, -9.2498e-01,  1.1129e+00, -6.6044e-01,  1.1965e+00,\n",
            "          1.1216e+00,  1.3553e-02, -2.9203e-01,  1.8618e+00,  8.6337e-01],\n",
            "        [ 9.3136e-02,  4.3871e-01, -5.6354e-01, -5.2801e-01, -6.3516e-01,\n",
            "          1.2161e+00, -1.4644e-01,  8.7889e-01, -2.1782e+00, -1.5149e-01],\n",
            "        [-5.5665e-01,  7.7832e-01,  1.9980e-01, -4.7667e-01,  1.5368e+00,\n",
            "         -8.9681e-01,  7.0462e-02,  6.4260e-01,  1.4848e+00, -1.7558e+00],\n",
            "        [ 2.0111e-01,  1.9787e-01,  2.8363e-01,  1.7210e-01, -6.7915e-01,\n",
            "         -2.9211e-01,  8.7254e-01, -1.6239e-01, -6.3656e-01, -1.4287e+00],\n",
            "        [ 1.6588e-01,  3.9723e-01,  8.8617e-01,  2.1837e-01, -7.4161e-01,\n",
            "         -1.0435e+00,  3.7173e-01, -1.0154e+00, -5.3333e-01,  1.4430e+00],\n",
            "        [ 1.0720e-01,  7.2399e-01, -5.7293e-01,  5.9061e-01,  6.0595e-02,\n",
            "         -5.4901e-01, -1.4979e+00,  8.2548e-01,  2.0812e-01,  1.3567e+00],\n",
            "        [-2.3666e+00, -3.2412e-01,  1.2713e+00, -6.4322e-01,  1.3885e+00,\n",
            "          5.9710e-01, -8.3490e-01,  4.6419e-01, -3.0334e-02,  8.6963e-02],\n",
            "        [-3.2968e-01, -1.7391e-01,  1.3189e+00, -7.9024e-02,  6.6718e-01,\n",
            "         -1.2511e+00, -2.3631e-01,  9.2156e-01, -4.1845e-01,  3.2484e-01],\n",
            "        [-4.6716e-02, -5.2039e-01,  6.1064e-01,  5.8791e-03, -5.7584e-01,\n",
            "         -6.2451e-01, -3.3809e-02, -8.2747e-01,  4.2721e-02,  5.0310e-01],\n",
            "        [ 3.0483e-01,  4.7102e-01, -3.6762e-01, -1.4770e+00,  1.3115e-01,\n",
            "         -1.1370e-01, -1.4018e+00, -3.4097e-01, -5.2890e-01,  1.2756e+00],\n",
            "        [-2.9052e-02, -1.0300e+00,  1.4951e+00,  6.6330e-01,  1.9184e-01,\n",
            "          3.2517e-01, -1.7158e-01,  1.1796e+00, -3.6515e-01, -1.1158e-01],\n",
            "        [-3.7223e-01,  1.1235e-01,  5.2458e-01,  8.6495e-01, -8.6669e-01,\n",
            "          7.4177e-01,  8.7777e-01,  2.1653e-01, -8.4715e-01,  3.4379e-01],\n",
            "        [ 5.2312e-01, -4.8984e-01, -9.3219e-01, -1.3750e+00,  1.5717e+00,\n",
            "          1.2056e+00, -8.8992e-01,  1.5066e-01, -2.6004e-01, -1.6082e+00],\n",
            "        [ 1.9444e+00, -4.6767e-01, -5.1049e-01, -7.9454e-01, -1.9109e-01,\n",
            "          2.6615e-01,  2.6372e-01, -1.0850e+00,  2.3091e-01,  4.9790e-01],\n",
            "        [-1.5249e-01, -8.8743e-01, -8.6158e-01, -2.4067e-01,  9.5697e-01,\n",
            "         -7.5013e-02,  1.2866e+00,  9.2153e-01,  4.8160e-01, -4.7744e-01],\n",
            "        [ 6.3202e-02,  6.4891e-01, -3.8890e-01, -2.0052e+00, -4.0168e-01,\n",
            "         -1.5907e+00,  1.1356e+00, -6.6066e-01, -1.1071e+00, -2.4832e-01],\n",
            "        [ 1.6843e+00,  1.9302e-01, -3.6441e-01, -3.8269e-01,  1.2270e+00,\n",
            "         -3.8172e-01, -6.2290e-01,  3.7300e-01, -1.1818e-01,  1.4330e+00],\n",
            "        [-1.4369e-01,  1.0089e+00, -6.2551e-02, -9.2294e-01,  2.3637e-01,\n",
            "          9.3235e-01, -1.0432e+00, -2.6852e-01,  3.2684e-01,  9.3788e-03],\n",
            "        [ 2.1517e+00,  1.7128e-01,  2.5091e-02, -4.3496e-01, -5.7793e-02,\n",
            "         -1.6287e+00, -6.5260e-01,  1.1609e-01,  1.6084e+00, -2.2649e-02],\n",
            "        [-4.1894e-01, -5.7293e-01,  8.2354e-01,  6.5812e-01, -2.6125e-01,\n",
            "          1.7359e-01, -9.8806e-02,  4.6759e-01, -7.2220e-01, -9.5473e-02],\n",
            "        [ 1.5067e+00,  2.1197e-01, -4.3322e-01,  2.5785e-01, -6.2274e-01,\n",
            "          1.2790e+00,  1.6896e-01, -3.4073e-01, -2.7398e-02, -9.0112e-01]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xZJgWRyGEVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycwxAPHZLNST",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Question 12\n",
        "\n",
        "In the code above, why do we have 2 in '2.0*(y_pred - y)`?\n",
        "\n",
        "Ans: it is derivative of the loss = (y_pred - y).pow(2) w.r.t y_pred\n",
        "\n",
        "## Question 13\n",
        "In the code above, what does `grad_h[h < 0] = 0` signify?\n",
        "\n",
        "## Question 14\n",
        "In the code above, how many \"epochs\" have we trained the model for? \n",
        "\n",
        "Ans: 500\n",
        "\n",
        "## Question 15\n",
        "In the code above, if we take the trained model, and run it on fresh  inputs, the trained model will be able to predict fresh output with high accuracy. \n",
        "\n",
        "\n",
        "Ans: no"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-_MG6UuMJHM",
        "colab_type": "code",
        "outputId": "b4ecf950-a353-416f-829a-8ede6db6668d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "h = x.mm(w1)\n",
        "h_relu = h.clamp(min=0) # replaces all values below 0 as 0\n",
        "y_pred = h_relu.mm(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "loss = (y_pred - y).pow(2).sum().item()\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14749494.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnJ49I07JJI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xlxq2bcNtzE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Question 16\n",
        "In the code above, if we dont use clone in `grad_h = grad_h_relu.clone()` the model will still train without any issues. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFgo-8hFavnD",
        "colab_type": "text"
      },
      "source": [
        "Answers:\n",
        "\n",
        "1 -  4\n",
        "\n",
        "2 -   Increases \n",
        "\n",
        "3 -   Cutout makes the neural network look for other parts of object too.\n",
        "\n",
        "4 - 1\n",
        "\n",
        "5 -  0.5  \n",
        "\n",
        "6 - No \n",
        "\n",
        "7 -  No\n",
        "\n",
        "8 -  No\n",
        "\n",
        "9 -  18\n",
        "\n",
        "10 -  False\n",
        "\n",
        "11 -     \n",
        "\n",
        "12 - it is derivative of the loss = (y_pred - y).pow(2) w.r.t y_pred\n",
        "\n",
        "13 -  \n",
        "\n",
        "14 -  500\n",
        "\n",
        "15 -  \n",
        "\n",
        "16 -  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtmfY3ieaxU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}